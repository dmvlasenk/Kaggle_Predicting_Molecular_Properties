{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules, set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import os\n",
    "\n",
    "\n",
    "from Utils import *\n",
    "# os.listdir('../input/imputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.max_columns', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "submission_csv = pd.read_csv(f'{Config.DATA_PATH}\\\\sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658152</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658153</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658154</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658155</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658156</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         molecule_index  atom_index_0  atom_index_1  type\n",
       "id                                                       \n",
       "4658147  4               2             0             2JHC\n",
       "4658148  4               2             1             1JHC\n",
       "4658149  4               2             3             3JHH\n",
       "4658150  4               3             0             1JHC\n",
       "4658151  4               3             1             2JHC\n",
       "4658152  15              3             0             1JHC\n",
       "4658153  15              3             2             3JHC\n",
       "4658154  15              3             4             2JHH\n",
       "4658155  15              3             5             2JHH\n",
       "4658156  15              4             0             1JHC"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.read_csv(f'{Config.DATA_PATH}\\\\test.csv', index_col='id', dtype=train_dtypes)\n",
    "test_csv['molecule_index'] = test_csv['molecule_name'].str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "test_csv = test_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type']]\n",
    "test_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "def train_and_predict_for_one_coupling_type(coupling_type, submission, n_folds=5, n_splits=5, random_state=128):\n",
    "    model_type =  'lgb' #lgb  cat\n",
    "    print(f'*** Training Model for {coupling_type} ***')\n",
    "    XY_Data = pd.read_csv(f'{Config.INPUT_XY}/{coupling_type}.csv', index_col=0)\n",
    "    X_data, y_data = build_x_y_data(XY_Data)    \n",
    "    \n",
    "    columns = X_data.columns \n",
    "    XY_Test = pd.read_csv(f'{Config.INPUT_XY}/test_{coupling_type}.csv', index_col=0)\n",
    "    X_test, _ = build_x_y_data(XY_Test)\n",
    "    y_pred = np.zeros(X_test.shape[0], dtype='float32')\n",
    "\n",
    "    train_csv = load_train()\n",
    "\n",
    "    cv_score = 0\n",
    "    \n",
    "    if n_folds > n_splits:\n",
    "        n_splits = n_folds\n",
    "    \n",
    "    #kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    kfold = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    \n",
    "    cur_train_csv = train_csv[train_csv.type == coupling_type]\n",
    "    groups  = cur_train_csv.molecule_index.values\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X_data, y_data, groups=groups)):\n",
    "        if fold >= n_folds:\n",
    "            break\n",
    "\n",
    "        #X_train, X_val = X_data[train_index], X_data[val_index]\n",
    "        #y_train, y_val = y_data[train_index], y_data[val_index]\n",
    "        X_train, X_val = X_data[columns].iloc[train_index], X_data[columns].iloc[val_index]\n",
    "        y_train, y_val = y_data.iloc[train_index], y_data.iloc[val_index]\n",
    "\n",
    "        \n",
    "        model = \"\"\n",
    "        categorical_features = [col for col in X_train if col.startswith('atom_')]\n",
    "        if model_type == 'lgb':\n",
    "\n",
    "            model = LGBMRegressor(**Config.LGB_PARAMS, n_estimators=15000, n_jobs = 5)\n",
    "            #model = LGBMRegressor(**LGB_PARAMS, n_estimators=6000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='mae',\n",
    "                verbose=100, early_stopping_rounds=1000,\n",
    "                     categorical_feature = categorical_features)\n",
    "        \n",
    "        if model_type == 'cat': \n",
    "            model = CatBoostRegressor(eval_metric='MAE', **CAT_PARAMS, loss_function='MAE')\n",
    "            X_trainGlob = X_train\n",
    "            model.fit(X_train, y_train, eval_set=((X_val, y_val)), \n",
    "                      cat_features = categorical_features,\n",
    "                      use_best_model=True,\n",
    "                      verbose=False)\n",
    "\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_score = np.log(mean_absolute_error(y_val, y_val_pred))\n",
    "        print(f'{coupling_type} Fold {fold}, logMAE: {val_score}')\n",
    "        \n",
    "        cv_score += val_score / n_folds\n",
    "        y_pred += model.predict(X_test) / n_folds\n",
    "        \n",
    "    submission.loc[test_csv['type'] == coupling_type, 'scalar_coupling_constant'] = y_pred\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a separate model for each type of coupling. Dataset is split into 5 pieces and in this kernel we will use only 3 folds for speed up.\n",
    "\n",
    "Main tuning parameter is the number of atoms. I took good numbers, but accuracy can be improved a bit by tuning them for each type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking cross-validation scores for each type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training Model for 1JHN ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.537618\tvalid_1's l1: 0.585336\n",
      "[200]\ttraining's l1: 0.413442\tvalid_1's l1: 0.490275\n",
      "[300]\ttraining's l1: 0.351078\tvalid_1's l1: 0.451444\n",
      "[400]\ttraining's l1: 0.307937\tvalid_1's l1: 0.426764\n",
      "[500]\ttraining's l1: 0.275382\tvalid_1's l1: 0.409766\n",
      "[600]\ttraining's l1: 0.249522\tvalid_1's l1: 0.397709\n",
      "[700]\ttraining's l1: 0.227688\tvalid_1's l1: 0.387508\n",
      "[800]\ttraining's l1: 0.21004\tvalid_1's l1: 0.380222\n",
      "[900]\ttraining's l1: 0.194102\tvalid_1's l1: 0.374254\n",
      "[1000]\ttraining's l1: 0.180395\tvalid_1's l1: 0.369353\n",
      "[1100]\ttraining's l1: 0.168497\tvalid_1's l1: 0.365107\n",
      "[1200]\ttraining's l1: 0.157691\tvalid_1's l1: 0.361835\n",
      "[1300]\ttraining's l1: 0.148041\tvalid_1's l1: 0.359033\n",
      "[1400]\ttraining's l1: 0.139338\tvalid_1's l1: 0.356513\n",
      "[1500]\ttraining's l1: 0.131388\tvalid_1's l1: 0.354285\n",
      "[1600]\ttraining's l1: 0.123978\tvalid_1's l1: 0.352345\n",
      "[1700]\ttraining's l1: 0.117423\tvalid_1's l1: 0.350892\n",
      "[1800]\ttraining's l1: 0.11122\tvalid_1's l1: 0.349371\n",
      "[1900]\ttraining's l1: 0.105556\tvalid_1's l1: 0.34809\n",
      "[2000]\ttraining's l1: 0.100226\tvalid_1's l1: 0.346932\n",
      "[2100]\ttraining's l1: 0.0954543\tvalid_1's l1: 0.345892\n",
      "[2200]\ttraining's l1: 0.0908331\tvalid_1's l1: 0.344769\n",
      "[2300]\ttraining's l1: 0.0865718\tvalid_1's l1: 0.343852\n",
      "[2400]\ttraining's l1: 0.0826119\tvalid_1's l1: 0.342918\n",
      "[2500]\ttraining's l1: 0.0790143\tvalid_1's l1: 0.342189\n",
      "[2600]\ttraining's l1: 0.0754771\tvalid_1's l1: 0.341552\n",
      "[2700]\ttraining's l1: 0.0721183\tvalid_1's l1: 0.340915\n",
      "[2800]\ttraining's l1: 0.0689405\tvalid_1's l1: 0.340233\n",
      "[2900]\ttraining's l1: 0.0659565\tvalid_1's l1: 0.339759\n",
      "[3000]\ttraining's l1: 0.0631487\tvalid_1's l1: 0.339288\n",
      "[3100]\ttraining's l1: 0.0605165\tvalid_1's l1: 0.33877\n",
      "[3200]\ttraining's l1: 0.0580052\tvalid_1's l1: 0.33826\n",
      "[3300]\ttraining's l1: 0.0556697\tvalid_1's l1: 0.33772\n",
      "[3400]\ttraining's l1: 0.0534235\tvalid_1's l1: 0.337414\n",
      "[3500]\ttraining's l1: 0.0513595\tvalid_1's l1: 0.337179\n",
      "[3600]\ttraining's l1: 0.0493182\tvalid_1's l1: 0.336823\n",
      "[3700]\ttraining's l1: 0.0473423\tvalid_1's l1: 0.336513\n",
      "[3800]\ttraining's l1: 0.0455365\tvalid_1's l1: 0.336247\n",
      "[3900]\ttraining's l1: 0.0438112\tvalid_1's l1: 0.335977\n",
      "[4000]\ttraining's l1: 0.0421289\tvalid_1's l1: 0.335813\n",
      "[4100]\ttraining's l1: 0.0405695\tvalid_1's l1: 0.335659\n",
      "[4200]\ttraining's l1: 0.0391147\tvalid_1's l1: 0.335451\n",
      "[4300]\ttraining's l1: 0.0376994\tvalid_1's l1: 0.33522\n",
      "[4400]\ttraining's l1: 0.036351\tvalid_1's l1: 0.335011\n",
      "[4500]\ttraining's l1: 0.0350303\tvalid_1's l1: 0.334827\n",
      "[4600]\ttraining's l1: 0.0338266\tvalid_1's l1: 0.334745\n",
      "[4700]\ttraining's l1: 0.0326244\tvalid_1's l1: 0.334596\n",
      "[4800]\ttraining's l1: 0.0315136\tvalid_1's l1: 0.334504\n",
      "[4900]\ttraining's l1: 0.0304251\tvalid_1's l1: 0.334375\n",
      "[5000]\ttraining's l1: 0.029355\tvalid_1's l1: 0.334238\n",
      "[5100]\ttraining's l1: 0.0284029\tvalid_1's l1: 0.33416\n",
      "[5200]\ttraining's l1: 0.0274344\tvalid_1's l1: 0.33407\n",
      "[5300]\ttraining's l1: 0.0264601\tvalid_1's l1: 0.333992\n",
      "[5400]\ttraining's l1: 0.0255612\tvalid_1's l1: 0.333876\n",
      "[5500]\ttraining's l1: 0.0247116\tvalid_1's l1: 0.333805\n",
      "[5600]\ttraining's l1: 0.0238866\tvalid_1's l1: 0.333683\n",
      "[5700]\ttraining's l1: 0.0230907\tvalid_1's l1: 0.333594\n",
      "[5800]\ttraining's l1: 0.02236\tvalid_1's l1: 0.333531\n",
      "[5900]\ttraining's l1: 0.0216359\tvalid_1's l1: 0.333464\n",
      "[6000]\ttraining's l1: 0.0209554\tvalid_1's l1: 0.333379\n",
      "[6100]\ttraining's l1: 0.0202709\tvalid_1's l1: 0.333285\n",
      "[6200]\ttraining's l1: 0.0196262\tvalid_1's l1: 0.333203\n",
      "[6300]\ttraining's l1: 0.0189845\tvalid_1's l1: 0.333142\n",
      "[6400]\ttraining's l1: 0.0183809\tvalid_1's l1: 0.333083\n",
      "[6500]\ttraining's l1: 0.0178237\tvalid_1's l1: 0.333\n",
      "[6600]\ttraining's l1: 0.0172722\tvalid_1's l1: 0.332961\n",
      "[6700]\ttraining's l1: 0.0167273\tvalid_1's l1: 0.332892\n",
      "[6800]\ttraining's l1: 0.0162122\tvalid_1's l1: 0.332851\n",
      "[6900]\ttraining's l1: 0.01573\tvalid_1's l1: 0.332817\n",
      "[7000]\ttraining's l1: 0.0152518\tvalid_1's l1: 0.332769\n",
      "[7100]\ttraining's l1: 0.0147979\tvalid_1's l1: 0.33275\n",
      "[7200]\ttraining's l1: 0.0143524\tvalid_1's l1: 0.33271\n",
      "[7300]\ttraining's l1: 0.0139353\tvalid_1's l1: 0.332687\n",
      "[7400]\ttraining's l1: 0.0135186\tvalid_1's l1: 0.332654\n",
      "[7500]\ttraining's l1: 0.0131259\tvalid_1's l1: 0.332629\n",
      "[7600]\ttraining's l1: 0.0127484\tvalid_1's l1: 0.332611\n",
      "[7700]\ttraining's l1: 0.0123857\tvalid_1's l1: 0.332593\n",
      "[7800]\ttraining's l1: 0.012028\tvalid_1's l1: 0.332571\n",
      "[7900]\ttraining's l1: 0.011685\tvalid_1's l1: 0.332557\n",
      "[8000]\ttraining's l1: 0.0113539\tvalid_1's l1: 0.332537\n",
      "[8100]\ttraining's l1: 0.011042\tvalid_1's l1: 0.332504\n",
      "[8200]\ttraining's l1: 0.0107315\tvalid_1's l1: 0.332463\n",
      "[8300]\ttraining's l1: 0.0104363\tvalid_1's l1: 0.332444\n",
      "[8400]\ttraining's l1: 0.0101583\tvalid_1's l1: 0.332427\n",
      "[8500]\ttraining's l1: 0.00987164\tvalid_1's l1: 0.332389\n",
      "[8600]\ttraining's l1: 0.00960141\tvalid_1's l1: 0.332363\n",
      "[8700]\ttraining's l1: 0.00934058\tvalid_1's l1: 0.332342\n",
      "[8800]\ttraining's l1: 0.0090871\tvalid_1's l1: 0.33232\n",
      "[8900]\ttraining's l1: 0.00884712\tvalid_1's l1: 0.332307\n",
      "[9000]\ttraining's l1: 0.00860009\tvalid_1's l1: 0.332292\n",
      "[9100]\ttraining's l1: 0.00836697\tvalid_1's l1: 0.332279\n",
      "[9200]\ttraining's l1: 0.00813643\tvalid_1's l1: 0.332258\n",
      "[9300]\ttraining's l1: 0.00790155\tvalid_1's l1: 0.332244\n",
      "[9400]\ttraining's l1: 0.00768277\tvalid_1's l1: 0.332224\n",
      "[9500]\ttraining's l1: 0.00747308\tvalid_1's l1: 0.332208\n",
      "[9600]\ttraining's l1: 0.00728287\tvalid_1's l1: 0.332188\n",
      "[9700]\ttraining's l1: 0.00709521\tvalid_1's l1: 0.332172\n",
      "[9800]\ttraining's l1: 0.00690825\tvalid_1's l1: 0.332159\n",
      "[9900]\ttraining's l1: 0.00672937\tvalid_1's l1: 0.332146\n",
      "[10000]\ttraining's l1: 0.00655567\tvalid_1's l1: 0.332134\n",
      "[10100]\ttraining's l1: 0.006383\tvalid_1's l1: 0.332122\n",
      "[10200]\ttraining's l1: 0.00621248\tvalid_1's l1: 0.33211\n",
      "[10300]\ttraining's l1: 0.00605779\tvalid_1's l1: 0.332096\n",
      "[10400]\ttraining's l1: 0.0059108\tvalid_1's l1: 0.332089\n",
      "[10500]\ttraining's l1: 0.0057719\tvalid_1's l1: 0.332079\n",
      "[10600]\ttraining's l1: 0.00562188\tvalid_1's l1: 0.332073\n",
      "[10700]\ttraining's l1: 0.00548214\tvalid_1's l1: 0.332058\n",
      "[10800]\ttraining's l1: 0.00534391\tvalid_1's l1: 0.332047\n",
      "[10900]\ttraining's l1: 0.00520919\tvalid_1's l1: 0.332041\n",
      "[11000]\ttraining's l1: 0.00507654\tvalid_1's l1: 0.332025\n",
      "[11100]\ttraining's l1: 0.0049544\tvalid_1's l1: 0.332023\n",
      "[11200]\ttraining's l1: 0.0048328\tvalid_1's l1: 0.332016\n",
      "[11300]\ttraining's l1: 0.00471969\tvalid_1's l1: 0.332011\n",
      "[11400]\ttraining's l1: 0.00460125\tvalid_1's l1: 0.331995\n",
      "[11500]\ttraining's l1: 0.00448436\tvalid_1's l1: 0.33199\n",
      "[11600]\ttraining's l1: 0.00436313\tvalid_1's l1: 0.331986\n",
      "[11700]\ttraining's l1: 0.00425396\tvalid_1's l1: 0.331975\n",
      "[11800]\ttraining's l1: 0.00414478\tvalid_1's l1: 0.331971\n",
      "[11900]\ttraining's l1: 0.00404058\tvalid_1's l1: 0.33197\n",
      "[12000]\ttraining's l1: 0.0039454\tvalid_1's l1: 0.331963\n",
      "[12100]\ttraining's l1: 0.00385269\tvalid_1's l1: 0.331954\n",
      "[12200]\ttraining's l1: 0.00376065\tvalid_1's l1: 0.331947\n",
      "[12300]\ttraining's l1: 0.0036738\tvalid_1's l1: 0.331939\n",
      "[12400]\ttraining's l1: 0.00358999\tvalid_1's l1: 0.331934\n",
      "[12500]\ttraining's l1: 0.00350402\tvalid_1's l1: 0.331929\n",
      "[12600]\ttraining's l1: 0.00342014\tvalid_1's l1: 0.331928\n",
      "[12700]\ttraining's l1: 0.00334079\tvalid_1's l1: 0.331919\n",
      "[12800]\ttraining's l1: 0.00326292\tvalid_1's l1: 0.331912\n",
      "[12900]\ttraining's l1: 0.00318679\tvalid_1's l1: 0.331904\n",
      "[13000]\ttraining's l1: 0.0031128\tvalid_1's l1: 0.331904\n",
      "[13100]\ttraining's l1: 0.00303863\tvalid_1's l1: 0.331899\n",
      "[13200]\ttraining's l1: 0.00296749\tvalid_1's l1: 0.331899\n",
      "[13300]\ttraining's l1: 0.00290064\tvalid_1's l1: 0.331898\n",
      "[13400]\ttraining's l1: 0.00283076\tvalid_1's l1: 0.331896\n",
      "[13500]\ttraining's l1: 0.00276145\tvalid_1's l1: 0.331896\n",
      "[13600]\ttraining's l1: 0.00270016\tvalid_1's l1: 0.331893\n",
      "[13700]\ttraining's l1: 0.00264055\tvalid_1's l1: 0.331895\n",
      "[13800]\ttraining's l1: 0.00257829\tvalid_1's l1: 0.331895\n",
      "[13900]\ttraining's l1: 0.00252019\tvalid_1's l1: 0.33189\n",
      "[14000]\ttraining's l1: 0.00246384\tvalid_1's l1: 0.331888\n",
      "[14100]\ttraining's l1: 0.00241094\tvalid_1's l1: 0.331887\n",
      "[14200]\ttraining's l1: 0.00235723\tvalid_1's l1: 0.331885\n",
      "[14300]\ttraining's l1: 0.00230491\tvalid_1's l1: 0.331881\n",
      "[14400]\ttraining's l1: 0.00225481\tvalid_1's l1: 0.331881\n",
      "[14500]\ttraining's l1: 0.00220465\tvalid_1's l1: 0.33188\n",
      "[14600]\ttraining's l1: 0.00215725\tvalid_1's l1: 0.331881\n",
      "[14700]\ttraining's l1: 0.0021106\tvalid_1's l1: 0.331879\n",
      "[14800]\ttraining's l1: 0.002065\tvalid_1's l1: 0.331875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14900]\ttraining's l1: 0.00201781\tvalid_1's l1: 0.331873\n",
      "[15000]\ttraining's l1: 0.00197249\tvalid_1's l1: 0.331868\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00197249\tvalid_1's l1: 0.331868\n",
      "1JHN Fold 0, logMAE: -1.1030186004080824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.537012\tvalid_1's l1: 0.603523\n",
      "[200]\ttraining's l1: 0.411885\tvalid_1's l1: 0.509113\n",
      "[300]\ttraining's l1: 0.349734\tvalid_1's l1: 0.466607\n",
      "[400]\ttraining's l1: 0.308393\tvalid_1's l1: 0.440447\n",
      "[500]\ttraining's l1: 0.277049\tvalid_1's l1: 0.423058\n",
      "[600]\ttraining's l1: 0.250312\tvalid_1's l1: 0.408879\n",
      "[700]\ttraining's l1: 0.229175\tvalid_1's l1: 0.398289\n",
      "[800]\ttraining's l1: 0.21107\tvalid_1's l1: 0.390041\n",
      "[900]\ttraining's l1: 0.19581\tvalid_1's l1: 0.384381\n",
      "[1000]\ttraining's l1: 0.182215\tvalid_1's l1: 0.378879\n",
      "[1100]\ttraining's l1: 0.16981\tvalid_1's l1: 0.374208\n",
      "[1200]\ttraining's l1: 0.158821\tvalid_1's l1: 0.370042\n",
      "[1300]\ttraining's l1: 0.149446\tvalid_1's l1: 0.366989\n",
      "[1400]\ttraining's l1: 0.140754\tvalid_1's l1: 0.364265\n",
      "[1500]\ttraining's l1: 0.133004\tvalid_1's l1: 0.362158\n",
      "[1600]\ttraining's l1: 0.125615\tvalid_1's l1: 0.359975\n",
      "[1700]\ttraining's l1: 0.118572\tvalid_1's l1: 0.357909\n",
      "[1800]\ttraining's l1: 0.112533\tvalid_1's l1: 0.356026\n",
      "[1900]\ttraining's l1: 0.106861\tvalid_1's l1: 0.354534\n",
      "[2000]\ttraining's l1: 0.101551\tvalid_1's l1: 0.353348\n",
      "[2100]\ttraining's l1: 0.0965028\tvalid_1's l1: 0.352077\n",
      "[2200]\ttraining's l1: 0.0918016\tvalid_1's l1: 0.351171\n",
      "[2300]\ttraining's l1: 0.0874718\tvalid_1's l1: 0.350149\n",
      "[2400]\ttraining's l1: 0.0833997\tvalid_1's l1: 0.349444\n",
      "[2500]\ttraining's l1: 0.0796051\tvalid_1's l1: 0.348732\n",
      "[2600]\ttraining's l1: 0.0759277\tvalid_1's l1: 0.348004\n",
      "[2700]\ttraining's l1: 0.0724905\tvalid_1's l1: 0.347305\n",
      "[2800]\ttraining's l1: 0.0692878\tvalid_1's l1: 0.346594\n",
      "[2900]\ttraining's l1: 0.0664208\tvalid_1's l1: 0.346123\n",
      "[3000]\ttraining's l1: 0.0636664\tvalid_1's l1: 0.345731\n",
      "[3100]\ttraining's l1: 0.0609852\tvalid_1's l1: 0.345285\n",
      "[3200]\ttraining's l1: 0.0585509\tvalid_1's l1: 0.344851\n",
      "[3300]\ttraining's l1: 0.0561864\tvalid_1's l1: 0.344546\n",
      "[3400]\ttraining's l1: 0.0539095\tvalid_1's l1: 0.344218\n",
      "[3500]\ttraining's l1: 0.0516863\tvalid_1's l1: 0.343874\n",
      "[3600]\ttraining's l1: 0.0496964\tvalid_1's l1: 0.343589\n",
      "[3700]\ttraining's l1: 0.0477963\tvalid_1's l1: 0.343214\n",
      "[3800]\ttraining's l1: 0.0459671\tvalid_1's l1: 0.342939\n",
      "[3900]\ttraining's l1: 0.0441849\tvalid_1's l1: 0.342685\n",
      "[4000]\ttraining's l1: 0.0425298\tvalid_1's l1: 0.342484\n",
      "[4100]\ttraining's l1: 0.0409441\tvalid_1's l1: 0.342221\n",
      "[4200]\ttraining's l1: 0.0394501\tvalid_1's l1: 0.341987\n",
      "[4300]\ttraining's l1: 0.0379818\tvalid_1's l1: 0.341807\n",
      "[4400]\ttraining's l1: 0.0366062\tvalid_1's l1: 0.341628\n",
      "[4500]\ttraining's l1: 0.0353059\tvalid_1's l1: 0.341496\n",
      "[4600]\ttraining's l1: 0.0340718\tvalid_1's l1: 0.341346\n",
      "[4700]\ttraining's l1: 0.0328563\tvalid_1's l1: 0.341214\n",
      "[4800]\ttraining's l1: 0.0316825\tvalid_1's l1: 0.341059\n",
      "[4900]\ttraining's l1: 0.0305427\tvalid_1's l1: 0.340942\n",
      "[5000]\ttraining's l1: 0.0295098\tvalid_1's l1: 0.340825\n",
      "[5100]\ttraining's l1: 0.0284963\tvalid_1's l1: 0.34071\n",
      "[5200]\ttraining's l1: 0.0275397\tvalid_1's l1: 0.340634\n",
      "[5300]\ttraining's l1: 0.0266436\tvalid_1's l1: 0.34054\n",
      "[5400]\ttraining's l1: 0.0257405\tvalid_1's l1: 0.340478\n",
      "[5500]\ttraining's l1: 0.0248914\tvalid_1's l1: 0.340382\n",
      "[5600]\ttraining's l1: 0.0240374\tvalid_1's l1: 0.340295\n",
      "[5700]\ttraining's l1: 0.0232665\tvalid_1's l1: 0.340224\n",
      "[5800]\ttraining's l1: 0.0225093\tvalid_1's l1: 0.340141\n",
      "[5900]\ttraining's l1: 0.0217822\tvalid_1's l1: 0.340083\n",
      "[6000]\ttraining's l1: 0.02108\tvalid_1's l1: 0.340002\n",
      "[6100]\ttraining's l1: 0.0203941\tvalid_1's l1: 0.339934\n",
      "[6200]\ttraining's l1: 0.0197598\tvalid_1's l1: 0.339861\n",
      "[6300]\ttraining's l1: 0.0191387\tvalid_1's l1: 0.339784\n",
      "[6400]\ttraining's l1: 0.0185316\tvalid_1's l1: 0.33976\n",
      "[6500]\ttraining's l1: 0.017954\tvalid_1's l1: 0.339729\n",
      "[6600]\ttraining's l1: 0.0173833\tvalid_1's l1: 0.339677\n",
      "[6700]\ttraining's l1: 0.0168534\tvalid_1's l1: 0.339623\n",
      "[6800]\ttraining's l1: 0.0163359\tvalid_1's l1: 0.339566\n",
      "[6900]\ttraining's l1: 0.015841\tvalid_1's l1: 0.339536\n",
      "[7000]\ttraining's l1: 0.0153727\tvalid_1's l1: 0.339516\n",
      "[7100]\ttraining's l1: 0.0149045\tvalid_1's l1: 0.339475\n",
      "[7200]\ttraining's l1: 0.0144624\tvalid_1's l1: 0.339443\n",
      "[7300]\ttraining's l1: 0.0140556\tvalid_1's l1: 0.339413\n",
      "[7400]\ttraining's l1: 0.0136452\tvalid_1's l1: 0.339396\n",
      "[7500]\ttraining's l1: 0.0132513\tvalid_1's l1: 0.339351\n",
      "[7600]\ttraining's l1: 0.0128609\tvalid_1's l1: 0.339313\n",
      "[7700]\ttraining's l1: 0.012471\tvalid_1's l1: 0.339288\n",
      "[7800]\ttraining's l1: 0.0121069\tvalid_1's l1: 0.339271\n",
      "[7900]\ttraining's l1: 0.0117652\tvalid_1's l1: 0.339235\n",
      "[8000]\ttraining's l1: 0.0114385\tvalid_1's l1: 0.339216\n",
      "[8100]\ttraining's l1: 0.0111006\tvalid_1's l1: 0.339196\n",
      "[8200]\ttraining's l1: 0.0107824\tvalid_1's l1: 0.339181\n",
      "[8300]\ttraining's l1: 0.0104892\tvalid_1's l1: 0.339168\n",
      "[8400]\ttraining's l1: 0.0101942\tvalid_1's l1: 0.339136\n",
      "[8500]\ttraining's l1: 0.00991999\tvalid_1's l1: 0.339114\n",
      "[8600]\ttraining's l1: 0.00965086\tvalid_1's l1: 0.339091\n",
      "[8700]\ttraining's l1: 0.00938929\tvalid_1's l1: 0.339086\n",
      "[8800]\ttraining's l1: 0.00913523\tvalid_1's l1: 0.339077\n",
      "[8900]\ttraining's l1: 0.00887903\tvalid_1's l1: 0.339055\n",
      "[9000]\ttraining's l1: 0.00862335\tvalid_1's l1: 0.339024\n",
      "[9100]\ttraining's l1: 0.00838714\tvalid_1's l1: 0.338989\n",
      "[9200]\ttraining's l1: 0.00816515\tvalid_1's l1: 0.338983\n",
      "[9300]\ttraining's l1: 0.0079472\tvalid_1's l1: 0.338974\n",
      "[9400]\ttraining's l1: 0.0077335\tvalid_1's l1: 0.338963\n",
      "[9500]\ttraining's l1: 0.00753277\tvalid_1's l1: 0.33895\n",
      "[9600]\ttraining's l1: 0.00733496\tvalid_1's l1: 0.338939\n",
      "[9700]\ttraining's l1: 0.0071397\tvalid_1's l1: 0.338927\n",
      "[9800]\ttraining's l1: 0.00694602\tvalid_1's l1: 0.338914\n",
      "[9900]\ttraining's l1: 0.00676191\tvalid_1's l1: 0.338904\n",
      "[10000]\ttraining's l1: 0.00658405\tvalid_1's l1: 0.338876\n",
      "[10100]\ttraining's l1: 0.00641612\tvalid_1's l1: 0.338862\n",
      "[10200]\ttraining's l1: 0.00624524\tvalid_1's l1: 0.338851\n",
      "[10300]\ttraining's l1: 0.00608264\tvalid_1's l1: 0.338843\n",
      "[10400]\ttraining's l1: 0.00592155\tvalid_1's l1: 0.338838\n",
      "[10500]\ttraining's l1: 0.00576847\tvalid_1's l1: 0.33883\n",
      "[10600]\ttraining's l1: 0.00562315\tvalid_1's l1: 0.338822\n",
      "[10700]\ttraining's l1: 0.00548449\tvalid_1's l1: 0.33881\n",
      "[10800]\ttraining's l1: 0.00534798\tvalid_1's l1: 0.338804\n",
      "[10900]\ttraining's l1: 0.00521245\tvalid_1's l1: 0.338796\n",
      "[11000]\ttraining's l1: 0.00508043\tvalid_1's l1: 0.338789\n",
      "[11100]\ttraining's l1: 0.00495445\tvalid_1's l1: 0.338779\n",
      "[11200]\ttraining's l1: 0.00482799\tvalid_1's l1: 0.338778\n",
      "[11300]\ttraining's l1: 0.00470835\tvalid_1's l1: 0.338773\n",
      "[11400]\ttraining's l1: 0.00458858\tvalid_1's l1: 0.338767\n",
      "[11500]\ttraining's l1: 0.00447807\tvalid_1's l1: 0.338769\n",
      "[11600]\ttraining's l1: 0.00436647\tvalid_1's l1: 0.338761\n",
      "[11700]\ttraining's l1: 0.00425632\tvalid_1's l1: 0.338757\n",
      "[11800]\ttraining's l1: 0.00415214\tvalid_1's l1: 0.338753\n",
      "[11900]\ttraining's l1: 0.00405741\tvalid_1's l1: 0.33875\n",
      "[12000]\ttraining's l1: 0.00395827\tvalid_1's l1: 0.33874\n",
      "[12100]\ttraining's l1: 0.00386069\tvalid_1's l1: 0.33874\n",
      "[12200]\ttraining's l1: 0.00377262\tvalid_1's l1: 0.338736\n",
      "[12300]\ttraining's l1: 0.00368431\tvalid_1's l1: 0.338729\n",
      "[12400]\ttraining's l1: 0.00359249\tvalid_1's l1: 0.338726\n",
      "[12500]\ttraining's l1: 0.00350597\tvalid_1's l1: 0.338718\n",
      "[12600]\ttraining's l1: 0.00342323\tvalid_1's l1: 0.338708\n",
      "[12700]\ttraining's l1: 0.00334074\tvalid_1's l1: 0.338701\n",
      "[12800]\ttraining's l1: 0.0032573\tvalid_1's l1: 0.338695\n",
      "[12900]\ttraining's l1: 0.00317918\tvalid_1's l1: 0.338686\n",
      "[13000]\ttraining's l1: 0.00310428\tvalid_1's l1: 0.338676\n",
      "[13100]\ttraining's l1: 0.00303219\tvalid_1's l1: 0.338675\n",
      "[13200]\ttraining's l1: 0.00295802\tvalid_1's l1: 0.338675\n",
      "[13300]\ttraining's l1: 0.0028907\tvalid_1's l1: 0.338673\n",
      "[13400]\ttraining's l1: 0.00281958\tvalid_1's l1: 0.338667\n",
      "[13500]\ttraining's l1: 0.00275605\tvalid_1's l1: 0.338662\n",
      "[13600]\ttraining's l1: 0.00268986\tvalid_1's l1: 0.338662\n",
      "[13700]\ttraining's l1: 0.00262932\tvalid_1's l1: 0.338662\n",
      "[13800]\ttraining's l1: 0.00257198\tvalid_1's l1: 0.338654\n",
      "[13900]\ttraining's l1: 0.00251284\tvalid_1's l1: 0.338653\n",
      "[14000]\ttraining's l1: 0.00245625\tvalid_1's l1: 0.338649\n",
      "[14100]\ttraining's l1: 0.00240023\tvalid_1's l1: 0.338644\n",
      "[14200]\ttraining's l1: 0.00234563\tvalid_1's l1: 0.338637\n",
      "[14300]\ttraining's l1: 0.00229426\tvalid_1's l1: 0.338635\n",
      "[14400]\ttraining's l1: 0.0022427\tvalid_1's l1: 0.338631\n",
      "[14500]\ttraining's l1: 0.00219266\tvalid_1's l1: 0.33863\n",
      "[14600]\ttraining's l1: 0.00214426\tvalid_1's l1: 0.338626\n",
      "[14700]\ttraining's l1: 0.00209662\tvalid_1's l1: 0.338621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14800]\ttraining's l1: 0.00205358\tvalid_1's l1: 0.338623\n",
      "[14900]\ttraining's l1: 0.00200733\tvalid_1's l1: 0.338621\n",
      "[15000]\ttraining's l1: 0.00196486\tvalid_1's l1: 0.338621\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00196486\tvalid_1's l1: 0.338621\n",
      "1JHN Fold 1, logMAE: -1.0830265176329374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.53977\tvalid_1's l1: 0.59804\n",
      "[200]\ttraining's l1: 0.414936\tvalid_1's l1: 0.498964\n",
      "[300]\ttraining's l1: 0.351226\tvalid_1's l1: 0.456103\n",
      "[400]\ttraining's l1: 0.308706\tvalid_1's l1: 0.432642\n",
      "[500]\ttraining's l1: 0.275941\tvalid_1's l1: 0.414787\n",
      "[600]\ttraining's l1: 0.249937\tvalid_1's l1: 0.401873\n",
      "[700]\ttraining's l1: 0.228674\tvalid_1's l1: 0.392167\n",
      "[800]\ttraining's l1: 0.21056\tvalid_1's l1: 0.384104\n",
      "[900]\ttraining's l1: 0.195051\tvalid_1's l1: 0.377895\n",
      "[1000]\ttraining's l1: 0.181534\tvalid_1's l1: 0.372706\n",
      "[1100]\ttraining's l1: 0.169451\tvalid_1's l1: 0.368555\n",
      "[1200]\ttraining's l1: 0.158758\tvalid_1's l1: 0.36498\n",
      "[1300]\ttraining's l1: 0.148948\tvalid_1's l1: 0.361909\n",
      "[1400]\ttraining's l1: 0.140002\tvalid_1's l1: 0.35928\n",
      "[1500]\ttraining's l1: 0.131959\tvalid_1's l1: 0.356816\n",
      "[1600]\ttraining's l1: 0.124516\tvalid_1's l1: 0.354865\n",
      "[1700]\ttraining's l1: 0.117935\tvalid_1's l1: 0.353084\n",
      "[1800]\ttraining's l1: 0.111955\tvalid_1's l1: 0.351818\n",
      "[1900]\ttraining's l1: 0.106266\tvalid_1's l1: 0.350398\n",
      "[2000]\ttraining's l1: 0.101081\tvalid_1's l1: 0.349227\n",
      "[2100]\ttraining's l1: 0.0961953\tvalid_1's l1: 0.348067\n",
      "[2200]\ttraining's l1: 0.0916365\tvalid_1's l1: 0.347017\n",
      "[2300]\ttraining's l1: 0.0873237\tvalid_1's l1: 0.346145\n",
      "[2400]\ttraining's l1: 0.0833812\tvalid_1's l1: 0.345149\n",
      "[2500]\ttraining's l1: 0.0794661\tvalid_1's l1: 0.344231\n",
      "[2600]\ttraining's l1: 0.0760229\tvalid_1's l1: 0.343581\n",
      "[2700]\ttraining's l1: 0.0727678\tvalid_1's l1: 0.342746\n",
      "[2800]\ttraining's l1: 0.0696727\tvalid_1's l1: 0.342076\n",
      "[2900]\ttraining's l1: 0.0666666\tvalid_1's l1: 0.341608\n",
      "[3000]\ttraining's l1: 0.0638985\tvalid_1's l1: 0.34113\n",
      "[3100]\ttraining's l1: 0.0612682\tvalid_1's l1: 0.3407\n",
      "[3200]\ttraining's l1: 0.0587963\tvalid_1's l1: 0.340323\n",
      "[3300]\ttraining's l1: 0.0564374\tvalid_1's l1: 0.339922\n",
      "[3400]\ttraining's l1: 0.054121\tvalid_1's l1: 0.339611\n",
      "[3500]\ttraining's l1: 0.0519612\tvalid_1's l1: 0.339289\n",
      "[3600]\ttraining's l1: 0.0498737\tvalid_1's l1: 0.339008\n",
      "[3700]\ttraining's l1: 0.0478786\tvalid_1's l1: 0.338654\n",
      "[3800]\ttraining's l1: 0.0460845\tvalid_1's l1: 0.338468\n",
      "[3900]\ttraining's l1: 0.0443852\tvalid_1's l1: 0.33822\n",
      "[4000]\ttraining's l1: 0.0427237\tvalid_1's l1: 0.33792\n",
      "[4100]\ttraining's l1: 0.041143\tvalid_1's l1: 0.337724\n",
      "[4200]\ttraining's l1: 0.0396362\tvalid_1's l1: 0.337545\n",
      "[4300]\ttraining's l1: 0.038212\tvalid_1's l1: 0.337361\n",
      "[4400]\ttraining's l1: 0.0368367\tvalid_1's l1: 0.337181\n",
      "[4500]\ttraining's l1: 0.035565\tvalid_1's l1: 0.337052\n",
      "[4600]\ttraining's l1: 0.0343148\tvalid_1's l1: 0.336925\n",
      "[4700]\ttraining's l1: 0.0330839\tvalid_1's l1: 0.336813\n",
      "[4800]\ttraining's l1: 0.0319336\tvalid_1's l1: 0.336672\n",
      "[4900]\ttraining's l1: 0.0308464\tvalid_1's l1: 0.336511\n",
      "[5000]\ttraining's l1: 0.0297802\tvalid_1's l1: 0.336328\n",
      "[5100]\ttraining's l1: 0.0287544\tvalid_1's l1: 0.336204\n",
      "[5200]\ttraining's l1: 0.0277916\tvalid_1's l1: 0.336106\n",
      "[5300]\ttraining's l1: 0.0268838\tvalid_1's l1: 0.336001\n",
      "[5400]\ttraining's l1: 0.0260295\tvalid_1's l1: 0.335855\n",
      "[5500]\ttraining's l1: 0.0251589\tvalid_1's l1: 0.335779\n",
      "[5600]\ttraining's l1: 0.0243477\tvalid_1's l1: 0.335654\n",
      "[5700]\ttraining's l1: 0.0235372\tvalid_1's l1: 0.335561\n",
      "[5800]\ttraining's l1: 0.0227958\tvalid_1's l1: 0.33548\n",
      "[5900]\ttraining's l1: 0.0220375\tvalid_1's l1: 0.335401\n",
      "[6000]\ttraining's l1: 0.0213253\tvalid_1's l1: 0.335309\n",
      "[6100]\ttraining's l1: 0.0206703\tvalid_1's l1: 0.335221\n",
      "[6200]\ttraining's l1: 0.0200312\tvalid_1's l1: 0.33513\n",
      "[6300]\ttraining's l1: 0.0194363\tvalid_1's l1: 0.335058\n",
      "[6400]\ttraining's l1: 0.0188343\tvalid_1's l1: 0.334976\n",
      "[6500]\ttraining's l1: 0.0182384\tvalid_1's l1: 0.334912\n",
      "[6600]\ttraining's l1: 0.0176687\tvalid_1's l1: 0.334847\n",
      "[6700]\ttraining's l1: 0.0171293\tvalid_1's l1: 0.334795\n",
      "[6800]\ttraining's l1: 0.0166059\tvalid_1's l1: 0.334764\n",
      "[6900]\ttraining's l1: 0.0160748\tvalid_1's l1: 0.334724\n",
      "[7000]\ttraining's l1: 0.0155983\tvalid_1's l1: 0.334675\n",
      "[7100]\ttraining's l1: 0.015136\tvalid_1's l1: 0.334655\n",
      "[7200]\ttraining's l1: 0.0146867\tvalid_1's l1: 0.334634\n",
      "[7300]\ttraining's l1: 0.0142548\tvalid_1's l1: 0.33457\n",
      "[7400]\ttraining's l1: 0.0138388\tvalid_1's l1: 0.334543\n",
      "[7500]\ttraining's l1: 0.0134273\tvalid_1's l1: 0.334516\n",
      "[7600]\ttraining's l1: 0.0130343\tvalid_1's l1: 0.334486\n",
      "[7700]\ttraining's l1: 0.0126656\tvalid_1's l1: 0.334426\n",
      "[7800]\ttraining's l1: 0.0123083\tvalid_1's l1: 0.334386\n",
      "[7900]\ttraining's l1: 0.0119634\tvalid_1's l1: 0.334351\n",
      "[8000]\ttraining's l1: 0.0116181\tvalid_1's l1: 0.334315\n",
      "[8100]\ttraining's l1: 0.0113033\tvalid_1's l1: 0.334306\n",
      "[8200]\ttraining's l1: 0.0109806\tvalid_1's l1: 0.334277\n",
      "[8300]\ttraining's l1: 0.0106827\tvalid_1's l1: 0.334247\n",
      "[8400]\ttraining's l1: 0.0103817\tvalid_1's l1: 0.334207\n",
      "[8500]\ttraining's l1: 0.0100757\tvalid_1's l1: 0.33418\n",
      "[8600]\ttraining's l1: 0.00979033\tvalid_1's l1: 0.334155\n",
      "[8700]\ttraining's l1: 0.00951145\tvalid_1's l1: 0.334135\n",
      "[8800]\ttraining's l1: 0.00924929\tvalid_1's l1: 0.334113\n",
      "[8900]\ttraining's l1: 0.00899606\tvalid_1's l1: 0.334101\n",
      "[9000]\ttraining's l1: 0.00874289\tvalid_1's l1: 0.334084\n",
      "[9100]\ttraining's l1: 0.00849955\tvalid_1's l1: 0.334064\n",
      "[9200]\ttraining's l1: 0.00826944\tvalid_1's l1: 0.334048\n",
      "[9300]\ttraining's l1: 0.00804841\tvalid_1's l1: 0.334025\n",
      "[9400]\ttraining's l1: 0.00783245\tvalid_1's l1: 0.33401\n",
      "[9500]\ttraining's l1: 0.00762438\tvalid_1's l1: 0.333993\n",
      "[9600]\ttraining's l1: 0.00742168\tvalid_1's l1: 0.333974\n",
      "[9700]\ttraining's l1: 0.00722335\tvalid_1's l1: 0.333964\n",
      "[9800]\ttraining's l1: 0.007027\tvalid_1's l1: 0.333952\n",
      "[9900]\ttraining's l1: 0.00684535\tvalid_1's l1: 0.333942\n",
      "[10000]\ttraining's l1: 0.00666732\tvalid_1's l1: 0.333931\n",
      "[10100]\ttraining's l1: 0.00649532\tvalid_1's l1: 0.333911\n",
      "[10200]\ttraining's l1: 0.00633055\tvalid_1's l1: 0.333905\n",
      "[10300]\ttraining's l1: 0.00617141\tvalid_1's l1: 0.33389\n",
      "[10400]\ttraining's l1: 0.00600893\tvalid_1's l1: 0.333881\n",
      "[10500]\ttraining's l1: 0.00584968\tvalid_1's l1: 0.333869\n",
      "[10600]\ttraining's l1: 0.0056979\tvalid_1's l1: 0.333859\n",
      "[10700]\ttraining's l1: 0.00554679\tvalid_1's l1: 0.333847\n",
      "[10800]\ttraining's l1: 0.0054093\tvalid_1's l1: 0.333833\n",
      "[10900]\ttraining's l1: 0.00527155\tvalid_1's l1: 0.333825\n",
      "[11000]\ttraining's l1: 0.00514043\tvalid_1's l1: 0.333817\n",
      "[11100]\ttraining's l1: 0.00500918\tvalid_1's l1: 0.333809\n",
      "[11200]\ttraining's l1: 0.0048827\tvalid_1's l1: 0.333802\n",
      "[11300]\ttraining's l1: 0.00476264\tvalid_1's l1: 0.333794\n",
      "[11400]\ttraining's l1: 0.00464278\tvalid_1's l1: 0.333793\n",
      "[11500]\ttraining's l1: 0.00452758\tvalid_1's l1: 0.333788\n",
      "[11600]\ttraining's l1: 0.00441901\tvalid_1's l1: 0.333788\n",
      "[11700]\ttraining's l1: 0.00431321\tvalid_1's l1: 0.333789\n",
      "[11800]\ttraining's l1: 0.00420796\tvalid_1's l1: 0.333786\n",
      "[11900]\ttraining's l1: 0.00410457\tvalid_1's l1: 0.333783\n",
      "[12000]\ttraining's l1: 0.00400623\tvalid_1's l1: 0.333779\n",
      "[12100]\ttraining's l1: 0.00391135\tvalid_1's l1: 0.333778\n",
      "[12200]\ttraining's l1: 0.00381675\tvalid_1's l1: 0.333774\n",
      "[12300]\ttraining's l1: 0.00372932\tvalid_1's l1: 0.333771\n",
      "[12400]\ttraining's l1: 0.00363888\tvalid_1's l1: 0.333769\n",
      "[12500]\ttraining's l1: 0.00355088\tvalid_1's l1: 0.333767\n",
      "[12600]\ttraining's l1: 0.00346388\tvalid_1's l1: 0.333759\n",
      "[12700]\ttraining's l1: 0.00337616\tvalid_1's l1: 0.333756\n",
      "[12800]\ttraining's l1: 0.00329822\tvalid_1's l1: 0.333754\n",
      "[12900]\ttraining's l1: 0.00322274\tvalid_1's l1: 0.333753\n",
      "[13000]\ttraining's l1: 0.00314714\tvalid_1's l1: 0.333746\n",
      "[13100]\ttraining's l1: 0.00307303\tvalid_1's l1: 0.333746\n",
      "[13200]\ttraining's l1: 0.00300316\tvalid_1's l1: 0.333746\n",
      "[13300]\ttraining's l1: 0.00293508\tvalid_1's l1: 0.333738\n",
      "[13400]\ttraining's l1: 0.00287022\tvalid_1's l1: 0.333732\n",
      "[13500]\ttraining's l1: 0.00280619\tvalid_1's l1: 0.33373\n",
      "[13600]\ttraining's l1: 0.00274555\tvalid_1's l1: 0.333726\n",
      "[13700]\ttraining's l1: 0.00268643\tvalid_1's l1: 0.333722\n",
      "[13800]\ttraining's l1: 0.00262678\tvalid_1's l1: 0.333716\n",
      "[13900]\ttraining's l1: 0.00256736\tvalid_1's l1: 0.333712\n",
      "[14000]\ttraining's l1: 0.00251252\tvalid_1's l1: 0.333709\n",
      "[14100]\ttraining's l1: 0.00245725\tvalid_1's l1: 0.333706\n",
      "[14200]\ttraining's l1: 0.00240398\tvalid_1's l1: 0.333706\n",
      "[14300]\ttraining's l1: 0.00235074\tvalid_1's l1: 0.333705\n",
      "[14400]\ttraining's l1: 0.00229545\tvalid_1's l1: 0.333707\n",
      "[14500]\ttraining's l1: 0.00224241\tvalid_1's l1: 0.333704\n",
      "[14600]\ttraining's l1: 0.0021885\tvalid_1's l1: 0.333698\n",
      "[14700]\ttraining's l1: 0.00214084\tvalid_1's l1: 0.333696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14800]\ttraining's l1: 0.00209293\tvalid_1's l1: 0.333693\n",
      "[14900]\ttraining's l1: 0.00204362\tvalid_1's l1: 0.33369\n",
      "[15000]\ttraining's l1: 0.00199811\tvalid_1's l1: 0.333689\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00199811\tvalid_1's l1: 0.333689\n",
      "1JHN Fold 2, logMAE: -1.0975467621221928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.541761\tvalid_1's l1: 0.58421\n",
      "[200]\ttraining's l1: 0.415321\tvalid_1's l1: 0.488139\n",
      "[300]\ttraining's l1: 0.353529\tvalid_1's l1: 0.447525\n",
      "[400]\ttraining's l1: 0.310239\tvalid_1's l1: 0.422468\n",
      "[500]\ttraining's l1: 0.278294\tvalid_1's l1: 0.405074\n",
      "[600]\ttraining's l1: 0.252071\tvalid_1's l1: 0.392118\n",
      "[700]\ttraining's l1: 0.230636\tvalid_1's l1: 0.382064\n",
      "[800]\ttraining's l1: 0.212324\tvalid_1's l1: 0.374012\n",
      "[900]\ttraining's l1: 0.196569\tvalid_1's l1: 0.36761\n",
      "[1000]\ttraining's l1: 0.182566\tvalid_1's l1: 0.362312\n",
      "[1100]\ttraining's l1: 0.170631\tvalid_1's l1: 0.358414\n",
      "[1200]\ttraining's l1: 0.159607\tvalid_1's l1: 0.354331\n",
      "[1300]\ttraining's l1: 0.150131\tvalid_1's l1: 0.351431\n",
      "[1400]\ttraining's l1: 0.141182\tvalid_1's l1: 0.348516\n",
      "[1500]\ttraining's l1: 0.133155\tvalid_1's l1: 0.346165\n",
      "[1600]\ttraining's l1: 0.125904\tvalid_1's l1: 0.343842\n",
      "[1700]\ttraining's l1: 0.119118\tvalid_1's l1: 0.341983\n",
      "[1800]\ttraining's l1: 0.112947\tvalid_1's l1: 0.340571\n",
      "[1900]\ttraining's l1: 0.106918\tvalid_1's l1: 0.339029\n",
      "[2000]\ttraining's l1: 0.101699\tvalid_1's l1: 0.337839\n",
      "[2100]\ttraining's l1: 0.0965838\tvalid_1's l1: 0.336676\n",
      "[2200]\ttraining's l1: 0.0919332\tvalid_1's l1: 0.335641\n",
      "[2300]\ttraining's l1: 0.0875903\tvalid_1's l1: 0.334777\n",
      "[2400]\ttraining's l1: 0.0836057\tvalid_1's l1: 0.333888\n",
      "[2500]\ttraining's l1: 0.0796949\tvalid_1's l1: 0.332972\n",
      "[2600]\ttraining's l1: 0.076197\tvalid_1's l1: 0.332203\n",
      "[2700]\ttraining's l1: 0.0728162\tvalid_1's l1: 0.331541\n",
      "[2800]\ttraining's l1: 0.0696673\tvalid_1's l1: 0.330952\n",
      "[2900]\ttraining's l1: 0.066638\tvalid_1's l1: 0.330347\n",
      "[3000]\ttraining's l1: 0.0639033\tvalid_1's l1: 0.329674\n",
      "[3100]\ttraining's l1: 0.0612253\tvalid_1's l1: 0.32919\n",
      "[3200]\ttraining's l1: 0.0587848\tvalid_1's l1: 0.328744\n",
      "[3300]\ttraining's l1: 0.0563012\tvalid_1's l1: 0.32828\n",
      "[3400]\ttraining's l1: 0.0539927\tvalid_1's l1: 0.327852\n",
      "[3500]\ttraining's l1: 0.0518656\tvalid_1's l1: 0.327485\n",
      "[3600]\ttraining's l1: 0.0498261\tvalid_1's l1: 0.327209\n",
      "[3700]\ttraining's l1: 0.0479361\tvalid_1's l1: 0.326927\n",
      "[3800]\ttraining's l1: 0.0461294\tvalid_1's l1: 0.326634\n",
      "[3900]\ttraining's l1: 0.0444156\tvalid_1's l1: 0.326371\n",
      "[4000]\ttraining's l1: 0.0427404\tvalid_1's l1: 0.326218\n",
      "[4100]\ttraining's l1: 0.0411626\tvalid_1's l1: 0.326024\n",
      "[4200]\ttraining's l1: 0.0396778\tvalid_1's l1: 0.325814\n",
      "[4300]\ttraining's l1: 0.0382439\tvalid_1's l1: 0.325618\n",
      "[4400]\ttraining's l1: 0.036809\tvalid_1's l1: 0.325316\n",
      "[4500]\ttraining's l1: 0.035475\tvalid_1's l1: 0.325157\n",
      "[4600]\ttraining's l1: 0.0342153\tvalid_1's l1: 0.32496\n",
      "[4700]\ttraining's l1: 0.0330055\tvalid_1's l1: 0.324787\n",
      "[4800]\ttraining's l1: 0.0318454\tvalid_1's l1: 0.324608\n",
      "[4900]\ttraining's l1: 0.030752\tvalid_1's l1: 0.324446\n",
      "[5000]\ttraining's l1: 0.0296719\tvalid_1's l1: 0.324331\n",
      "[5100]\ttraining's l1: 0.0286533\tvalid_1's l1: 0.324152\n",
      "[5200]\ttraining's l1: 0.027665\tvalid_1's l1: 0.324008\n",
      "[5300]\ttraining's l1: 0.0267391\tvalid_1's l1: 0.323889\n",
      "[5400]\ttraining's l1: 0.0257889\tvalid_1's l1: 0.323775\n",
      "[5500]\ttraining's l1: 0.0249071\tvalid_1's l1: 0.323616\n",
      "[5600]\ttraining's l1: 0.0240859\tvalid_1's l1: 0.323522\n",
      "[5700]\ttraining's l1: 0.0233005\tvalid_1's l1: 0.323407\n",
      "[5800]\ttraining's l1: 0.0225992\tvalid_1's l1: 0.323293\n",
      "[5900]\ttraining's l1: 0.0218605\tvalid_1's l1: 0.323206\n",
      "[6000]\ttraining's l1: 0.0211656\tvalid_1's l1: 0.323164\n",
      "[6100]\ttraining's l1: 0.0204798\tvalid_1's l1: 0.323065\n",
      "[6200]\ttraining's l1: 0.0198337\tvalid_1's l1: 0.322989\n",
      "[6300]\ttraining's l1: 0.0192077\tvalid_1's l1: 0.322915\n",
      "[6400]\ttraining's l1: 0.018604\tvalid_1's l1: 0.322876\n",
      "[6500]\ttraining's l1: 0.0180136\tvalid_1's l1: 0.322836\n",
      "[6600]\ttraining's l1: 0.0174594\tvalid_1's l1: 0.322798\n",
      "[6700]\ttraining's l1: 0.0169131\tvalid_1's l1: 0.322748\n",
      "[6800]\ttraining's l1: 0.0163863\tvalid_1's l1: 0.322676\n",
      "[6900]\ttraining's l1: 0.0159015\tvalid_1's l1: 0.322645\n",
      "[7000]\ttraining's l1: 0.0154273\tvalid_1's l1: 0.322561\n",
      "[7100]\ttraining's l1: 0.0149627\tvalid_1's l1: 0.322521\n",
      "[7200]\ttraining's l1: 0.0145153\tvalid_1's l1: 0.322494\n",
      "[7300]\ttraining's l1: 0.0140721\tvalid_1's l1: 0.322447\n",
      "[7400]\ttraining's l1: 0.0136593\tvalid_1's l1: 0.322421\n",
      "[7500]\ttraining's l1: 0.0132637\tvalid_1's l1: 0.322377\n",
      "[7600]\ttraining's l1: 0.0128766\tvalid_1's l1: 0.322332\n",
      "[7700]\ttraining's l1: 0.0125028\tvalid_1's l1: 0.322311\n",
      "[7800]\ttraining's l1: 0.0121405\tvalid_1's l1: 0.322287\n",
      "[7900]\ttraining's l1: 0.0117693\tvalid_1's l1: 0.322257\n",
      "[8000]\ttraining's l1: 0.0114389\tvalid_1's l1: 0.322226\n",
      "[8100]\ttraining's l1: 0.011112\tvalid_1's l1: 0.322191\n",
      "[8200]\ttraining's l1: 0.0107965\tvalid_1's l1: 0.322168\n",
      "[8300]\ttraining's l1: 0.0104871\tvalid_1's l1: 0.32213\n",
      "[8400]\ttraining's l1: 0.0101895\tvalid_1's l1: 0.322089\n",
      "[8500]\ttraining's l1: 0.00991586\tvalid_1's l1: 0.322055\n",
      "[8600]\ttraining's l1: 0.00962656\tvalid_1's l1: 0.322012\n",
      "[8700]\ttraining's l1: 0.00935674\tvalid_1's l1: 0.321991\n",
      "[8800]\ttraining's l1: 0.00908101\tvalid_1's l1: 0.321978\n",
      "[8900]\ttraining's l1: 0.00883985\tvalid_1's l1: 0.321947\n",
      "[9000]\ttraining's l1: 0.00859783\tvalid_1's l1: 0.321928\n",
      "[9100]\ttraining's l1: 0.00836212\tvalid_1's l1: 0.321904\n",
      "[9200]\ttraining's l1: 0.0081327\tvalid_1's l1: 0.321882\n",
      "[9300]\ttraining's l1: 0.00791953\tvalid_1's l1: 0.321879\n",
      "[9400]\ttraining's l1: 0.00770977\tvalid_1's l1: 0.321868\n",
      "[9500]\ttraining's l1: 0.00750767\tvalid_1's l1: 0.32185\n",
      "[9600]\ttraining's l1: 0.00730282\tvalid_1's l1: 0.321835\n",
      "[9700]\ttraining's l1: 0.00711288\tvalid_1's l1: 0.321813\n",
      "[9800]\ttraining's l1: 0.00693338\tvalid_1's l1: 0.321813\n",
      "[9900]\ttraining's l1: 0.00675437\tvalid_1's l1: 0.321802\n",
      "[10000]\ttraining's l1: 0.0065886\tvalid_1's l1: 0.321784\n",
      "[10100]\ttraining's l1: 0.00641238\tvalid_1's l1: 0.321768\n",
      "[10200]\ttraining's l1: 0.00625087\tvalid_1's l1: 0.321762\n",
      "[10300]\ttraining's l1: 0.00608523\tvalid_1's l1: 0.321753\n",
      "[10400]\ttraining's l1: 0.00592637\tvalid_1's l1: 0.321736\n",
      "[10500]\ttraining's l1: 0.00577082\tvalid_1's l1: 0.32173\n",
      "[10600]\ttraining's l1: 0.00562738\tvalid_1's l1: 0.321721\n",
      "[10700]\ttraining's l1: 0.00548458\tvalid_1's l1: 0.321717\n",
      "[10800]\ttraining's l1: 0.00534064\tvalid_1's l1: 0.321713\n",
      "[10900]\ttraining's l1: 0.00520063\tvalid_1's l1: 0.321707\n",
      "[11000]\ttraining's l1: 0.00506581\tvalid_1's l1: 0.321698\n",
      "[11100]\ttraining's l1: 0.004937\tvalid_1's l1: 0.321695\n",
      "[11200]\ttraining's l1: 0.0048256\tvalid_1's l1: 0.321686\n",
      "[11300]\ttraining's l1: 0.00470632\tvalid_1's l1: 0.321684\n",
      "[11400]\ttraining's l1: 0.00459188\tvalid_1's l1: 0.32168\n",
      "[11500]\ttraining's l1: 0.00448121\tvalid_1's l1: 0.321674\n",
      "[11600]\ttraining's l1: 0.00437765\tvalid_1's l1: 0.321669\n",
      "[11700]\ttraining's l1: 0.00426602\tvalid_1's l1: 0.321668\n",
      "[11800]\ttraining's l1: 0.00416336\tvalid_1's l1: 0.321659\n",
      "[11900]\ttraining's l1: 0.00405837\tvalid_1's l1: 0.321664\n",
      "[12000]\ttraining's l1: 0.00395962\tvalid_1's l1: 0.321659\n",
      "[12100]\ttraining's l1: 0.0038628\tvalid_1's l1: 0.321648\n",
      "[12200]\ttraining's l1: 0.00376886\tvalid_1's l1: 0.321646\n",
      "[12300]\ttraining's l1: 0.00367697\tvalid_1's l1: 0.321643\n",
      "[12400]\ttraining's l1: 0.00359012\tvalid_1's l1: 0.321639\n",
      "[12500]\ttraining's l1: 0.00350291\tvalid_1's l1: 0.321632\n",
      "[12600]\ttraining's l1: 0.00341931\tvalid_1's l1: 0.321631\n",
      "[12700]\ttraining's l1: 0.00334301\tvalid_1's l1: 0.321626\n",
      "[12800]\ttraining's l1: 0.00326269\tvalid_1's l1: 0.321619\n",
      "[12900]\ttraining's l1: 0.0031877\tvalid_1's l1: 0.321613\n",
      "[13000]\ttraining's l1: 0.00311559\tvalid_1's l1: 0.32161\n",
      "[13100]\ttraining's l1: 0.0030437\tvalid_1's l1: 0.321607\n",
      "[13200]\ttraining's l1: 0.00297216\tvalid_1's l1: 0.321604\n",
      "[13300]\ttraining's l1: 0.00290678\tvalid_1's l1: 0.321602\n",
      "[13400]\ttraining's l1: 0.00283608\tvalid_1's l1: 0.321596\n",
      "[13500]\ttraining's l1: 0.00277103\tvalid_1's l1: 0.321593\n",
      "[13600]\ttraining's l1: 0.0027105\tvalid_1's l1: 0.32159\n",
      "[13700]\ttraining's l1: 0.0026485\tvalid_1's l1: 0.321587\n",
      "[13800]\ttraining's l1: 0.00259027\tvalid_1's l1: 0.321581\n",
      "[13900]\ttraining's l1: 0.00253594\tvalid_1's l1: 0.321578\n",
      "[14000]\ttraining's l1: 0.00247807\tvalid_1's l1: 0.321576\n",
      "[14100]\ttraining's l1: 0.00242196\tvalid_1's l1: 0.321575\n",
      "[14200]\ttraining's l1: 0.00237087\tvalid_1's l1: 0.321572\n",
      "[14300]\ttraining's l1: 0.00231955\tvalid_1's l1: 0.321569\n",
      "[14400]\ttraining's l1: 0.00226681\tvalid_1's l1: 0.321569\n",
      "[14500]\ttraining's l1: 0.00221588\tvalid_1's l1: 0.321563\n",
      "[14600]\ttraining's l1: 0.00216541\tvalid_1's l1: 0.321559\n",
      "[14700]\ttraining's l1: 0.00211639\tvalid_1's l1: 0.321556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14800]\ttraining's l1: 0.00206967\tvalid_1's l1: 0.321552\n",
      "[14900]\ttraining's l1: 0.00202495\tvalid_1's l1: 0.321546\n",
      "[15000]\ttraining's l1: 0.00197925\tvalid_1's l1: 0.321541\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00197925\tvalid_1's l1: 0.321541\n",
      "1JHN Fold 3, logMAE: -1.134631051876189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.542299\tvalid_1's l1: 0.594727\n",
      "[200]\ttraining's l1: 0.417184\tvalid_1's l1: 0.494472\n",
      "[300]\ttraining's l1: 0.352835\tvalid_1's l1: 0.450002\n",
      "[400]\ttraining's l1: 0.310211\tvalid_1's l1: 0.42515\n",
      "[500]\ttraining's l1: 0.277884\tvalid_1's l1: 0.40858\n",
      "[600]\ttraining's l1: 0.251792\tvalid_1's l1: 0.395746\n",
      "[700]\ttraining's l1: 0.230588\tvalid_1's l1: 0.385981\n",
      "[800]\ttraining's l1: 0.212581\tvalid_1's l1: 0.378146\n",
      "[900]\ttraining's l1: 0.19692\tvalid_1's l1: 0.372039\n",
      "[1000]\ttraining's l1: 0.18366\tvalid_1's l1: 0.367312\n",
      "[1100]\ttraining's l1: 0.171647\tvalid_1's l1: 0.36339\n",
      "[1200]\ttraining's l1: 0.160567\tvalid_1's l1: 0.359444\n",
      "[1300]\ttraining's l1: 0.150619\tvalid_1's l1: 0.356552\n",
      "[1400]\ttraining's l1: 0.14169\tvalid_1's l1: 0.353499\n",
      "[1500]\ttraining's l1: 0.133527\tvalid_1's l1: 0.351447\n",
      "[1600]\ttraining's l1: 0.126252\tvalid_1's l1: 0.349777\n",
      "[1700]\ttraining's l1: 0.119529\tvalid_1's l1: 0.348063\n",
      "[1800]\ttraining's l1: 0.113211\tvalid_1's l1: 0.346794\n",
      "[1900]\ttraining's l1: 0.107357\tvalid_1's l1: 0.345395\n",
      "[2000]\ttraining's l1: 0.101894\tvalid_1's l1: 0.343944\n",
      "[2100]\ttraining's l1: 0.0968617\tvalid_1's l1: 0.342731\n",
      "[2200]\ttraining's l1: 0.092181\tvalid_1's l1: 0.341677\n",
      "[2300]\ttraining's l1: 0.0879579\tvalid_1's l1: 0.340763\n",
      "[2400]\ttraining's l1: 0.0837989\tvalid_1's l1: 0.339853\n",
      "[2500]\ttraining's l1: 0.0799526\tvalid_1's l1: 0.338992\n",
      "[2600]\ttraining's l1: 0.0763802\tvalid_1's l1: 0.338201\n",
      "[2700]\ttraining's l1: 0.072947\tvalid_1's l1: 0.337636\n",
      "[2800]\ttraining's l1: 0.069809\tvalid_1's l1: 0.337019\n",
      "[2900]\ttraining's l1: 0.0667637\tvalid_1's l1: 0.336445\n",
      "[3000]\ttraining's l1: 0.0638242\tvalid_1's l1: 0.335929\n",
      "[3100]\ttraining's l1: 0.0611553\tvalid_1's l1: 0.335536\n",
      "[3200]\ttraining's l1: 0.0587081\tvalid_1's l1: 0.335109\n",
      "[3300]\ttraining's l1: 0.0562659\tvalid_1's l1: 0.334772\n",
      "[3400]\ttraining's l1: 0.0540047\tvalid_1's l1: 0.334313\n",
      "[3500]\ttraining's l1: 0.051986\tvalid_1's l1: 0.334086\n",
      "[3600]\ttraining's l1: 0.0499645\tvalid_1's l1: 0.333808\n",
      "[3700]\ttraining's l1: 0.0480301\tvalid_1's l1: 0.333552\n",
      "[3800]\ttraining's l1: 0.0461937\tvalid_1's l1: 0.333301\n",
      "[3900]\ttraining's l1: 0.0444265\tvalid_1's l1: 0.333008\n",
      "[4000]\ttraining's l1: 0.0427674\tvalid_1's l1: 0.33275\n",
      "[4100]\ttraining's l1: 0.0412127\tvalid_1's l1: 0.332582\n",
      "[4200]\ttraining's l1: 0.0396603\tvalid_1's l1: 0.332403\n",
      "[4300]\ttraining's l1: 0.0381979\tvalid_1's l1: 0.332135\n",
      "[4400]\ttraining's l1: 0.0368502\tvalid_1's l1: 0.331908\n",
      "[4500]\ttraining's l1: 0.0355128\tvalid_1's l1: 0.331792\n",
      "[4600]\ttraining's l1: 0.0342301\tvalid_1's l1: 0.331629\n",
      "[4700]\ttraining's l1: 0.0330217\tvalid_1's l1: 0.331504\n",
      "[4800]\ttraining's l1: 0.0319063\tvalid_1's l1: 0.331395\n",
      "[4900]\ttraining's l1: 0.0308257\tvalid_1's l1: 0.331213\n",
      "[5000]\ttraining's l1: 0.0297785\tvalid_1's l1: 0.331116\n",
      "[5100]\ttraining's l1: 0.0287231\tvalid_1's l1: 0.33101\n",
      "[5200]\ttraining's l1: 0.0277037\tvalid_1's l1: 0.33089\n",
      "[5300]\ttraining's l1: 0.0268034\tvalid_1's l1: 0.330802\n",
      "[5400]\ttraining's l1: 0.0258775\tvalid_1's l1: 0.33069\n",
      "[5500]\ttraining's l1: 0.0249858\tvalid_1's l1: 0.330559\n",
      "[5600]\ttraining's l1: 0.0241456\tvalid_1's l1: 0.330482\n",
      "[5700]\ttraining's l1: 0.0233604\tvalid_1's l1: 0.330389\n",
      "[5800]\ttraining's l1: 0.0226089\tvalid_1's l1: 0.330306\n",
      "[5900]\ttraining's l1: 0.0218794\tvalid_1's l1: 0.330221\n",
      "[6000]\ttraining's l1: 0.0211852\tvalid_1's l1: 0.330151\n",
      "[6100]\ttraining's l1: 0.020536\tvalid_1's l1: 0.330086\n",
      "[6200]\ttraining's l1: 0.0198917\tvalid_1's l1: 0.330002\n",
      "[6300]\ttraining's l1: 0.0192737\tvalid_1's l1: 0.329905\n",
      "[6400]\ttraining's l1: 0.0186585\tvalid_1's l1: 0.329834\n",
      "[6500]\ttraining's l1: 0.0180847\tvalid_1's l1: 0.329776\n",
      "[6600]\ttraining's l1: 0.0175078\tvalid_1's l1: 0.329717\n",
      "[6700]\ttraining's l1: 0.0169524\tvalid_1's l1: 0.329647\n",
      "[6800]\ttraining's l1: 0.016445\tvalid_1's l1: 0.329567\n",
      "[6900]\ttraining's l1: 0.0159732\tvalid_1's l1: 0.3295\n",
      "[7000]\ttraining's l1: 0.0154797\tvalid_1's l1: 0.32944\n",
      "[7100]\ttraining's l1: 0.0150151\tvalid_1's l1: 0.32939\n",
      "[7200]\ttraining's l1: 0.0145846\tvalid_1's l1: 0.329348\n",
      "[7300]\ttraining's l1: 0.0141445\tvalid_1's l1: 0.32932\n",
      "[7400]\ttraining's l1: 0.0137231\tvalid_1's l1: 0.329296\n",
      "[7500]\ttraining's l1: 0.0133048\tvalid_1's l1: 0.329238\n",
      "[7600]\ttraining's l1: 0.0129133\tvalid_1's l1: 0.329201\n",
      "[7700]\ttraining's l1: 0.0125389\tvalid_1's l1: 0.329162\n",
      "[7800]\ttraining's l1: 0.012192\tvalid_1's l1: 0.329122\n",
      "[7900]\ttraining's l1: 0.011849\tvalid_1's l1: 0.329067\n",
      "[8000]\ttraining's l1: 0.011507\tvalid_1's l1: 0.329045\n",
      "[8100]\ttraining's l1: 0.0111772\tvalid_1's l1: 0.328998\n",
      "[8200]\ttraining's l1: 0.0108505\tvalid_1's l1: 0.328974\n",
      "[8300]\ttraining's l1: 0.0105408\tvalid_1's l1: 0.328947\n",
      "[8400]\ttraining's l1: 0.0102313\tvalid_1's l1: 0.328905\n",
      "[8500]\ttraining's l1: 0.00994224\tvalid_1's l1: 0.328891\n",
      "[8600]\ttraining's l1: 0.00966255\tvalid_1's l1: 0.328856\n",
      "[8700]\ttraining's l1: 0.00938889\tvalid_1's l1: 0.328815\n",
      "[8800]\ttraining's l1: 0.00913352\tvalid_1's l1: 0.328792\n",
      "[8900]\ttraining's l1: 0.00887777\tvalid_1's l1: 0.328769\n",
      "[9000]\ttraining's l1: 0.00862871\tvalid_1's l1: 0.328742\n",
      "[9100]\ttraining's l1: 0.00839499\tvalid_1's l1: 0.328708\n",
      "[9200]\ttraining's l1: 0.00817003\tvalid_1's l1: 0.328671\n",
      "[9300]\ttraining's l1: 0.0079595\tvalid_1's l1: 0.328662\n",
      "[9400]\ttraining's l1: 0.00773973\tvalid_1's l1: 0.328641\n",
      "[9500]\ttraining's l1: 0.00752898\tvalid_1's l1: 0.328619\n",
      "[9600]\ttraining's l1: 0.00732799\tvalid_1's l1: 0.328594\n",
      "[9700]\ttraining's l1: 0.00714041\tvalid_1's l1: 0.32858\n",
      "[9800]\ttraining's l1: 0.00695648\tvalid_1's l1: 0.32857\n",
      "[9900]\ttraining's l1: 0.00678148\tvalid_1's l1: 0.328568\n",
      "[10000]\ttraining's l1: 0.00659989\tvalid_1's l1: 0.328553\n",
      "[10100]\ttraining's l1: 0.00642805\tvalid_1's l1: 0.328534\n",
      "[10200]\ttraining's l1: 0.00624978\tvalid_1's l1: 0.328507\n",
      "[10300]\ttraining's l1: 0.00608683\tvalid_1's l1: 0.328492\n",
      "[10400]\ttraining's l1: 0.00592691\tvalid_1's l1: 0.328478\n",
      "[10500]\ttraining's l1: 0.00577986\tvalid_1's l1: 0.328466\n",
      "[10600]\ttraining's l1: 0.00563114\tvalid_1's l1: 0.32845\n",
      "[10700]\ttraining's l1: 0.0054911\tvalid_1's l1: 0.328435\n",
      "[10800]\ttraining's l1: 0.00535375\tvalid_1's l1: 0.32842\n",
      "[10900]\ttraining's l1: 0.00522418\tvalid_1's l1: 0.328408\n",
      "[11000]\ttraining's l1: 0.00509179\tvalid_1's l1: 0.328399\n",
      "[11100]\ttraining's l1: 0.00496826\tvalid_1's l1: 0.328386\n",
      "[11200]\ttraining's l1: 0.00484182\tvalid_1's l1: 0.328369\n",
      "[11300]\ttraining's l1: 0.00472149\tvalid_1's l1: 0.328352\n",
      "[11400]\ttraining's l1: 0.0046005\tvalid_1's l1: 0.328348\n",
      "[11500]\ttraining's l1: 0.00448625\tvalid_1's l1: 0.328336\n",
      "[11600]\ttraining's l1: 0.00437981\tvalid_1's l1: 0.328325\n",
      "[11700]\ttraining's l1: 0.00427128\tvalid_1's l1: 0.328317\n",
      "[11800]\ttraining's l1: 0.0041705\tvalid_1's l1: 0.328309\n",
      "[11900]\ttraining's l1: 0.00407034\tvalid_1's l1: 0.328295\n",
      "[12000]\ttraining's l1: 0.00397126\tvalid_1's l1: 0.328286\n",
      "[12100]\ttraining's l1: 0.00387955\tvalid_1's l1: 0.328281\n",
      "[12200]\ttraining's l1: 0.0037883\tvalid_1's l1: 0.328276\n",
      "[12300]\ttraining's l1: 0.00369841\tvalid_1's l1: 0.328269\n",
      "[12400]\ttraining's l1: 0.00361145\tvalid_1's l1: 0.328269\n",
      "[12500]\ttraining's l1: 0.00352366\tvalid_1's l1: 0.328265\n",
      "[12600]\ttraining's l1: 0.0034339\tvalid_1's l1: 0.328257\n",
      "[12700]\ttraining's l1: 0.00335066\tvalid_1's l1: 0.32825\n",
      "[12800]\ttraining's l1: 0.00327038\tvalid_1's l1: 0.328243\n",
      "[12900]\ttraining's l1: 0.00319321\tvalid_1's l1: 0.328238\n",
      "[13000]\ttraining's l1: 0.00311735\tvalid_1's l1: 0.328231\n",
      "[13100]\ttraining's l1: 0.00304471\tvalid_1's l1: 0.328226\n",
      "[13200]\ttraining's l1: 0.00297478\tvalid_1's l1: 0.328222\n",
      "[13300]\ttraining's l1: 0.00290289\tvalid_1's l1: 0.328217\n",
      "[13400]\ttraining's l1: 0.00283255\tvalid_1's l1: 0.32821\n",
      "[13500]\ttraining's l1: 0.0027665\tvalid_1's l1: 0.328208\n",
      "[13600]\ttraining's l1: 0.00270332\tvalid_1's l1: 0.328204\n",
      "[13700]\ttraining's l1: 0.00264228\tvalid_1's l1: 0.328199\n",
      "[13800]\ttraining's l1: 0.0025833\tvalid_1's l1: 0.328196\n",
      "[13900]\ttraining's l1: 0.00252761\tvalid_1's l1: 0.328194\n",
      "[14000]\ttraining's l1: 0.00247057\tvalid_1's l1: 0.328191\n",
      "[14100]\ttraining's l1: 0.00241422\tvalid_1's l1: 0.328187\n",
      "[14200]\ttraining's l1: 0.00236066\tvalid_1's l1: 0.328181\n",
      "[14300]\ttraining's l1: 0.00230648\tvalid_1's l1: 0.328176\n",
      "[14400]\ttraining's l1: 0.00225485\tvalid_1's l1: 0.328173\n",
      "[14500]\ttraining's l1: 0.00220549\tvalid_1's l1: 0.32817\n",
      "[14600]\ttraining's l1: 0.00215748\tvalid_1's l1: 0.328166\n",
      "[14700]\ttraining's l1: 0.00210904\tvalid_1's l1: 0.328159\n",
      "[14800]\ttraining's l1: 0.00206517\tvalid_1's l1: 0.328156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14900]\ttraining's l1: 0.00202127\tvalid_1's l1: 0.328152\n",
      "[15000]\ttraining's l1: 0.00197866\tvalid_1's l1: 0.328149\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00197866\tvalid_1's l1: 0.328149\n",
      "1JHN Fold 4, logMAE: -1.1142873188537183\n",
      "*** Training Model for 1JHC ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 1.66051\tvalid_1's l1: 1.67471\n",
      "[200]\ttraining's l1: 1.41447\tvalid_1's l1: 1.43559\n",
      "[300]\ttraining's l1: 1.29766\tvalid_1's l1: 1.32463\n",
      "[400]\ttraining's l1: 1.21948\tvalid_1's l1: 1.25288\n",
      "[500]\ttraining's l1: 1.16105\tvalid_1's l1: 1.20039\n",
      "[600]\ttraining's l1: 1.11565\tvalid_1's l1: 1.16061\n",
      "[700]\ttraining's l1: 1.07602\tvalid_1's l1: 1.1264\n",
      "[800]\ttraining's l1: 1.04421\tvalid_1's l1: 1.09958\n",
      "[900]\ttraining's l1: 1.01476\tvalid_1's l1: 1.07531\n",
      "[1000]\ttraining's l1: 0.987843\tvalid_1's l1: 1.05333\n",
      "[1100]\ttraining's l1: 0.964155\tvalid_1's l1: 1.03436\n",
      "[1200]\ttraining's l1: 0.942556\tvalid_1's l1: 1.01773\n",
      "[1300]\ttraining's l1: 0.923184\tvalid_1's l1: 1.00292\n",
      "[1400]\ttraining's l1: 0.905723\tvalid_1's l1: 0.989946\n",
      "[1500]\ttraining's l1: 0.889057\tvalid_1's l1: 0.977886\n",
      "[1600]\ttraining's l1: 0.873192\tvalid_1's l1: 0.966301\n",
      "[1700]\ttraining's l1: 0.857542\tvalid_1's l1: 0.954942\n",
      "[1800]\ttraining's l1: 0.843985\tvalid_1's l1: 0.945161\n",
      "[1900]\ttraining's l1: 0.830913\tvalid_1's l1: 0.93591\n",
      "[2000]\ttraining's l1: 0.8183\tvalid_1's l1: 0.927236\n",
      "[2100]\ttraining's l1: 0.806333\tvalid_1's l1: 0.919089\n",
      "[2200]\ttraining's l1: 0.794675\tvalid_1's l1: 0.910847\n",
      "[2300]\ttraining's l1: 0.784075\tvalid_1's l1: 0.903784\n",
      "[2400]\ttraining's l1: 0.773478\tvalid_1's l1: 0.896526\n",
      "[2500]\ttraining's l1: 0.763531\tvalid_1's l1: 0.889826\n",
      "[2600]\ttraining's l1: 0.754004\tvalid_1's l1: 0.883645\n",
      "[2700]\ttraining's l1: 0.74469\tvalid_1's l1: 0.877567\n",
      "[2800]\ttraining's l1: 0.736215\tvalid_1's l1: 0.871982\n",
      "[2900]\ttraining's l1: 0.727657\tvalid_1's l1: 0.866645\n",
      "[3000]\ttraining's l1: 0.719241\tvalid_1's l1: 0.86154\n",
      "[3100]\ttraining's l1: 0.710962\tvalid_1's l1: 0.856275\n",
      "[3200]\ttraining's l1: 0.702926\tvalid_1's l1: 0.851319\n",
      "[3300]\ttraining's l1: 0.695287\tvalid_1's l1: 0.846506\n",
      "[3400]\ttraining's l1: 0.687686\tvalid_1's l1: 0.841841\n",
      "[3500]\ttraining's l1: 0.680908\tvalid_1's l1: 0.83794\n",
      "[3600]\ttraining's l1: 0.674136\tvalid_1's l1: 0.834124\n",
      "[3700]\ttraining's l1: 0.667607\tvalid_1's l1: 0.830388\n",
      "[3800]\ttraining's l1: 0.661042\tvalid_1's l1: 0.826639\n",
      "[3900]\ttraining's l1: 0.654617\tvalid_1's l1: 0.822972\n",
      "[4000]\ttraining's l1: 0.64846\tvalid_1's l1: 0.819363\n",
      "[4100]\ttraining's l1: 0.642771\tvalid_1's l1: 0.816396\n",
      "[4200]\ttraining's l1: 0.637172\tvalid_1's l1: 0.81333\n",
      "[4300]\ttraining's l1: 0.631001\tvalid_1's l1: 0.809592\n",
      "[4400]\ttraining's l1: 0.625226\tvalid_1's l1: 0.806369\n",
      "[4500]\ttraining's l1: 0.619862\tvalid_1's l1: 0.803431\n",
      "[4600]\ttraining's l1: 0.614436\tvalid_1's l1: 0.800485\n",
      "[4700]\ttraining's l1: 0.609127\tvalid_1's l1: 0.797582\n",
      "[4800]\ttraining's l1: 0.603822\tvalid_1's l1: 0.794785\n",
      "[4900]\ttraining's l1: 0.598563\tvalid_1's l1: 0.792133\n",
      "[5000]\ttraining's l1: 0.593575\tvalid_1's l1: 0.789472\n",
      "[5100]\ttraining's l1: 0.588805\tvalid_1's l1: 0.787032\n",
      "[5200]\ttraining's l1: 0.584239\tvalid_1's l1: 0.784714\n",
      "[5300]\ttraining's l1: 0.579613\tvalid_1's l1: 0.782369\n",
      "[5400]\ttraining's l1: 0.574992\tvalid_1's l1: 0.780072\n",
      "[5500]\ttraining's l1: 0.570345\tvalid_1's l1: 0.777738\n",
      "[5600]\ttraining's l1: 0.566018\tvalid_1's l1: 0.775485\n",
      "[5700]\ttraining's l1: 0.561861\tvalid_1's l1: 0.773473\n",
      "[5800]\ttraining's l1: 0.557569\tvalid_1's l1: 0.77138\n",
      "[5900]\ttraining's l1: 0.553579\tvalid_1's l1: 0.769482\n",
      "[6000]\ttraining's l1: 0.549581\tvalid_1's l1: 0.767504\n",
      "[6100]\ttraining's l1: 0.545535\tvalid_1's l1: 0.765522\n",
      "[6200]\ttraining's l1: 0.541563\tvalid_1's l1: 0.763532\n",
      "[6300]\ttraining's l1: 0.537845\tvalid_1's l1: 0.76179\n",
      "[6400]\ttraining's l1: 0.534102\tvalid_1's l1: 0.759976\n",
      "[6500]\ttraining's l1: 0.530399\tvalid_1's l1: 0.758318\n",
      "[6600]\ttraining's l1: 0.526784\tvalid_1's l1: 0.756603\n",
      "[6700]\ttraining's l1: 0.523102\tvalid_1's l1: 0.754785\n",
      "[6800]\ttraining's l1: 0.519371\tvalid_1's l1: 0.753002\n",
      "[6900]\ttraining's l1: 0.515954\tvalid_1's l1: 0.751499\n",
      "[7000]\ttraining's l1: 0.51241\tvalid_1's l1: 0.749811\n",
      "[7100]\ttraining's l1: 0.509072\tvalid_1's l1: 0.748279\n",
      "[7200]\ttraining's l1: 0.505963\tvalid_1's l1: 0.746877\n",
      "[7300]\ttraining's l1: 0.502577\tvalid_1's l1: 0.745247\n",
      "[7400]\ttraining's l1: 0.499305\tvalid_1's l1: 0.743825\n",
      "[7500]\ttraining's l1: 0.496153\tvalid_1's l1: 0.742372\n",
      "[7600]\ttraining's l1: 0.492822\tvalid_1's l1: 0.741001\n",
      "[7700]\ttraining's l1: 0.489594\tvalid_1's l1: 0.739707\n",
      "[7800]\ttraining's l1: 0.486472\tvalid_1's l1: 0.73831\n",
      "[7900]\ttraining's l1: 0.483331\tvalid_1's l1: 0.736941\n",
      "[8000]\ttraining's l1: 0.480297\tvalid_1's l1: 0.735547\n",
      "[8100]\ttraining's l1: 0.477369\tvalid_1's l1: 0.73432\n",
      "[8200]\ttraining's l1: 0.474424\tvalid_1's l1: 0.733052\n",
      "[8300]\ttraining's l1: 0.471508\tvalid_1's l1: 0.73187\n",
      "[8400]\ttraining's l1: 0.468654\tvalid_1's l1: 0.730691\n",
      "[8500]\ttraining's l1: 0.465691\tvalid_1's l1: 0.729464\n",
      "[8600]\ttraining's l1: 0.462852\tvalid_1's l1: 0.728245\n",
      "[8700]\ttraining's l1: 0.460155\tvalid_1's l1: 0.727162\n",
      "[8800]\ttraining's l1: 0.457493\tvalid_1's l1: 0.72612\n",
      "[8900]\ttraining's l1: 0.454789\tvalid_1's l1: 0.725036\n",
      "[9000]\ttraining's l1: 0.452108\tvalid_1's l1: 0.72401\n",
      "[9100]\ttraining's l1: 0.449586\tvalid_1's l1: 0.723071\n",
      "[9200]\ttraining's l1: 0.447017\tvalid_1's l1: 0.721975\n",
      "[9300]\ttraining's l1: 0.444523\tvalid_1's l1: 0.720975\n",
      "[9400]\ttraining's l1: 0.441984\tvalid_1's l1: 0.72001\n",
      "[9500]\ttraining's l1: 0.439588\tvalid_1's l1: 0.719148\n",
      "[9600]\ttraining's l1: 0.43708\tvalid_1's l1: 0.718204\n",
      "[9700]\ttraining's l1: 0.434465\tvalid_1's l1: 0.717122\n",
      "[9800]\ttraining's l1: 0.431936\tvalid_1's l1: 0.716077\n",
      "[9900]\ttraining's l1: 0.429557\tvalid_1's l1: 0.715152\n",
      "[10000]\ttraining's l1: 0.427134\tvalid_1's l1: 0.714193\n",
      "[10100]\ttraining's l1: 0.424729\tvalid_1's l1: 0.713301\n",
      "[10200]\ttraining's l1: 0.422432\tvalid_1's l1: 0.712429\n",
      "[10300]\ttraining's l1: 0.42015\tvalid_1's l1: 0.711631\n",
      "[10400]\ttraining's l1: 0.417971\tvalid_1's l1: 0.710795\n",
      "[10500]\ttraining's l1: 0.415628\tvalid_1's l1: 0.709843\n",
      "[10600]\ttraining's l1: 0.413386\tvalid_1's l1: 0.709047\n",
      "[10700]\ttraining's l1: 0.411217\tvalid_1's l1: 0.708249\n",
      "[10800]\ttraining's l1: 0.409024\tvalid_1's l1: 0.707423\n",
      "[10900]\ttraining's l1: 0.406825\tvalid_1's l1: 0.706678\n",
      "[11000]\ttraining's l1: 0.404638\tvalid_1's l1: 0.705863\n",
      "[11100]\ttraining's l1: 0.402504\tvalid_1's l1: 0.705099\n",
      "[11200]\ttraining's l1: 0.400401\tvalid_1's l1: 0.704332\n",
      "[11300]\ttraining's l1: 0.398332\tvalid_1's l1: 0.703556\n",
      "[11400]\ttraining's l1: 0.396253\tvalid_1's l1: 0.702748\n",
      "[11500]\ttraining's l1: 0.394133\tvalid_1's l1: 0.701919\n",
      "[11600]\ttraining's l1: 0.392087\tvalid_1's l1: 0.701217\n",
      "[11700]\ttraining's l1: 0.390059\tvalid_1's l1: 0.700511\n",
      "[11800]\ttraining's l1: 0.388034\tvalid_1's l1: 0.699786\n",
      "[11900]\ttraining's l1: 0.386076\tvalid_1's l1: 0.699068\n",
      "[12000]\ttraining's l1: 0.384086\tvalid_1's l1: 0.698414\n",
      "[12100]\ttraining's l1: 0.382133\tvalid_1's l1: 0.697771\n",
      "[12200]\ttraining's l1: 0.380231\tvalid_1's l1: 0.697147\n",
      "[12300]\ttraining's l1: 0.378238\tvalid_1's l1: 0.696474\n",
      "[12400]\ttraining's l1: 0.37633\tvalid_1's l1: 0.695799\n",
      "[12500]\ttraining's l1: 0.374477\tvalid_1's l1: 0.695088\n",
      "[12600]\ttraining's l1: 0.372685\tvalid_1's l1: 0.694505\n",
      "[12700]\ttraining's l1: 0.370813\tvalid_1's l1: 0.693845\n",
      "[12800]\ttraining's l1: 0.368994\tvalid_1's l1: 0.693217\n",
      "[12900]\ttraining's l1: 0.367198\tvalid_1's l1: 0.692573\n",
      "[13000]\ttraining's l1: 0.365449\tvalid_1's l1: 0.691981\n",
      "[13100]\ttraining's l1: 0.363703\tvalid_1's l1: 0.691428\n",
      "[13200]\ttraining's l1: 0.361957\tvalid_1's l1: 0.690867\n",
      "[13300]\ttraining's l1: 0.360223\tvalid_1's l1: 0.690351\n",
      "[13400]\ttraining's l1: 0.358476\tvalid_1's l1: 0.689704\n",
      "[13500]\ttraining's l1: 0.356808\tvalid_1's l1: 0.689133\n",
      "[13600]\ttraining's l1: 0.355133\tvalid_1's l1: 0.68856\n",
      "[13700]\ttraining's l1: 0.353408\tvalid_1's l1: 0.687951\n",
      "[13800]\ttraining's l1: 0.35171\tvalid_1's l1: 0.687343\n",
      "[13900]\ttraining's l1: 0.350056\tvalid_1's l1: 0.686815\n",
      "[14000]\ttraining's l1: 0.348499\tvalid_1's l1: 0.686352\n",
      "[14100]\ttraining's l1: 0.34688\tvalid_1's l1: 0.685823\n",
      "[14200]\ttraining's l1: 0.345368\tvalid_1's l1: 0.685423\n",
      "[14300]\ttraining's l1: 0.343773\tvalid_1's l1: 0.684939\n",
      "[14400]\ttraining's l1: 0.342192\tvalid_1's l1: 0.684356\n",
      "[14500]\ttraining's l1: 0.340664\tvalid_1's l1: 0.683862\n",
      "[14600]\ttraining's l1: 0.339128\tvalid_1's l1: 0.68339\n",
      "[14700]\ttraining's l1: 0.337583\tvalid_1's l1: 0.682894\n",
      "[14800]\ttraining's l1: 0.33608\tvalid_1's l1: 0.682408\n",
      "[14900]\ttraining's l1: 0.334546\tvalid_1's l1: 0.681944\n",
      "[15000]\ttraining's l1: 0.333057\tvalid_1's l1: 0.681495\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.333057\tvalid_1's l1: 0.681495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1JHC Fold 0, logMAE: -0.38346623764805493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 1.65115\tvalid_1's l1: 1.66903\n",
      "[200]\ttraining's l1: 1.41365\tvalid_1's l1: 1.438\n",
      "[300]\ttraining's l1: 1.2992\tvalid_1's l1: 1.32897\n",
      "[400]\ttraining's l1: 1.22386\tvalid_1's l1: 1.25889\n",
      "[500]\ttraining's l1: 1.16431\tvalid_1's l1: 1.20566\n",
      "[600]\ttraining's l1: 1.11829\tvalid_1's l1: 1.16507\n",
      "[700]\ttraining's l1: 1.0811\tvalid_1's l1: 1.13333\n",
      "[800]\ttraining's l1: 1.04406\tvalid_1's l1: 1.10139\n",
      "[900]\ttraining's l1: 1.01431\tvalid_1's l1: 1.0766\n",
      "[1000]\ttraining's l1: 0.988869\tvalid_1's l1: 1.0558\n",
      "[1100]\ttraining's l1: 0.965882\tvalid_1's l1: 1.03711\n",
      "[1200]\ttraining's l1: 0.943448\tvalid_1's l1: 1.01935\n",
      "[1300]\ttraining's l1: 0.92285\tvalid_1's l1: 1.00347\n",
      "[1400]\ttraining's l1: 0.905338\tvalid_1's l1: 0.990023\n",
      "[1500]\ttraining's l1: 0.887401\tvalid_1's l1: 0.976222\n",
      "[1600]\ttraining's l1: 0.871979\tvalid_1's l1: 0.964761\n",
      "[1700]\ttraining's l1: 0.856829\tvalid_1's l1: 0.953565\n",
      "[1800]\ttraining's l1: 0.842956\tvalid_1's l1: 0.94349\n",
      "[1900]\ttraining's l1: 0.830298\tvalid_1's l1: 0.93469\n",
      "[2000]\ttraining's l1: 0.817729\tvalid_1's l1: 0.925983\n",
      "[2100]\ttraining's l1: 0.805454\tvalid_1's l1: 0.917012\n",
      "[2200]\ttraining's l1: 0.794361\tvalid_1's l1: 0.909327\n",
      "[2300]\ttraining's l1: 0.783726\tvalid_1's l1: 0.902115\n",
      "[2400]\ttraining's l1: 0.772737\tvalid_1's l1: 0.894273\n",
      "[2500]\ttraining's l1: 0.762222\tvalid_1's l1: 0.887102\n",
      "[2600]\ttraining's l1: 0.752322\tvalid_1's l1: 0.880442\n",
      "[2700]\ttraining's l1: 0.743333\tvalid_1's l1: 0.874633\n",
      "[2800]\ttraining's l1: 0.734342\tvalid_1's l1: 0.868942\n",
      "[2900]\ttraining's l1: 0.725657\tvalid_1's l1: 0.863323\n",
      "[3000]\ttraining's l1: 0.717903\tvalid_1's l1: 0.858488\n",
      "[3100]\ttraining's l1: 0.709862\tvalid_1's l1: 0.853481\n",
      "[3200]\ttraining's l1: 0.702117\tvalid_1's l1: 0.848642\n",
      "[3300]\ttraining's l1: 0.694624\tvalid_1's l1: 0.843953\n",
      "[3400]\ttraining's l1: 0.687172\tvalid_1's l1: 0.839133\n",
      "[3500]\ttraining's l1: 0.6801\tvalid_1's l1: 0.834887\n",
      "[3600]\ttraining's l1: 0.67325\tvalid_1's l1: 0.830806\n",
      "[3700]\ttraining's l1: 0.666524\tvalid_1's l1: 0.826681\n",
      "[3800]\ttraining's l1: 0.660084\tvalid_1's l1: 0.822856\n",
      "[3900]\ttraining's l1: 0.653718\tvalid_1's l1: 0.819025\n",
      "[4000]\ttraining's l1: 0.648038\tvalid_1's l1: 0.815923\n",
      "[4100]\ttraining's l1: 0.641999\tvalid_1's l1: 0.812378\n",
      "[4200]\ttraining's l1: 0.636153\tvalid_1's l1: 0.809165\n",
      "[4300]\ttraining's l1: 0.630269\tvalid_1's l1: 0.805827\n",
      "[4400]\ttraining's l1: 0.624742\tvalid_1's l1: 0.802819\n",
      "[4500]\ttraining's l1: 0.619237\tvalid_1's l1: 0.799872\n",
      "[4600]\ttraining's l1: 0.614088\tvalid_1's l1: 0.797129\n",
      "[4700]\ttraining's l1: 0.609086\tvalid_1's l1: 0.794486\n",
      "[4800]\ttraining's l1: 0.603976\tvalid_1's l1: 0.791631\n",
      "[4900]\ttraining's l1: 0.598855\tvalid_1's l1: 0.788929\n",
      "[5000]\ttraining's l1: 0.594056\tvalid_1's l1: 0.786489\n",
      "[5100]\ttraining's l1: 0.589467\tvalid_1's l1: 0.784197\n",
      "[5200]\ttraining's l1: 0.584601\tvalid_1's l1: 0.781466\n",
      "[5300]\ttraining's l1: 0.579902\tvalid_1's l1: 0.77898\n",
      "[5400]\ttraining's l1: 0.575417\tvalid_1's l1: 0.776601\n",
      "[5500]\ttraining's l1: 0.570894\tvalid_1's l1: 0.774243\n",
      "[5600]\ttraining's l1: 0.566541\tvalid_1's l1: 0.772171\n",
      "[5700]\ttraining's l1: 0.562201\tvalid_1's l1: 0.76992\n",
      "[5800]\ttraining's l1: 0.55818\tvalid_1's l1: 0.767976\n",
      "[5900]\ttraining's l1: 0.554048\tvalid_1's l1: 0.765834\n",
      "[6000]\ttraining's l1: 0.550057\tvalid_1's l1: 0.763854\n",
      "[6100]\ttraining's l1: 0.546061\tvalid_1's l1: 0.761834\n",
      "[6200]\ttraining's l1: 0.542223\tvalid_1's l1: 0.760106\n",
      "[6300]\ttraining's l1: 0.53838\tvalid_1's l1: 0.75831\n",
      "[6400]\ttraining's l1: 0.534639\tvalid_1's l1: 0.756459\n",
      "[6500]\ttraining's l1: 0.531044\tvalid_1's l1: 0.754783\n",
      "[6600]\ttraining's l1: 0.527237\tvalid_1's l1: 0.752897\n",
      "[6700]\ttraining's l1: 0.523699\tvalid_1's l1: 0.751166\n",
      "[6800]\ttraining's l1: 0.520085\tvalid_1's l1: 0.749514\n",
      "[6900]\ttraining's l1: 0.516661\tvalid_1's l1: 0.748036\n",
      "[7000]\ttraining's l1: 0.513172\tvalid_1's l1: 0.746416\n",
      "[7100]\ttraining's l1: 0.509722\tvalid_1's l1: 0.744762\n",
      "[7200]\ttraining's l1: 0.506394\tvalid_1's l1: 0.743275\n",
      "[7300]\ttraining's l1: 0.50311\tvalid_1's l1: 0.741824\n",
      "[7400]\ttraining's l1: 0.499826\tvalid_1's l1: 0.740382\n",
      "[7500]\ttraining's l1: 0.496566\tvalid_1's l1: 0.738927\n",
      "[7600]\ttraining's l1: 0.493156\tvalid_1's l1: 0.737234\n",
      "[7700]\ttraining's l1: 0.489943\tvalid_1's l1: 0.735855\n",
      "[7800]\ttraining's l1: 0.486918\tvalid_1's l1: 0.734541\n",
      "[7900]\ttraining's l1: 0.483862\tvalid_1's l1: 0.733125\n",
      "[8000]\ttraining's l1: 0.480871\tvalid_1's l1: 0.731934\n",
      "[8100]\ttraining's l1: 0.477953\tvalid_1's l1: 0.730657\n",
      "[8200]\ttraining's l1: 0.475023\tvalid_1's l1: 0.729474\n",
      "[8300]\ttraining's l1: 0.472245\tvalid_1's l1: 0.728355\n",
      "[8400]\ttraining's l1: 0.469336\tvalid_1's l1: 0.72717\n",
      "[8500]\ttraining's l1: 0.4666\tvalid_1's l1: 0.726115\n",
      "[8600]\ttraining's l1: 0.463837\tvalid_1's l1: 0.725058\n",
      "[8700]\ttraining's l1: 0.461123\tvalid_1's l1: 0.723879\n",
      "[8800]\ttraining's l1: 0.458413\tvalid_1's l1: 0.722678\n",
      "[8900]\ttraining's l1: 0.455787\tvalid_1's l1: 0.721679\n",
      "[9000]\ttraining's l1: 0.453161\tvalid_1's l1: 0.720698\n",
      "[9100]\ttraining's l1: 0.450582\tvalid_1's l1: 0.719645\n",
      "[9200]\ttraining's l1: 0.447983\tvalid_1's l1: 0.718587\n",
      "[9300]\ttraining's l1: 0.445384\tvalid_1's l1: 0.717594\n",
      "[9400]\ttraining's l1: 0.442858\tvalid_1's l1: 0.716591\n",
      "[9500]\ttraining's l1: 0.440369\tvalid_1's l1: 0.715662\n",
      "[9600]\ttraining's l1: 0.437787\tvalid_1's l1: 0.714659\n",
      "[9700]\ttraining's l1: 0.435302\tvalid_1's l1: 0.713667\n",
      "[9800]\ttraining's l1: 0.432712\tvalid_1's l1: 0.712587\n",
      "[9900]\ttraining's l1: 0.430295\tvalid_1's l1: 0.7116\n",
      "[10000]\ttraining's l1: 0.427905\tvalid_1's l1: 0.710653\n",
      "[10100]\ttraining's l1: 0.425623\tvalid_1's l1: 0.709777\n",
      "[10200]\ttraining's l1: 0.42333\tvalid_1's l1: 0.708954\n",
      "[10300]\ttraining's l1: 0.421015\tvalid_1's l1: 0.708094\n",
      "[10400]\ttraining's l1: 0.41872\tvalid_1's l1: 0.707255\n",
      "[10500]\ttraining's l1: 0.416454\tvalid_1's l1: 0.706443\n",
      "[10600]\ttraining's l1: 0.414228\tvalid_1's l1: 0.705517\n",
      "[10700]\ttraining's l1: 0.412165\tvalid_1's l1: 0.704787\n",
      "[10800]\ttraining's l1: 0.410035\tvalid_1's l1: 0.704065\n",
      "[10900]\ttraining's l1: 0.4078\tvalid_1's l1: 0.703259\n",
      "[11000]\ttraining's l1: 0.405565\tvalid_1's l1: 0.702435\n",
      "[11100]\ttraining's l1: 0.403418\tvalid_1's l1: 0.701642\n",
      "[11200]\ttraining's l1: 0.401221\tvalid_1's l1: 0.700833\n",
      "[11300]\ttraining's l1: 0.399133\tvalid_1's l1: 0.700095\n",
      "[11400]\ttraining's l1: 0.397042\tvalid_1's l1: 0.699384\n",
      "[11500]\ttraining's l1: 0.394909\tvalid_1's l1: 0.69858\n",
      "[11600]\ttraining's l1: 0.392875\tvalid_1's l1: 0.697836\n",
      "[11700]\ttraining's l1: 0.390898\tvalid_1's l1: 0.697086\n",
      "[11800]\ttraining's l1: 0.388964\tvalid_1's l1: 0.696333\n",
      "[11900]\ttraining's l1: 0.386952\tvalid_1's l1: 0.695591\n",
      "[12000]\ttraining's l1: 0.384937\tvalid_1's l1: 0.694894\n",
      "[12100]\ttraining's l1: 0.38301\tvalid_1's l1: 0.694191\n",
      "[12200]\ttraining's l1: 0.381113\tvalid_1's l1: 0.693524\n",
      "[12300]\ttraining's l1: 0.379171\tvalid_1's l1: 0.692827\n",
      "[12400]\ttraining's l1: 0.377348\tvalid_1's l1: 0.692223\n",
      "[12500]\ttraining's l1: 0.375504\tvalid_1's l1: 0.691561\n",
      "[12600]\ttraining's l1: 0.373693\tvalid_1's l1: 0.690955\n",
      "[12700]\ttraining's l1: 0.371943\tvalid_1's l1: 0.690346\n",
      "[12800]\ttraining's l1: 0.37007\tvalid_1's l1: 0.689685\n",
      "[12900]\ttraining's l1: 0.368299\tvalid_1's l1: 0.689121\n",
      "[13000]\ttraining's l1: 0.36657\tvalid_1's l1: 0.688565\n",
      "[13100]\ttraining's l1: 0.364824\tvalid_1's l1: 0.688014\n",
      "[13200]\ttraining's l1: 0.363172\tvalid_1's l1: 0.687557\n",
      "[13300]\ttraining's l1: 0.361431\tvalid_1's l1: 0.687022\n",
      "[13400]\ttraining's l1: 0.35962\tvalid_1's l1: 0.686371\n",
      "[13500]\ttraining's l1: 0.357904\tvalid_1's l1: 0.685786\n",
      "[13600]\ttraining's l1: 0.356199\tvalid_1's l1: 0.685255\n",
      "[13700]\ttraining's l1: 0.354435\tvalid_1's l1: 0.684569\n",
      "[13800]\ttraining's l1: 0.35281\tvalid_1's l1: 0.684037\n",
      "[13900]\ttraining's l1: 0.351198\tvalid_1's l1: 0.683533\n",
      "[14000]\ttraining's l1: 0.349484\tvalid_1's l1: 0.682907\n",
      "[14100]\ttraining's l1: 0.347832\tvalid_1's l1: 0.682381\n",
      "[14200]\ttraining's l1: 0.346224\tvalid_1's l1: 0.681885\n",
      "[14300]\ttraining's l1: 0.344634\tvalid_1's l1: 0.681354\n",
      "[14400]\ttraining's l1: 0.34304\tvalid_1's l1: 0.680847\n",
      "[14500]\ttraining's l1: 0.341476\tvalid_1's l1: 0.680298\n",
      "[14600]\ttraining's l1: 0.339924\tvalid_1's l1: 0.679835\n",
      "[14700]\ttraining's l1: 0.338357\tvalid_1's l1: 0.679267\n",
      "[14800]\ttraining's l1: 0.336836\tvalid_1's l1: 0.678765\n",
      "[14900]\ttraining's l1: 0.335182\tvalid_1's l1: 0.678201\n",
      "[15000]\ttraining's l1: 0.333695\tvalid_1's l1: 0.677704\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.333695\tvalid_1's l1: 0.677704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1JHC Fold 1, logMAE: -0.3890450446345543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 1.66081\tvalid_1's l1: 1.67279\n",
      "[200]\ttraining's l1: 1.42005\tvalid_1's l1: 1.43854\n",
      "[300]\ttraining's l1: 1.30156\tvalid_1's l1: 1.32611\n",
      "[400]\ttraining's l1: 1.22132\tvalid_1's l1: 1.25168\n",
      "[500]\ttraining's l1: 1.1621\tvalid_1's l1: 1.19791\n",
      "[600]\ttraining's l1: 1.11566\tvalid_1's l1: 1.15634\n",
      "[700]\ttraining's l1: 1.07655\tvalid_1's l1: 1.12202\n",
      "[800]\ttraining's l1: 1.04386\tvalid_1's l1: 1.09441\n",
      "[900]\ttraining's l1: 1.01635\tvalid_1's l1: 1.07164\n",
      "[1000]\ttraining's l1: 0.99071\tvalid_1's l1: 1.05098\n",
      "[1100]\ttraining's l1: 0.967182\tvalid_1's l1: 1.03199\n",
      "[1200]\ttraining's l1: 0.946473\tvalid_1's l1: 1.01582\n",
      "[1300]\ttraining's l1: 0.925612\tvalid_1's l1: 0.999392\n",
      "[1400]\ttraining's l1: 0.906878\tvalid_1's l1: 0.985091\n",
      "[1500]\ttraining's l1: 0.890418\tvalid_1's l1: 0.972772\n",
      "[1600]\ttraining's l1: 0.874859\tvalid_1's l1: 0.96135\n",
      "[1700]\ttraining's l1: 0.860315\tvalid_1's l1: 0.950701\n",
      "[1800]\ttraining's l1: 0.846182\tvalid_1's l1: 0.940329\n",
      "[1900]\ttraining's l1: 0.832277\tvalid_1's l1: 0.930142\n",
      "[2000]\ttraining's l1: 0.818783\tvalid_1's l1: 0.920318\n",
      "[2100]\ttraining's l1: 0.806309\tvalid_1's l1: 0.911626\n",
      "[2200]\ttraining's l1: 0.794171\tvalid_1's l1: 0.90323\n",
      "[2300]\ttraining's l1: 0.782618\tvalid_1's l1: 0.895174\n",
      "[2400]\ttraining's l1: 0.771807\tvalid_1's l1: 0.887696\n",
      "[2500]\ttraining's l1: 0.761621\tvalid_1's l1: 0.880833\n",
      "[2600]\ttraining's l1: 0.751373\tvalid_1's l1: 0.873878\n",
      "[2700]\ttraining's l1: 0.742211\tvalid_1's l1: 0.867961\n",
      "[2800]\ttraining's l1: 0.732951\tvalid_1's l1: 0.861774\n",
      "[2900]\ttraining's l1: 0.72435\tvalid_1's l1: 0.856246\n",
      "[3000]\ttraining's l1: 0.716254\tvalid_1's l1: 0.851332\n",
      "[3100]\ttraining's l1: 0.708518\tvalid_1's l1: 0.846737\n",
      "[3200]\ttraining's l1: 0.700932\tvalid_1's l1: 0.842019\n",
      "[3300]\ttraining's l1: 0.693398\tvalid_1's l1: 0.83744\n",
      "[3400]\ttraining's l1: 0.68615\tvalid_1's l1: 0.833094\n",
      "[3500]\ttraining's l1: 0.679298\tvalid_1's l1: 0.828968\n",
      "[3600]\ttraining's l1: 0.672378\tvalid_1's l1: 0.824808\n",
      "[3700]\ttraining's l1: 0.665739\tvalid_1's l1: 0.821024\n",
      "[3800]\ttraining's l1: 0.65937\tvalid_1's l1: 0.817513\n",
      "[3900]\ttraining's l1: 0.653046\tvalid_1's l1: 0.814048\n",
      "[4000]\ttraining's l1: 0.646829\tvalid_1's l1: 0.810553\n",
      "[4100]\ttraining's l1: 0.640449\tvalid_1's l1: 0.80669\n",
      "[4200]\ttraining's l1: 0.63452\tvalid_1's l1: 0.80343\n",
      "[4300]\ttraining's l1: 0.628909\tvalid_1's l1: 0.800292\n",
      "[4400]\ttraining's l1: 0.623337\tvalid_1's l1: 0.797289\n",
      "[4500]\ttraining's l1: 0.618002\tvalid_1's l1: 0.79438\n",
      "[4600]\ttraining's l1: 0.612632\tvalid_1's l1: 0.791468\n",
      "[4700]\ttraining's l1: 0.607515\tvalid_1's l1: 0.78879\n",
      "[4800]\ttraining's l1: 0.602221\tvalid_1's l1: 0.785915\n",
      "[4900]\ttraining's l1: 0.597576\tvalid_1's l1: 0.783592\n",
      "[5000]\ttraining's l1: 0.592744\tvalid_1's l1: 0.780974\n",
      "[5100]\ttraining's l1: 0.588081\tvalid_1's l1: 0.778774\n",
      "[5200]\ttraining's l1: 0.583446\tvalid_1's l1: 0.776479\n",
      "[5300]\ttraining's l1: 0.578723\tvalid_1's l1: 0.774173\n",
      "[5400]\ttraining's l1: 0.574139\tvalid_1's l1: 0.771843\n",
      "[5500]\ttraining's l1: 0.569591\tvalid_1's l1: 0.769504\n",
      "[5600]\ttraining's l1: 0.565265\tvalid_1's l1: 0.767315\n",
      "[5700]\ttraining's l1: 0.561058\tvalid_1's l1: 0.765299\n",
      "[5800]\ttraining's l1: 0.556927\tvalid_1's l1: 0.763126\n",
      "[5900]\ttraining's l1: 0.552847\tvalid_1's l1: 0.761199\n",
      "[6000]\ttraining's l1: 0.548808\tvalid_1's l1: 0.759191\n",
      "[6100]\ttraining's l1: 0.545038\tvalid_1's l1: 0.757393\n",
      "[6200]\ttraining's l1: 0.541143\tvalid_1's l1: 0.75555\n",
      "[6300]\ttraining's l1: 0.537376\tvalid_1's l1: 0.753794\n",
      "[6400]\ttraining's l1: 0.533668\tvalid_1's l1: 0.752022\n",
      "[6500]\ttraining's l1: 0.530037\tvalid_1's l1: 0.75046\n",
      "[6600]\ttraining's l1: 0.5265\tvalid_1's l1: 0.748759\n",
      "[6700]\ttraining's l1: 0.522724\tvalid_1's l1: 0.746968\n",
      "[6800]\ttraining's l1: 0.519307\tvalid_1's l1: 0.745497\n",
      "[6900]\ttraining's l1: 0.515826\tvalid_1's l1: 0.743776\n",
      "[7000]\ttraining's l1: 0.512445\tvalid_1's l1: 0.742314\n",
      "[7100]\ttraining's l1: 0.508972\tvalid_1's l1: 0.740682\n",
      "[7200]\ttraining's l1: 0.505647\tvalid_1's l1: 0.739171\n",
      "[7300]\ttraining's l1: 0.502309\tvalid_1's l1: 0.737734\n",
      "[7400]\ttraining's l1: 0.498951\tvalid_1's l1: 0.736053\n",
      "[7500]\ttraining's l1: 0.495719\tvalid_1's l1: 0.734762\n",
      "[7600]\ttraining's l1: 0.492549\tvalid_1's l1: 0.733426\n",
      "[7700]\ttraining's l1: 0.489431\tvalid_1's l1: 0.732018\n",
      "[7800]\ttraining's l1: 0.486459\tvalid_1's l1: 0.730812\n",
      "[7900]\ttraining's l1: 0.483402\tvalid_1's l1: 0.729472\n",
      "[8000]\ttraining's l1: 0.480381\tvalid_1's l1: 0.728188\n",
      "[8100]\ttraining's l1: 0.477474\tvalid_1's l1: 0.726959\n",
      "[8200]\ttraining's l1: 0.474568\tvalid_1's l1: 0.725619\n",
      "[8300]\ttraining's l1: 0.471737\tvalid_1's l1: 0.724491\n",
      "[8400]\ttraining's l1: 0.468886\tvalid_1's l1: 0.723282\n",
      "[8500]\ttraining's l1: 0.466042\tvalid_1's l1: 0.722089\n",
      "[8600]\ttraining's l1: 0.463269\tvalid_1's l1: 0.721047\n",
      "[8700]\ttraining's l1: 0.460417\tvalid_1's l1: 0.719856\n",
      "[8800]\ttraining's l1: 0.457617\tvalid_1's l1: 0.718606\n",
      "[8900]\ttraining's l1: 0.454917\tvalid_1's l1: 0.717564\n",
      "[9000]\ttraining's l1: 0.452246\tvalid_1's l1: 0.716471\n",
      "[9100]\ttraining's l1: 0.449517\tvalid_1's l1: 0.715276\n",
      "[9200]\ttraining's l1: 0.446827\tvalid_1's l1: 0.714153\n",
      "[9300]\ttraining's l1: 0.444299\tvalid_1's l1: 0.713103\n",
      "[9400]\ttraining's l1: 0.441924\tvalid_1's l1: 0.71215\n",
      "[9500]\ttraining's l1: 0.439469\tvalid_1's l1: 0.711135\n",
      "[9600]\ttraining's l1: 0.437026\tvalid_1's l1: 0.710159\n",
      "[9700]\ttraining's l1: 0.434524\tvalid_1's l1: 0.709102\n",
      "[9800]\ttraining's l1: 0.432061\tvalid_1's l1: 0.70816\n",
      "[9900]\ttraining's l1: 0.429666\tvalid_1's l1: 0.707167\n",
      "[10000]\ttraining's l1: 0.427349\tvalid_1's l1: 0.706278\n",
      "[10100]\ttraining's l1: 0.425076\tvalid_1's l1: 0.705423\n",
      "[10200]\ttraining's l1: 0.42276\tvalid_1's l1: 0.704595\n",
      "[10300]\ttraining's l1: 0.420382\tvalid_1's l1: 0.703682\n",
      "[10400]\ttraining's l1: 0.418048\tvalid_1's l1: 0.702805\n",
      "[10500]\ttraining's l1: 0.415807\tvalid_1's l1: 0.701925\n",
      "[10600]\ttraining's l1: 0.413602\tvalid_1's l1: 0.701097\n",
      "[10700]\ttraining's l1: 0.411487\tvalid_1's l1: 0.700304\n",
      "[10800]\ttraining's l1: 0.409245\tvalid_1's l1: 0.699406\n",
      "[10900]\ttraining's l1: 0.406972\tvalid_1's l1: 0.698452\n",
      "[11000]\ttraining's l1: 0.404758\tvalid_1's l1: 0.697529\n",
      "[11100]\ttraining's l1: 0.402616\tvalid_1's l1: 0.696758\n",
      "[11200]\ttraining's l1: 0.400462\tvalid_1's l1: 0.695932\n",
      "[11300]\ttraining's l1: 0.398314\tvalid_1's l1: 0.69513\n",
      "[11400]\ttraining's l1: 0.396297\tvalid_1's l1: 0.694416\n",
      "[11500]\ttraining's l1: 0.394261\tvalid_1's l1: 0.693671\n",
      "[11600]\ttraining's l1: 0.392256\tvalid_1's l1: 0.69291\n",
      "[11700]\ttraining's l1: 0.390234\tvalid_1's l1: 0.692249\n",
      "[11800]\ttraining's l1: 0.388286\tvalid_1's l1: 0.69159\n",
      "[11900]\ttraining's l1: 0.386304\tvalid_1's l1: 0.690863\n",
      "[12000]\ttraining's l1: 0.384301\tvalid_1's l1: 0.69016\n",
      "[12100]\ttraining's l1: 0.382304\tvalid_1's l1: 0.689461\n",
      "[12200]\ttraining's l1: 0.380424\tvalid_1's l1: 0.688833\n",
      "[12300]\ttraining's l1: 0.378518\tvalid_1's l1: 0.688194\n",
      "[12400]\ttraining's l1: 0.376664\tvalid_1's l1: 0.687564\n",
      "[12500]\ttraining's l1: 0.374854\tvalid_1's l1: 0.68693\n",
      "[12600]\ttraining's l1: 0.373042\tvalid_1's l1: 0.686277\n",
      "[12700]\ttraining's l1: 0.37121\tvalid_1's l1: 0.685627\n",
      "[12800]\ttraining's l1: 0.36942\tvalid_1's l1: 0.685102\n",
      "[12900]\ttraining's l1: 0.367674\tvalid_1's l1: 0.684565\n",
      "[13000]\ttraining's l1: 0.36588\tvalid_1's l1: 0.68398\n",
      "[13100]\ttraining's l1: 0.364066\tvalid_1's l1: 0.683368\n",
      "[13200]\ttraining's l1: 0.362275\tvalid_1's l1: 0.682682\n",
      "[13300]\ttraining's l1: 0.360617\tvalid_1's l1: 0.682143\n",
      "[13400]\ttraining's l1: 0.3589\tvalid_1's l1: 0.68162\n",
      "[13500]\ttraining's l1: 0.357185\tvalid_1's l1: 0.681045\n",
      "[13600]\ttraining's l1: 0.355481\tvalid_1's l1: 0.680526\n",
      "[13700]\ttraining's l1: 0.353767\tvalid_1's l1: 0.679993\n",
      "[13800]\ttraining's l1: 0.35205\tvalid_1's l1: 0.679422\n",
      "[13900]\ttraining's l1: 0.350429\tvalid_1's l1: 0.678903\n",
      "[14000]\ttraining's l1: 0.348806\tvalid_1's l1: 0.678339\n",
      "[14100]\ttraining's l1: 0.347199\tvalid_1's l1: 0.677761\n",
      "[14200]\ttraining's l1: 0.345576\tvalid_1's l1: 0.6772\n",
      "[14300]\ttraining's l1: 0.343976\tvalid_1's l1: 0.67664\n",
      "[14400]\ttraining's l1: 0.342468\tvalid_1's l1: 0.676092\n",
      "[14500]\ttraining's l1: 0.340871\tvalid_1's l1: 0.67552\n",
      "[14600]\ttraining's l1: 0.339284\tvalid_1's l1: 0.674954\n",
      "[14700]\ttraining's l1: 0.337737\tvalid_1's l1: 0.674504\n",
      "[14800]\ttraining's l1: 0.336132\tvalid_1's l1: 0.673958\n",
      "[14900]\ttraining's l1: 0.334631\tvalid_1's l1: 0.673453\n",
      "[15000]\ttraining's l1: 0.333174\tvalid_1's l1: 0.672974\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.333174\tvalid_1's l1: 0.672974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1JHC Fold 2, logMAE: -0.3960480457339578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 1.66579\tvalid_1's l1: 1.66946\n",
      "[200]\ttraining's l1: 1.42545\tvalid_1's l1: 1.43948\n",
      "[300]\ttraining's l1: 1.30676\tvalid_1's l1: 1.3282\n",
      "[400]\ttraining's l1: 1.22777\tvalid_1's l1: 1.25561\n",
      "[500]\ttraining's l1: 1.16557\tvalid_1's l1: 1.19891\n",
      "[600]\ttraining's l1: 1.11963\tvalid_1's l1: 1.15799\n",
      "[700]\ttraining's l1: 1.08169\tvalid_1's l1: 1.12497\n",
      "[800]\ttraining's l1: 1.04913\tvalid_1's l1: 1.09694\n",
      "[900]\ttraining's l1: 1.01899\tvalid_1's l1: 1.07169\n",
      "[1000]\ttraining's l1: 0.994524\tvalid_1's l1: 1.05215\n",
      "[1100]\ttraining's l1: 0.970307\tvalid_1's l1: 1.03242\n",
      "[1200]\ttraining's l1: 0.948877\tvalid_1's l1: 1.01564\n",
      "[1300]\ttraining's l1: 0.928661\tvalid_1's l1: 1.00002\n",
      "[1400]\ttraining's l1: 0.910205\tvalid_1's l1: 0.985763\n",
      "[1500]\ttraining's l1: 0.892318\tvalid_1's l1: 0.972104\n",
      "[1600]\ttraining's l1: 0.876289\tvalid_1's l1: 0.960237\n",
      "[1700]\ttraining's l1: 0.861775\tvalid_1's l1: 0.949946\n",
      "[1800]\ttraining's l1: 0.846903\tvalid_1's l1: 0.939261\n",
      "[1900]\ttraining's l1: 0.833209\tvalid_1's l1: 0.929459\n",
      "[2000]\ttraining's l1: 0.820677\tvalid_1's l1: 0.920533\n",
      "[2100]\ttraining's l1: 0.807563\tvalid_1's l1: 0.910809\n",
      "[2200]\ttraining's l1: 0.796328\tvalid_1's l1: 0.903305\n",
      "[2300]\ttraining's l1: 0.785879\tvalid_1's l1: 0.896204\n",
      "[2400]\ttraining's l1: 0.775606\tvalid_1's l1: 0.889499\n",
      "[2500]\ttraining's l1: 0.765915\tvalid_1's l1: 0.883154\n",
      "[2600]\ttraining's l1: 0.756122\tvalid_1's l1: 0.876408\n",
      "[2700]\ttraining's l1: 0.747252\tvalid_1's l1: 0.870603\n",
      "[2800]\ttraining's l1: 0.738219\tvalid_1's l1: 0.864914\n",
      "[2900]\ttraining's l1: 0.729358\tvalid_1's l1: 0.859186\n",
      "[3000]\ttraining's l1: 0.721001\tvalid_1's l1: 0.854093\n",
      "[3100]\ttraining's l1: 0.713404\tvalid_1's l1: 0.849568\n",
      "[3200]\ttraining's l1: 0.705244\tvalid_1's l1: 0.844298\n",
      "[3300]\ttraining's l1: 0.69785\tvalid_1's l1: 0.839974\n",
      "[3400]\ttraining's l1: 0.690099\tvalid_1's l1: 0.835242\n",
      "[3500]\ttraining's l1: 0.683242\tvalid_1's l1: 0.831105\n",
      "[3600]\ttraining's l1: 0.676369\tvalid_1's l1: 0.827041\n",
      "[3700]\ttraining's l1: 0.669726\tvalid_1's l1: 0.823031\n",
      "[3800]\ttraining's l1: 0.662988\tvalid_1's l1: 0.819063\n",
      "[3900]\ttraining's l1: 0.6567\tvalid_1's l1: 0.815493\n",
      "[4000]\ttraining's l1: 0.650714\tvalid_1's l1: 0.812108\n",
      "[4100]\ttraining's l1: 0.644611\tvalid_1's l1: 0.808729\n",
      "[4200]\ttraining's l1: 0.638481\tvalid_1's l1: 0.805341\n",
      "[4300]\ttraining's l1: 0.632544\tvalid_1's l1: 0.80186\n",
      "[4400]\ttraining's l1: 0.627055\tvalid_1's l1: 0.798755\n",
      "[4500]\ttraining's l1: 0.621573\tvalid_1's l1: 0.795838\n",
      "[4600]\ttraining's l1: 0.616025\tvalid_1's l1: 0.792726\n",
      "[4700]\ttraining's l1: 0.610696\tvalid_1's l1: 0.789769\n",
      "[4800]\ttraining's l1: 0.605684\tvalid_1's l1: 0.787156\n",
      "[4900]\ttraining's l1: 0.600534\tvalid_1's l1: 0.784327\n",
      "[5000]\ttraining's l1: 0.595732\tvalid_1's l1: 0.781824\n",
      "[5100]\ttraining's l1: 0.591019\tvalid_1's l1: 0.77943\n",
      "[5200]\ttraining's l1: 0.585968\tvalid_1's l1: 0.776586\n",
      "[5300]\ttraining's l1: 0.581379\tvalid_1's l1: 0.774235\n",
      "[5400]\ttraining's l1: 0.576843\tvalid_1's l1: 0.771848\n",
      "[5500]\ttraining's l1: 0.572063\tvalid_1's l1: 0.76938\n",
      "[5600]\ttraining's l1: 0.56794\tvalid_1's l1: 0.767583\n",
      "[5700]\ttraining's l1: 0.563652\tvalid_1's l1: 0.765585\n",
      "[5800]\ttraining's l1: 0.559162\tvalid_1's l1: 0.763206\n",
      "[5900]\ttraining's l1: 0.55509\tvalid_1's l1: 0.7612\n",
      "[6000]\ttraining's l1: 0.551206\tvalid_1's l1: 0.75944\n",
      "[6100]\ttraining's l1: 0.547138\tvalid_1's l1: 0.757422\n",
      "[6200]\ttraining's l1: 0.543244\tvalid_1's l1: 0.755592\n",
      "[6300]\ttraining's l1: 0.539566\tvalid_1's l1: 0.753843\n",
      "[6400]\ttraining's l1: 0.535802\tvalid_1's l1: 0.752003\n",
      "[6500]\ttraining's l1: 0.532029\tvalid_1's l1: 0.750175\n",
      "[6600]\ttraining's l1: 0.52836\tvalid_1's l1: 0.748422\n",
      "[6700]\ttraining's l1: 0.524768\tvalid_1's l1: 0.746715\n",
      "[6800]\ttraining's l1: 0.521142\tvalid_1's l1: 0.744923\n",
      "[6900]\ttraining's l1: 0.517564\tvalid_1's l1: 0.743129\n",
      "[7000]\ttraining's l1: 0.51411\tvalid_1's l1: 0.741546\n",
      "[7100]\ttraining's l1: 0.510678\tvalid_1's l1: 0.740047\n",
      "[7200]\ttraining's l1: 0.507373\tvalid_1's l1: 0.738615\n",
      "[7300]\ttraining's l1: 0.504066\tvalid_1's l1: 0.737043\n",
      "[7400]\ttraining's l1: 0.500892\tvalid_1's l1: 0.735673\n",
      "[7500]\ttraining's l1: 0.497721\tvalid_1's l1: 0.734301\n",
      "[7600]\ttraining's l1: 0.4946\tvalid_1's l1: 0.732934\n",
      "[7700]\ttraining's l1: 0.491367\tvalid_1's l1: 0.73147\n",
      "[7800]\ttraining's l1: 0.488254\tvalid_1's l1: 0.730096\n",
      "[7900]\ttraining's l1: 0.485293\tvalid_1's l1: 0.728868\n",
      "[8000]\ttraining's l1: 0.482235\tvalid_1's l1: 0.727439\n",
      "[8100]\ttraining's l1: 0.479275\tvalid_1's l1: 0.726198\n",
      "[8200]\ttraining's l1: 0.476359\tvalid_1's l1: 0.725006\n",
      "[8300]\ttraining's l1: 0.473513\tvalid_1's l1: 0.723821\n",
      "[8400]\ttraining's l1: 0.470742\tvalid_1's l1: 0.722692\n",
      "[8500]\ttraining's l1: 0.467929\tvalid_1's l1: 0.721549\n",
      "[8600]\ttraining's l1: 0.46515\tvalid_1's l1: 0.720467\n",
      "[8700]\ttraining's l1: 0.462275\tvalid_1's l1: 0.719212\n",
      "[8800]\ttraining's l1: 0.45945\tvalid_1's l1: 0.718031\n",
      "[8900]\ttraining's l1: 0.456753\tvalid_1's l1: 0.716894\n",
      "[9000]\ttraining's l1: 0.453961\tvalid_1's l1: 0.715658\n",
      "[9100]\ttraining's l1: 0.451178\tvalid_1's l1: 0.714434\n",
      "[9200]\ttraining's l1: 0.448594\tvalid_1's l1: 0.713362\n",
      "[9300]\ttraining's l1: 0.446045\tvalid_1's l1: 0.712358\n",
      "[9400]\ttraining's l1: 0.443652\tvalid_1's l1: 0.711385\n",
      "[9500]\ttraining's l1: 0.441192\tvalid_1's l1: 0.710417\n",
      "[9600]\ttraining's l1: 0.438651\tvalid_1's l1: 0.709417\n",
      "[9700]\ttraining's l1: 0.436169\tvalid_1's l1: 0.708384\n",
      "[9800]\ttraining's l1: 0.433793\tvalid_1's l1: 0.707458\n",
      "[9900]\ttraining's l1: 0.431365\tvalid_1's l1: 0.706572\n",
      "[10000]\ttraining's l1: 0.428973\tvalid_1's l1: 0.705615\n",
      "[10100]\ttraining's l1: 0.426542\tvalid_1's l1: 0.704649\n",
      "[10200]\ttraining's l1: 0.424185\tvalid_1's l1: 0.703775\n",
      "[10300]\ttraining's l1: 0.421868\tvalid_1's l1: 0.702932\n",
      "[10400]\ttraining's l1: 0.419513\tvalid_1's l1: 0.701994\n",
      "[10500]\ttraining's l1: 0.417294\tvalid_1's l1: 0.701134\n",
      "[10600]\ttraining's l1: 0.414978\tvalid_1's l1: 0.70025\n",
      "[10700]\ttraining's l1: 0.412761\tvalid_1's l1: 0.699371\n",
      "[10800]\ttraining's l1: 0.410503\tvalid_1's l1: 0.698546\n",
      "[10900]\ttraining's l1: 0.408314\tvalid_1's l1: 0.697646\n",
      "[11000]\ttraining's l1: 0.406116\tvalid_1's l1: 0.696829\n",
      "[11100]\ttraining's l1: 0.403962\tvalid_1's l1: 0.696107\n",
      "[11200]\ttraining's l1: 0.401942\tvalid_1's l1: 0.695406\n",
      "[11300]\ttraining's l1: 0.39988\tvalid_1's l1: 0.69472\n",
      "[11400]\ttraining's l1: 0.397798\tvalid_1's l1: 0.693976\n",
      "[11500]\ttraining's l1: 0.395787\tvalid_1's l1: 0.693292\n",
      "[11600]\ttraining's l1: 0.393731\tvalid_1's l1: 0.692603\n",
      "[11700]\ttraining's l1: 0.391724\tvalid_1's l1: 0.691875\n",
      "[11800]\ttraining's l1: 0.389654\tvalid_1's l1: 0.691129\n",
      "[11900]\ttraining's l1: 0.387549\tvalid_1's l1: 0.690361\n",
      "[12000]\ttraining's l1: 0.385542\tvalid_1's l1: 0.689713\n",
      "[12100]\ttraining's l1: 0.383558\tvalid_1's l1: 0.689042\n",
      "[12200]\ttraining's l1: 0.381716\tvalid_1's l1: 0.688457\n",
      "[12300]\ttraining's l1: 0.379885\tvalid_1's l1: 0.68783\n",
      "[12400]\ttraining's l1: 0.377992\tvalid_1's l1: 0.687223\n",
      "[12500]\ttraining's l1: 0.37608\tvalid_1's l1: 0.686585\n",
      "[12600]\ttraining's l1: 0.374285\tvalid_1's l1: 0.686051\n",
      "[12700]\ttraining's l1: 0.3724\tvalid_1's l1: 0.685396\n",
      "[12800]\ttraining's l1: 0.370502\tvalid_1's l1: 0.684713\n",
      "[12900]\ttraining's l1: 0.368719\tvalid_1's l1: 0.684101\n",
      "[13000]\ttraining's l1: 0.366898\tvalid_1's l1: 0.683419\n",
      "[13100]\ttraining's l1: 0.365128\tvalid_1's l1: 0.682765\n",
      "[13200]\ttraining's l1: 0.363403\tvalid_1's l1: 0.682123\n",
      "[13300]\ttraining's l1: 0.361658\tvalid_1's l1: 0.681578\n",
      "[13400]\ttraining's l1: 0.359961\tvalid_1's l1: 0.681038\n",
      "[13500]\ttraining's l1: 0.358207\tvalid_1's l1: 0.680453\n",
      "[13600]\ttraining's l1: 0.356539\tvalid_1's l1: 0.679894\n",
      "[13700]\ttraining's l1: 0.354886\tvalid_1's l1: 0.679345\n",
      "[13800]\ttraining's l1: 0.353197\tvalid_1's l1: 0.678756\n",
      "[13900]\ttraining's l1: 0.351591\tvalid_1's l1: 0.678218\n",
      "[14000]\ttraining's l1: 0.34993\tvalid_1's l1: 0.677621\n",
      "[14100]\ttraining's l1: 0.348296\tvalid_1's l1: 0.6771\n",
      "[14200]\ttraining's l1: 0.346638\tvalid_1's l1: 0.67654\n",
      "[14300]\ttraining's l1: 0.34502\tvalid_1's l1: 0.67602\n",
      "[14400]\ttraining's l1: 0.343467\tvalid_1's l1: 0.675507\n",
      "[14500]\ttraining's l1: 0.341895\tvalid_1's l1: 0.674959\n",
      "[14600]\ttraining's l1: 0.34032\tvalid_1's l1: 0.674477\n",
      "[14700]\ttraining's l1: 0.338728\tvalid_1's l1: 0.673937\n",
      "[14800]\ttraining's l1: 0.337173\tvalid_1's l1: 0.673443\n",
      "[14900]\ttraining's l1: 0.335686\tvalid_1's l1: 0.672985\n",
      "[15000]\ttraining's l1: 0.334085\tvalid_1's l1: 0.67247\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.334085\tvalid_1's l1: 0.67247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1JHC Fold 3, logMAE: -0.3967982850266533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 1.65984\tvalid_1's l1: 1.6712\n",
      "[200]\ttraining's l1: 1.41866\tvalid_1's l1: 1.44041\n",
      "[300]\ttraining's l1: 1.2997\tvalid_1's l1: 1.32759\n",
      "[400]\ttraining's l1: 1.22344\tvalid_1's l1: 1.25796\n",
      "[500]\ttraining's l1: 1.16103\tvalid_1's l1: 1.20211\n",
      "[600]\ttraining's l1: 1.11525\tvalid_1's l1: 1.16215\n",
      "[700]\ttraining's l1: 1.07706\tvalid_1's l1: 1.12971\n",
      "[800]\ttraining's l1: 1.04331\tvalid_1's l1: 1.10109\n",
      "[900]\ttraining's l1: 1.01198\tvalid_1's l1: 1.07518\n",
      "[1000]\ttraining's l1: 0.985088\tvalid_1's l1: 1.0531\n",
      "[1100]\ttraining's l1: 0.961891\tvalid_1's l1: 1.03439\n",
      "[1200]\ttraining's l1: 0.940641\tvalid_1's l1: 1.01756\n",
      "[1300]\ttraining's l1: 0.921079\tvalid_1's l1: 1.00216\n",
      "[1400]\ttraining's l1: 0.903565\tvalid_1's l1: 0.988253\n",
      "[1500]\ttraining's l1: 0.886856\tvalid_1's l1: 0.975988\n",
      "[1600]\ttraining's l1: 0.870292\tvalid_1's l1: 0.963492\n",
      "[1700]\ttraining's l1: 0.855228\tvalid_1's l1: 0.952214\n",
      "[1800]\ttraining's l1: 0.841116\tvalid_1's l1: 0.941855\n",
      "[1900]\ttraining's l1: 0.828545\tvalid_1's l1: 0.933097\n",
      "[2000]\ttraining's l1: 0.815982\tvalid_1's l1: 0.924208\n",
      "[2100]\ttraining's l1: 0.804071\tvalid_1's l1: 0.915849\n",
      "[2200]\ttraining's l1: 0.792407\tvalid_1's l1: 0.907485\n",
      "[2300]\ttraining's l1: 0.781029\tvalid_1's l1: 0.899579\n",
      "[2400]\ttraining's l1: 0.77059\tvalid_1's l1: 0.892432\n",
      "[2500]\ttraining's l1: 0.761064\tvalid_1's l1: 0.886321\n",
      "[2600]\ttraining's l1: 0.750811\tvalid_1's l1: 0.879497\n",
      "[2700]\ttraining's l1: 0.741616\tvalid_1's l1: 0.87348\n",
      "[2800]\ttraining's l1: 0.732988\tvalid_1's l1: 0.867866\n",
      "[2900]\ttraining's l1: 0.724372\tvalid_1's l1: 0.862191\n",
      "[3000]\ttraining's l1: 0.71625\tvalid_1's l1: 0.857129\n",
      "[3100]\ttraining's l1: 0.708226\tvalid_1's l1: 0.852201\n",
      "[3200]\ttraining's l1: 0.700026\tvalid_1's l1: 0.846915\n",
      "[3300]\ttraining's l1: 0.692671\tvalid_1's l1: 0.84239\n",
      "[3400]\ttraining's l1: 0.685532\tvalid_1's l1: 0.838181\n",
      "[3500]\ttraining's l1: 0.678688\tvalid_1's l1: 0.834212\n",
      "[3600]\ttraining's l1: 0.671771\tvalid_1's l1: 0.830041\n",
      "[3700]\ttraining's l1: 0.665089\tvalid_1's l1: 0.826046\n",
      "[3800]\ttraining's l1: 0.658557\tvalid_1's l1: 0.822411\n",
      "[3900]\ttraining's l1: 0.652372\tvalid_1's l1: 0.818929\n",
      "[4000]\ttraining's l1: 0.646464\tvalid_1's l1: 0.815486\n",
      "[4100]\ttraining's l1: 0.640747\tvalid_1's l1: 0.812221\n",
      "[4200]\ttraining's l1: 0.634737\tvalid_1's l1: 0.808605\n",
      "[4300]\ttraining's l1: 0.629132\tvalid_1's l1: 0.805501\n",
      "[4400]\ttraining's l1: 0.623494\tvalid_1's l1: 0.802355\n",
      "[4500]\ttraining's l1: 0.618068\tvalid_1's l1: 0.799211\n",
      "[4600]\ttraining's l1: 0.612689\tvalid_1's l1: 0.796283\n",
      "[4700]\ttraining's l1: 0.607592\tvalid_1's l1: 0.793753\n",
      "[4800]\ttraining's l1: 0.602665\tvalid_1's l1: 0.791163\n",
      "[4900]\ttraining's l1: 0.597745\tvalid_1's l1: 0.788656\n",
      "[5000]\ttraining's l1: 0.592778\tvalid_1's l1: 0.786054\n",
      "[5100]\ttraining's l1: 0.588079\tvalid_1's l1: 0.783639\n",
      "[5200]\ttraining's l1: 0.583305\tvalid_1's l1: 0.781149\n",
      "[5300]\ttraining's l1: 0.578578\tvalid_1's l1: 0.778709\n",
      "[5400]\ttraining's l1: 0.574131\tvalid_1's l1: 0.776417\n",
      "[5500]\ttraining's l1: 0.569695\tvalid_1's l1: 0.774315\n",
      "[5600]\ttraining's l1: 0.565347\tvalid_1's l1: 0.772351\n",
      "[5700]\ttraining's l1: 0.561131\tvalid_1's l1: 0.770242\n",
      "[5800]\ttraining's l1: 0.556988\tvalid_1's l1: 0.768141\n",
      "[5900]\ttraining's l1: 0.553032\tvalid_1's l1: 0.766333\n",
      "[6000]\ttraining's l1: 0.549032\tvalid_1's l1: 0.764425\n",
      "[6100]\ttraining's l1: 0.545076\tvalid_1's l1: 0.762487\n",
      "[6200]\ttraining's l1: 0.541157\tvalid_1's l1: 0.760563\n",
      "[6300]\ttraining's l1: 0.537257\tvalid_1's l1: 0.758702\n",
      "[6400]\ttraining's l1: 0.533448\tvalid_1's l1: 0.75687\n",
      "[6500]\ttraining's l1: 0.529551\tvalid_1's l1: 0.754928\n",
      "[6600]\ttraining's l1: 0.525806\tvalid_1's l1: 0.753135\n",
      "[6700]\ttraining's l1: 0.522179\tvalid_1's l1: 0.751487\n",
      "[6800]\ttraining's l1: 0.518721\tvalid_1's l1: 0.749999\n",
      "[6900]\ttraining's l1: 0.515336\tvalid_1's l1: 0.748499\n",
      "[7000]\ttraining's l1: 0.511834\tvalid_1's l1: 0.746875\n",
      "[7100]\ttraining's l1: 0.508378\tvalid_1's l1: 0.74527\n",
      "[7200]\ttraining's l1: 0.504973\tvalid_1's l1: 0.743745\n",
      "[7300]\ttraining's l1: 0.501625\tvalid_1's l1: 0.742202\n",
      "[7400]\ttraining's l1: 0.498302\tvalid_1's l1: 0.740628\n",
      "[7500]\ttraining's l1: 0.495023\tvalid_1's l1: 0.739182\n",
      "[7600]\ttraining's l1: 0.491767\tvalid_1's l1: 0.737844\n",
      "[7700]\ttraining's l1: 0.488569\tvalid_1's l1: 0.7365\n",
      "[7800]\ttraining's l1: 0.485504\tvalid_1's l1: 0.735175\n",
      "[7900]\ttraining's l1: 0.482434\tvalid_1's l1: 0.733842\n",
      "[8000]\ttraining's l1: 0.479455\tvalid_1's l1: 0.732634\n",
      "[8100]\ttraining's l1: 0.476618\tvalid_1's l1: 0.731399\n",
      "[8200]\ttraining's l1: 0.473633\tvalid_1's l1: 0.730238\n",
      "[8300]\ttraining's l1: 0.47077\tvalid_1's l1: 0.729099\n",
      "[8400]\ttraining's l1: 0.467946\tvalid_1's l1: 0.727866\n",
      "[8500]\ttraining's l1: 0.465215\tvalid_1's l1: 0.726761\n",
      "[8600]\ttraining's l1: 0.46253\tvalid_1's l1: 0.725688\n",
      "[8700]\ttraining's l1: 0.459746\tvalid_1's l1: 0.724494\n",
      "[8800]\ttraining's l1: 0.45712\tvalid_1's l1: 0.723418\n",
      "[8900]\ttraining's l1: 0.454511\tvalid_1's l1: 0.72237\n",
      "[9000]\ttraining's l1: 0.451776\tvalid_1's l1: 0.721208\n",
      "[9100]\ttraining's l1: 0.44914\tvalid_1's l1: 0.720134\n",
      "[9200]\ttraining's l1: 0.446444\tvalid_1's l1: 0.718976\n",
      "[9300]\ttraining's l1: 0.443839\tvalid_1's l1: 0.717972\n",
      "[9400]\ttraining's l1: 0.441327\tvalid_1's l1: 0.716981\n",
      "[9500]\ttraining's l1: 0.438759\tvalid_1's l1: 0.715973\n",
      "[9600]\ttraining's l1: 0.436234\tvalid_1's l1: 0.71497\n",
      "[9700]\ttraining's l1: 0.43378\tvalid_1's l1: 0.714009\n",
      "[9800]\ttraining's l1: 0.431438\tvalid_1's l1: 0.71305\n",
      "[9900]\ttraining's l1: 0.428988\tvalid_1's l1: 0.712016\n",
      "[10000]\ttraining's l1: 0.42658\tvalid_1's l1: 0.711051\n",
      "[10100]\ttraining's l1: 0.424217\tvalid_1's l1: 0.710123\n",
      "[10200]\ttraining's l1: 0.421914\tvalid_1's l1: 0.709209\n",
      "[10300]\ttraining's l1: 0.419623\tvalid_1's l1: 0.708382\n",
      "[10400]\ttraining's l1: 0.41733\tvalid_1's l1: 0.707477\n",
      "[10500]\ttraining's l1: 0.415128\tvalid_1's l1: 0.706714\n",
      "[10600]\ttraining's l1: 0.412875\tvalid_1's l1: 0.705855\n",
      "[10700]\ttraining's l1: 0.410675\tvalid_1's l1: 0.705025\n",
      "[10800]\ttraining's l1: 0.408542\tvalid_1's l1: 0.704274\n",
      "[10900]\ttraining's l1: 0.40633\tvalid_1's l1: 0.703403\n",
      "[11000]\ttraining's l1: 0.404194\tvalid_1's l1: 0.702607\n",
      "[11100]\ttraining's l1: 0.402125\tvalid_1's l1: 0.701862\n",
      "[11200]\ttraining's l1: 0.399986\tvalid_1's l1: 0.701033\n",
      "[11300]\ttraining's l1: 0.397882\tvalid_1's l1: 0.700292\n",
      "[11400]\ttraining's l1: 0.395861\tvalid_1's l1: 0.699582\n",
      "[11500]\ttraining's l1: 0.393843\tvalid_1's l1: 0.698885\n",
      "[11600]\ttraining's l1: 0.391869\tvalid_1's l1: 0.69822\n",
      "[11700]\ttraining's l1: 0.389873\tvalid_1's l1: 0.697588\n",
      "[11800]\ttraining's l1: 0.387927\tvalid_1's l1: 0.696865\n",
      "[11900]\ttraining's l1: 0.386044\tvalid_1's l1: 0.696178\n",
      "[12000]\ttraining's l1: 0.384137\tvalid_1's l1: 0.69553\n",
      "[12100]\ttraining's l1: 0.382224\tvalid_1's l1: 0.69488\n",
      "[12200]\ttraining's l1: 0.380315\tvalid_1's l1: 0.694209\n",
      "[12300]\ttraining's l1: 0.378432\tvalid_1's l1: 0.693541\n",
      "[12400]\ttraining's l1: 0.376523\tvalid_1's l1: 0.692854\n",
      "[12500]\ttraining's l1: 0.374638\tvalid_1's l1: 0.692274\n",
      "[12600]\ttraining's l1: 0.372751\tvalid_1's l1: 0.691548\n",
      "[12700]\ttraining's l1: 0.370873\tvalid_1's l1: 0.690784\n",
      "[12800]\ttraining's l1: 0.369014\tvalid_1's l1: 0.690106\n",
      "[12900]\ttraining's l1: 0.367203\tvalid_1's l1: 0.689489\n",
      "[13000]\ttraining's l1: 0.365397\tvalid_1's l1: 0.688849\n",
      "[13100]\ttraining's l1: 0.363599\tvalid_1's l1: 0.688313\n",
      "[13200]\ttraining's l1: 0.361902\tvalid_1's l1: 0.687758\n",
      "[13300]\ttraining's l1: 0.360116\tvalid_1's l1: 0.687186\n",
      "[13400]\ttraining's l1: 0.358481\tvalid_1's l1: 0.6866\n",
      "[13500]\ttraining's l1: 0.356762\tvalid_1's l1: 0.686037\n",
      "[13600]\ttraining's l1: 0.355093\tvalid_1's l1: 0.685413\n",
      "[13700]\ttraining's l1: 0.353414\tvalid_1's l1: 0.684839\n",
      "[13800]\ttraining's l1: 0.351752\tvalid_1's l1: 0.68425\n",
      "[13900]\ttraining's l1: 0.350157\tvalid_1's l1: 0.683711\n",
      "[14000]\ttraining's l1: 0.34847\tvalid_1's l1: 0.683168\n",
      "[14100]\ttraining's l1: 0.346919\tvalid_1's l1: 0.682648\n",
      "[14200]\ttraining's l1: 0.345298\tvalid_1's l1: 0.682142\n",
      "[14300]\ttraining's l1: 0.343743\tvalid_1's l1: 0.681623\n",
      "[14400]\ttraining's l1: 0.342121\tvalid_1's l1: 0.681091\n",
      "[14500]\ttraining's l1: 0.340525\tvalid_1's l1: 0.680617\n",
      "[14600]\ttraining's l1: 0.339004\tvalid_1's l1: 0.680146\n",
      "[14700]\ttraining's l1: 0.337449\tvalid_1's l1: 0.679631\n",
      "[14800]\ttraining's l1: 0.335889\tvalid_1's l1: 0.67907\n",
      "[14900]\ttraining's l1: 0.334342\tvalid_1's l1: 0.678575\n",
      "[15000]\ttraining's l1: 0.332763\tvalid_1's l1: 0.677968\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.332763\tvalid_1's l1: 0.677968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1JHC Fold 4, logMAE: -0.3886621951544665\n",
      "*** Training Model for 2JHH ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.362807\tvalid_1's l1: 0.368054\n",
      "[200]\ttraining's l1: 0.302626\tvalid_1's l1: 0.311385\n",
      "[300]\ttraining's l1: 0.272911\tvalid_1's l1: 0.284682\n",
      "[400]\ttraining's l1: 0.252741\tvalid_1's l1: 0.267006\n",
      "[500]\ttraining's l1: 0.237171\tvalid_1's l1: 0.253503\n",
      "[600]\ttraining's l1: 0.22457\tvalid_1's l1: 0.243259\n",
      "[700]\ttraining's l1: 0.213701\tvalid_1's l1: 0.234481\n",
      "[800]\ttraining's l1: 0.205094\tvalid_1's l1: 0.227769\n",
      "[900]\ttraining's l1: 0.197256\tvalid_1's l1: 0.221706\n",
      "[1000]\ttraining's l1: 0.189943\tvalid_1's l1: 0.216206\n",
      "[1100]\ttraining's l1: 0.183694\tvalid_1's l1: 0.211563\n",
      "[1200]\ttraining's l1: 0.177686\tvalid_1's l1: 0.207154\n",
      "[1300]\ttraining's l1: 0.172022\tvalid_1's l1: 0.203065\n",
      "[1400]\ttraining's l1: 0.166855\tvalid_1's l1: 0.199434\n",
      "[1500]\ttraining's l1: 0.16255\tvalid_1's l1: 0.196685\n",
      "[1600]\ttraining's l1: 0.15843\tvalid_1's l1: 0.193901\n",
      "[1700]\ttraining's l1: 0.154612\tvalid_1's l1: 0.191384\n",
      "[1800]\ttraining's l1: 0.150836\tvalid_1's l1: 0.188883\n",
      "[1900]\ttraining's l1: 0.147526\tvalid_1's l1: 0.186888\n",
      "[2000]\ttraining's l1: 0.144267\tvalid_1's l1: 0.184862\n",
      "[2100]\ttraining's l1: 0.141167\tvalid_1's l1: 0.182902\n",
      "[2200]\ttraining's l1: 0.138276\tvalid_1's l1: 0.181176\n",
      "[2300]\ttraining's l1: 0.135603\tvalid_1's l1: 0.179687\n",
      "[2400]\ttraining's l1: 0.133224\tvalid_1's l1: 0.178329\n",
      "[2500]\ttraining's l1: 0.130775\tvalid_1's l1: 0.176901\n",
      "[2600]\ttraining's l1: 0.12834\tvalid_1's l1: 0.175452\n",
      "[2700]\ttraining's l1: 0.126097\tvalid_1's l1: 0.174239\n",
      "[2800]\ttraining's l1: 0.124067\tvalid_1's l1: 0.173159\n",
      "[2900]\ttraining's l1: 0.121953\tvalid_1's l1: 0.171992\n",
      "[3000]\ttraining's l1: 0.120038\tvalid_1's l1: 0.170975\n",
      "[3100]\ttraining's l1: 0.118093\tvalid_1's l1: 0.16989\n",
      "[3200]\ttraining's l1: 0.116299\tvalid_1's l1: 0.169012\n",
      "[3300]\ttraining's l1: 0.114437\tvalid_1's l1: 0.16802\n",
      "[3400]\ttraining's l1: 0.112721\tvalid_1's l1: 0.167113\n",
      "[3500]\ttraining's l1: 0.1111\tvalid_1's l1: 0.16631\n",
      "[3600]\ttraining's l1: 0.109516\tvalid_1's l1: 0.165569\n",
      "[3700]\ttraining's l1: 0.108015\tvalid_1's l1: 0.164854\n",
      "[3800]\ttraining's l1: 0.106507\tvalid_1's l1: 0.164096\n",
      "[3900]\ttraining's l1: 0.105049\tvalid_1's l1: 0.163373\n",
      "[4000]\ttraining's l1: 0.103641\tvalid_1's l1: 0.162719\n",
      "[4100]\ttraining's l1: 0.102263\tvalid_1's l1: 0.162082\n",
      "[4200]\ttraining's l1: 0.100919\tvalid_1's l1: 0.161477\n",
      "[4300]\ttraining's l1: 0.0995948\tvalid_1's l1: 0.160852\n",
      "[4400]\ttraining's l1: 0.0983214\tvalid_1's l1: 0.16025\n",
      "[4500]\ttraining's l1: 0.0970973\tvalid_1's l1: 0.159703\n",
      "[4600]\ttraining's l1: 0.0959269\tvalid_1's l1: 0.159178\n",
      "[4700]\ttraining's l1: 0.0947388\tvalid_1's l1: 0.158637\n",
      "[4800]\ttraining's l1: 0.0935794\tvalid_1's l1: 0.15811\n",
      "[4900]\ttraining's l1: 0.0924442\tvalid_1's l1: 0.157559\n",
      "[5000]\ttraining's l1: 0.0913979\tvalid_1's l1: 0.157118\n",
      "[5100]\ttraining's l1: 0.0903545\tvalid_1's l1: 0.156698\n",
      "[5200]\ttraining's l1: 0.0893173\tvalid_1's l1: 0.156264\n",
      "[5300]\ttraining's l1: 0.088287\tvalid_1's l1: 0.155801\n",
      "[5400]\ttraining's l1: 0.0872575\tvalid_1's l1: 0.15535\n",
      "[5500]\ttraining's l1: 0.0862454\tvalid_1's l1: 0.154918\n",
      "[5600]\ttraining's l1: 0.0852926\tvalid_1's l1: 0.154515\n",
      "[5700]\ttraining's l1: 0.0843425\tvalid_1's l1: 0.154111\n",
      "[5800]\ttraining's l1: 0.0834354\tvalid_1's l1: 0.153739\n",
      "[5900]\ttraining's l1: 0.0825601\tvalid_1's l1: 0.153404\n",
      "[6000]\ttraining's l1: 0.0816658\tvalid_1's l1: 0.153049\n",
      "[6100]\ttraining's l1: 0.0808056\tvalid_1's l1: 0.152709\n",
      "[6200]\ttraining's l1: 0.0799533\tvalid_1's l1: 0.152381\n",
      "[6300]\ttraining's l1: 0.0791503\tvalid_1's l1: 0.152097\n",
      "[6400]\ttraining's l1: 0.0783622\tvalid_1's l1: 0.151772\n",
      "[6500]\ttraining's l1: 0.077526\tvalid_1's l1: 0.151446\n",
      "[6600]\ttraining's l1: 0.076764\tvalid_1's l1: 0.151183\n",
      "[6700]\ttraining's l1: 0.0760052\tvalid_1's l1: 0.150908\n",
      "[6800]\ttraining's l1: 0.0752496\tvalid_1's l1: 0.150596\n",
      "[6900]\ttraining's l1: 0.0745009\tvalid_1's l1: 0.150306\n",
      "[7000]\ttraining's l1: 0.0737979\tvalid_1's l1: 0.150039\n",
      "[7100]\ttraining's l1: 0.0731419\tvalid_1's l1: 0.149815\n",
      "[7200]\ttraining's l1: 0.0724409\tvalid_1's l1: 0.149555\n",
      "[7300]\ttraining's l1: 0.0717257\tvalid_1's l1: 0.149276\n",
      "[7400]\ttraining's l1: 0.0710196\tvalid_1's l1: 0.149024\n",
      "[7500]\ttraining's l1: 0.0703226\tvalid_1's l1: 0.14876\n",
      "[7600]\ttraining's l1: 0.0696371\tvalid_1's l1: 0.148467\n",
      "[7700]\ttraining's l1: 0.0689857\tvalid_1's l1: 0.148255\n",
      "[7800]\ttraining's l1: 0.0683516\tvalid_1's l1: 0.147994\n",
      "[7900]\ttraining's l1: 0.0677258\tvalid_1's l1: 0.14776\n",
      "[8000]\ttraining's l1: 0.0671171\tvalid_1's l1: 0.147572\n",
      "[8100]\ttraining's l1: 0.0665311\tvalid_1's l1: 0.147365\n",
      "[8200]\ttraining's l1: 0.0659205\tvalid_1's l1: 0.147159\n",
      "[8300]\ttraining's l1: 0.0653207\tvalid_1's l1: 0.146964\n",
      "[8400]\ttraining's l1: 0.0647283\tvalid_1's l1: 0.146747\n",
      "[8500]\ttraining's l1: 0.0641519\tvalid_1's l1: 0.146548\n",
      "[8600]\ttraining's l1: 0.0635811\tvalid_1's l1: 0.146375\n",
      "[8700]\ttraining's l1: 0.0630283\tvalid_1's l1: 0.146195\n",
      "[8800]\ttraining's l1: 0.0624767\tvalid_1's l1: 0.146014\n",
      "[8900]\ttraining's l1: 0.0619559\tvalid_1's l1: 0.145865\n",
      "[9000]\ttraining's l1: 0.0614131\tvalid_1's l1: 0.145652\n",
      "[9100]\ttraining's l1: 0.0608861\tvalid_1's l1: 0.145497\n",
      "[9200]\ttraining's l1: 0.0603586\tvalid_1's l1: 0.145339\n",
      "[9300]\ttraining's l1: 0.0598554\tvalid_1's l1: 0.145164\n",
      "[9400]\ttraining's l1: 0.059371\tvalid_1's l1: 0.145009\n",
      "[9500]\ttraining's l1: 0.0588674\tvalid_1's l1: 0.144842\n",
      "[9600]\ttraining's l1: 0.0583973\tvalid_1's l1: 0.144692\n",
      "[9700]\ttraining's l1: 0.0579146\tvalid_1's l1: 0.144525\n",
      "[9800]\ttraining's l1: 0.0574417\tvalid_1's l1: 0.144363\n",
      "[9900]\ttraining's l1: 0.056969\tvalid_1's l1: 0.144201\n",
      "[10000]\ttraining's l1: 0.0565141\tvalid_1's l1: 0.144059\n",
      "[10100]\ttraining's l1: 0.0560543\tvalid_1's l1: 0.143915\n",
      "[10200]\ttraining's l1: 0.0556044\tvalid_1's l1: 0.143774\n",
      "[10300]\ttraining's l1: 0.0551363\tvalid_1's l1: 0.143615\n",
      "[10400]\ttraining's l1: 0.0546868\tvalid_1's l1: 0.143487\n",
      "[10500]\ttraining's l1: 0.0542351\tvalid_1's l1: 0.143357\n",
      "[10600]\ttraining's l1: 0.053814\tvalid_1's l1: 0.143215\n",
      "[10700]\ttraining's l1: 0.0533791\tvalid_1's l1: 0.143071\n",
      "[10800]\ttraining's l1: 0.0529566\tvalid_1's l1: 0.142952\n",
      "[10900]\ttraining's l1: 0.0525447\tvalid_1's l1: 0.142826\n",
      "[11000]\ttraining's l1: 0.0521458\tvalid_1's l1: 0.142721\n",
      "[11100]\ttraining's l1: 0.0517527\tvalid_1's l1: 0.142586\n",
      "[11200]\ttraining's l1: 0.0513508\tvalid_1's l1: 0.142475\n",
      "[11300]\ttraining's l1: 0.050962\tvalid_1's l1: 0.142376\n",
      "[11400]\ttraining's l1: 0.0505727\tvalid_1's l1: 0.142261\n",
      "[11500]\ttraining's l1: 0.0501989\tvalid_1's l1: 0.142149\n",
      "[11600]\ttraining's l1: 0.0498155\tvalid_1's l1: 0.142047\n",
      "[11700]\ttraining's l1: 0.0494505\tvalid_1's l1: 0.141953\n",
      "[11800]\ttraining's l1: 0.0490884\tvalid_1's l1: 0.14187\n",
      "[11900]\ttraining's l1: 0.04872\tvalid_1's l1: 0.14178\n",
      "[12000]\ttraining's l1: 0.0483564\tvalid_1's l1: 0.141682\n",
      "[12100]\ttraining's l1: 0.0479822\tvalid_1's l1: 0.141572\n",
      "[12200]\ttraining's l1: 0.0476203\tvalid_1's l1: 0.14148\n",
      "[12300]\ttraining's l1: 0.0472735\tvalid_1's l1: 0.141385\n",
      "[12400]\ttraining's l1: 0.0469242\tvalid_1's l1: 0.141286\n",
      "[12500]\ttraining's l1: 0.0465703\tvalid_1's l1: 0.141196\n",
      "[12600]\ttraining's l1: 0.046225\tvalid_1's l1: 0.141094\n",
      "[12700]\ttraining's l1: 0.0459053\tvalid_1's l1: 0.141003\n",
      "[12800]\ttraining's l1: 0.0455448\tvalid_1's l1: 0.140891\n",
      "[12900]\ttraining's l1: 0.0452087\tvalid_1's l1: 0.140808\n",
      "[13000]\ttraining's l1: 0.0448867\tvalid_1's l1: 0.140734\n",
      "[13100]\ttraining's l1: 0.0445453\tvalid_1's l1: 0.140638\n",
      "[13200]\ttraining's l1: 0.0442297\tvalid_1's l1: 0.140559\n",
      "[13300]\ttraining's l1: 0.0439118\tvalid_1's l1: 0.140462\n",
      "[13400]\ttraining's l1: 0.043621\tvalid_1's l1: 0.14039\n",
      "[13500]\ttraining's l1: 0.0433265\tvalid_1's l1: 0.140308\n",
      "[13600]\ttraining's l1: 0.0430212\tvalid_1's l1: 0.140235\n",
      "[13700]\ttraining's l1: 0.0427151\tvalid_1's l1: 0.140156\n",
      "[13800]\ttraining's l1: 0.0424171\tvalid_1's l1: 0.140083\n",
      "[13900]\ttraining's l1: 0.0421151\tvalid_1's l1: 0.140011\n",
      "[14000]\ttraining's l1: 0.0418213\tvalid_1's l1: 0.139937\n",
      "[14100]\ttraining's l1: 0.0415263\tvalid_1's l1: 0.139864\n",
      "[14200]\ttraining's l1: 0.0412493\tvalid_1's l1: 0.139794\n",
      "[14300]\ttraining's l1: 0.0409708\tvalid_1's l1: 0.13973\n",
      "[14400]\ttraining's l1: 0.0406932\tvalid_1's l1: 0.13966\n",
      "[14500]\ttraining's l1: 0.0404225\tvalid_1's l1: 0.13959\n",
      "[14600]\ttraining's l1: 0.0401457\tvalid_1's l1: 0.139527\n",
      "[14700]\ttraining's l1: 0.0398573\tvalid_1's l1: 0.13945\n",
      "[14800]\ttraining's l1: 0.0395938\tvalid_1's l1: 0.1394\n",
      "[14900]\ttraining's l1: 0.0393154\tvalid_1's l1: 0.139327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0390445\tvalid_1's l1: 0.139259\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0390445\tvalid_1's l1: 0.139259\n",
      "2JHH Fold 0, logMAE: -1.9714206391579985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.362084\tvalid_1's l1: 0.368998\n",
      "[200]\ttraining's l1: 0.302823\tvalid_1's l1: 0.314147\n",
      "[300]\ttraining's l1: 0.272422\tvalid_1's l1: 0.287152\n",
      "[400]\ttraining's l1: 0.252312\tvalid_1's l1: 0.269684\n",
      "[500]\ttraining's l1: 0.237242\tvalid_1's l1: 0.257163\n",
      "[600]\ttraining's l1: 0.224278\tvalid_1's l1: 0.246541\n",
      "[700]\ttraining's l1: 0.214028\tvalid_1's l1: 0.238329\n",
      "[800]\ttraining's l1: 0.20475\tvalid_1's l1: 0.231026\n",
      "[900]\ttraining's l1: 0.197001\tvalid_1's l1: 0.225083\n",
      "[1000]\ttraining's l1: 0.190242\tvalid_1's l1: 0.219995\n",
      "[1100]\ttraining's l1: 0.183642\tvalid_1's l1: 0.21504\n",
      "[1200]\ttraining's l1: 0.177601\tvalid_1's l1: 0.210639\n",
      "[1300]\ttraining's l1: 0.172001\tvalid_1's l1: 0.206457\n",
      "[1400]\ttraining's l1: 0.16709\tvalid_1's l1: 0.202929\n",
      "[1500]\ttraining's l1: 0.162729\tvalid_1's l1: 0.199885\n",
      "[1600]\ttraining's l1: 0.158709\tvalid_1's l1: 0.197198\n",
      "[1700]\ttraining's l1: 0.154762\tvalid_1's l1: 0.194598\n",
      "[1800]\ttraining's l1: 0.151244\tvalid_1's l1: 0.192366\n",
      "[1900]\ttraining's l1: 0.147973\tvalid_1's l1: 0.190262\n",
      "[2000]\ttraining's l1: 0.144723\tvalid_1's l1: 0.188039\n",
      "[2100]\ttraining's l1: 0.141664\tvalid_1's l1: 0.186119\n",
      "[2200]\ttraining's l1: 0.138797\tvalid_1's l1: 0.184391\n",
      "[2300]\ttraining's l1: 0.136101\tvalid_1's l1: 0.182781\n",
      "[2400]\ttraining's l1: 0.133441\tvalid_1's l1: 0.181123\n",
      "[2500]\ttraining's l1: 0.131046\tvalid_1's l1: 0.179714\n",
      "[2600]\ttraining's l1: 0.128716\tvalid_1's l1: 0.178357\n",
      "[2700]\ttraining's l1: 0.126431\tvalid_1's l1: 0.177013\n",
      "[2800]\ttraining's l1: 0.124356\tvalid_1's l1: 0.175848\n",
      "[2900]\ttraining's l1: 0.122355\tvalid_1's l1: 0.174743\n",
      "[3000]\ttraining's l1: 0.120393\tvalid_1's l1: 0.173673\n",
      "[3100]\ttraining's l1: 0.118521\tvalid_1's l1: 0.172609\n",
      "[3200]\ttraining's l1: 0.116621\tvalid_1's l1: 0.171539\n",
      "[3300]\ttraining's l1: 0.114839\tvalid_1's l1: 0.170556\n",
      "[3400]\ttraining's l1: 0.113129\tvalid_1's l1: 0.169705\n",
      "[3500]\ttraining's l1: 0.111503\tvalid_1's l1: 0.168881\n",
      "[3600]\ttraining's l1: 0.109966\tvalid_1's l1: 0.168083\n",
      "[3700]\ttraining's l1: 0.108435\tvalid_1's l1: 0.167295\n",
      "[3800]\ttraining's l1: 0.106888\tvalid_1's l1: 0.16652\n",
      "[3900]\ttraining's l1: 0.105438\tvalid_1's l1: 0.165811\n",
      "[4000]\ttraining's l1: 0.104083\tvalid_1's l1: 0.16519\n",
      "[4100]\ttraining's l1: 0.102696\tvalid_1's l1: 0.164517\n",
      "[4200]\ttraining's l1: 0.101386\tvalid_1's l1: 0.163896\n",
      "[4300]\ttraining's l1: 0.100078\tvalid_1's l1: 0.163282\n",
      "[4400]\ttraining's l1: 0.0988333\tvalid_1's l1: 0.162717\n",
      "[4500]\ttraining's l1: 0.0976413\tvalid_1's l1: 0.162149\n",
      "[4600]\ttraining's l1: 0.0963529\tvalid_1's l1: 0.161529\n",
      "[4700]\ttraining's l1: 0.0951861\tvalid_1's l1: 0.160995\n",
      "[4800]\ttraining's l1: 0.0939945\tvalid_1's l1: 0.160463\n",
      "[4900]\ttraining's l1: 0.0928739\tvalid_1's l1: 0.159957\n",
      "[5000]\ttraining's l1: 0.0916989\tvalid_1's l1: 0.159351\n",
      "[5100]\ttraining's l1: 0.090609\tvalid_1's l1: 0.158844\n",
      "[5200]\ttraining's l1: 0.0895809\tvalid_1's l1: 0.158384\n",
      "[5300]\ttraining's l1: 0.0885577\tvalid_1's l1: 0.157931\n",
      "[5400]\ttraining's l1: 0.0875514\tvalid_1's l1: 0.15746\n",
      "[5500]\ttraining's l1: 0.0865806\tvalid_1's l1: 0.157062\n",
      "[5600]\ttraining's l1: 0.0856493\tvalid_1's l1: 0.156705\n",
      "[5700]\ttraining's l1: 0.0847318\tvalid_1's l1: 0.156341\n",
      "[5800]\ttraining's l1: 0.0837836\tvalid_1's l1: 0.155928\n",
      "[5900]\ttraining's l1: 0.082899\tvalid_1's l1: 0.155584\n",
      "[6000]\ttraining's l1: 0.0820396\tvalid_1's l1: 0.155222\n",
      "[6100]\ttraining's l1: 0.081138\tvalid_1's l1: 0.154819\n",
      "[6200]\ttraining's l1: 0.080311\tvalid_1's l1: 0.154482\n",
      "[6300]\ttraining's l1: 0.0794769\tvalid_1's l1: 0.15414\n",
      "[6400]\ttraining's l1: 0.0786306\tvalid_1's l1: 0.153766\n",
      "[6500]\ttraining's l1: 0.0778522\tvalid_1's l1: 0.15347\n",
      "[6600]\ttraining's l1: 0.0770687\tvalid_1's l1: 0.153184\n",
      "[6700]\ttraining's l1: 0.0763003\tvalid_1's l1: 0.152914\n",
      "[6800]\ttraining's l1: 0.0754905\tvalid_1's l1: 0.152528\n",
      "[6900]\ttraining's l1: 0.0747073\tvalid_1's l1: 0.152213\n",
      "[7000]\ttraining's l1: 0.073933\tvalid_1's l1: 0.151905\n",
      "[7100]\ttraining's l1: 0.073198\tvalid_1's l1: 0.151656\n",
      "[7200]\ttraining's l1: 0.0724707\tvalid_1's l1: 0.15135\n",
      "[7300]\ttraining's l1: 0.0717731\tvalid_1's l1: 0.151079\n",
      "[7400]\ttraining's l1: 0.0710695\tvalid_1's l1: 0.150827\n",
      "[7500]\ttraining's l1: 0.0704205\tvalid_1's l1: 0.150606\n",
      "[7600]\ttraining's l1: 0.0697235\tvalid_1's l1: 0.150343\n",
      "[7700]\ttraining's l1: 0.0690677\tvalid_1's l1: 0.150095\n",
      "[7800]\ttraining's l1: 0.0684372\tvalid_1's l1: 0.149859\n",
      "[7900]\ttraining's l1: 0.0678092\tvalid_1's l1: 0.149646\n",
      "[8000]\ttraining's l1: 0.0671941\tvalid_1's l1: 0.149419\n",
      "[8100]\ttraining's l1: 0.0665827\tvalid_1's l1: 0.149196\n",
      "[8200]\ttraining's l1: 0.0659531\tvalid_1's l1: 0.148952\n",
      "[8300]\ttraining's l1: 0.0653727\tvalid_1's l1: 0.148774\n",
      "[8400]\ttraining's l1: 0.0648195\tvalid_1's l1: 0.148585\n",
      "[8500]\ttraining's l1: 0.0642418\tvalid_1's l1: 0.148386\n",
      "[8600]\ttraining's l1: 0.0636904\tvalid_1's l1: 0.148215\n",
      "[8700]\ttraining's l1: 0.0631216\tvalid_1's l1: 0.148031\n",
      "[8800]\ttraining's l1: 0.0625822\tvalid_1's l1: 0.147853\n",
      "[8900]\ttraining's l1: 0.0620459\tvalid_1's l1: 0.147684\n",
      "[9000]\ttraining's l1: 0.0615256\tvalid_1's l1: 0.147496\n",
      "[9100]\ttraining's l1: 0.0609762\tvalid_1's l1: 0.147289\n",
      "[9200]\ttraining's l1: 0.0604495\tvalid_1's l1: 0.14711\n",
      "[9300]\ttraining's l1: 0.0599278\tvalid_1's l1: 0.146935\n",
      "[9400]\ttraining's l1: 0.0594147\tvalid_1's l1: 0.146762\n",
      "[9500]\ttraining's l1: 0.058908\tvalid_1's l1: 0.146586\n",
      "[9600]\ttraining's l1: 0.0584291\tvalid_1's l1: 0.146426\n",
      "[9700]\ttraining's l1: 0.0579331\tvalid_1's l1: 0.146257\n",
      "[9800]\ttraining's l1: 0.0574483\tvalid_1's l1: 0.14608\n",
      "[9900]\ttraining's l1: 0.0569798\tvalid_1's l1: 0.145906\n",
      "[10000]\ttraining's l1: 0.0565155\tvalid_1's l1: 0.14575\n",
      "[10100]\ttraining's l1: 0.056037\tvalid_1's l1: 0.145588\n",
      "[10200]\ttraining's l1: 0.0555705\tvalid_1's l1: 0.145446\n",
      "[10300]\ttraining's l1: 0.0551318\tvalid_1's l1: 0.145298\n",
      "[10400]\ttraining's l1: 0.0546918\tvalid_1's l1: 0.145179\n",
      "[10500]\ttraining's l1: 0.0542648\tvalid_1's l1: 0.145073\n",
      "[10600]\ttraining's l1: 0.0538441\tvalid_1's l1: 0.144945\n",
      "[10700]\ttraining's l1: 0.0533994\tvalid_1's l1: 0.144798\n",
      "[10800]\ttraining's l1: 0.0529711\tvalid_1's l1: 0.144667\n",
      "[10900]\ttraining's l1: 0.0525502\tvalid_1's l1: 0.144537\n",
      "[11000]\ttraining's l1: 0.0521138\tvalid_1's l1: 0.144413\n",
      "[11100]\ttraining's l1: 0.0517142\tvalid_1's l1: 0.144314\n",
      "[11200]\ttraining's l1: 0.0513148\tvalid_1's l1: 0.1442\n",
      "[11300]\ttraining's l1: 0.0508993\tvalid_1's l1: 0.144088\n",
      "[11400]\ttraining's l1: 0.0505161\tvalid_1's l1: 0.143987\n",
      "[11500]\ttraining's l1: 0.0501322\tvalid_1's l1: 0.143881\n",
      "[11600]\ttraining's l1: 0.0497327\tvalid_1's l1: 0.14376\n",
      "[11700]\ttraining's l1: 0.0493608\tvalid_1's l1: 0.143653\n",
      "[11800]\ttraining's l1: 0.0489823\tvalid_1's l1: 0.143538\n",
      "[11900]\ttraining's l1: 0.0486068\tvalid_1's l1: 0.143429\n",
      "[12000]\ttraining's l1: 0.0482302\tvalid_1's l1: 0.143314\n",
      "[12100]\ttraining's l1: 0.0478679\tvalid_1's l1: 0.143217\n",
      "[12200]\ttraining's l1: 0.0475122\tvalid_1's l1: 0.143119\n",
      "[12300]\ttraining's l1: 0.0471644\tvalid_1's l1: 0.143012\n",
      "[12400]\ttraining's l1: 0.0468237\tvalid_1's l1: 0.142925\n",
      "[12500]\ttraining's l1: 0.0464818\tvalid_1's l1: 0.142825\n",
      "[12600]\ttraining's l1: 0.0461324\tvalid_1's l1: 0.142736\n",
      "[12700]\ttraining's l1: 0.0458145\tvalid_1's l1: 0.142665\n",
      "[12800]\ttraining's l1: 0.0454695\tvalid_1's l1: 0.142566\n",
      "[12900]\ttraining's l1: 0.0451328\tvalid_1's l1: 0.142469\n",
      "[13000]\ttraining's l1: 0.044799\tvalid_1's l1: 0.142382\n",
      "[13100]\ttraining's l1: 0.0444727\tvalid_1's l1: 0.142302\n",
      "[13200]\ttraining's l1: 0.0441643\tvalid_1's l1: 0.142225\n",
      "[13300]\ttraining's l1: 0.0438457\tvalid_1's l1: 0.142147\n",
      "[13400]\ttraining's l1: 0.0435398\tvalid_1's l1: 0.142072\n",
      "[13500]\ttraining's l1: 0.0432336\tvalid_1's l1: 0.142006\n",
      "[13600]\ttraining's l1: 0.0429301\tvalid_1's l1: 0.141912\n",
      "[13700]\ttraining's l1: 0.0426169\tvalid_1's l1: 0.141832\n",
      "[13800]\ttraining's l1: 0.0423205\tvalid_1's l1: 0.14177\n",
      "[13900]\ttraining's l1: 0.0420341\tvalid_1's l1: 0.141707\n",
      "[14000]\ttraining's l1: 0.0417247\tvalid_1's l1: 0.141629\n",
      "[14100]\ttraining's l1: 0.0414428\tvalid_1's l1: 0.141556\n",
      "[14200]\ttraining's l1: 0.041143\tvalid_1's l1: 0.14147\n",
      "[14300]\ttraining's l1: 0.0408528\tvalid_1's l1: 0.1414\n",
      "[14400]\ttraining's l1: 0.0405721\tvalid_1's l1: 0.141337\n",
      "[14500]\ttraining's l1: 0.0403031\tvalid_1's l1: 0.141269\n",
      "[14600]\ttraining's l1: 0.0400284\tvalid_1's l1: 0.141202\n",
      "[14700]\ttraining's l1: 0.0397525\tvalid_1's l1: 0.14112\n",
      "[14800]\ttraining's l1: 0.0394834\tvalid_1's l1: 0.14105\n",
      "[14900]\ttraining's l1: 0.0392186\tvalid_1's l1: 0.140969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0389425\tvalid_1's l1: 0.140905\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0389425\tvalid_1's l1: 0.140905\n",
      "2JHH Fold 1, logMAE: -1.9596917422026616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.363058\tvalid_1's l1: 0.369434\n",
      "[200]\ttraining's l1: 0.304549\tvalid_1's l1: 0.314616\n",
      "[300]\ttraining's l1: 0.273853\tvalid_1's l1: 0.287536\n",
      "[400]\ttraining's l1: 0.252994\tvalid_1's l1: 0.269384\n",
      "[500]\ttraining's l1: 0.238058\tvalid_1's l1: 0.256685\n",
      "[600]\ttraining's l1: 0.22619\tvalid_1's l1: 0.247158\n",
      "[700]\ttraining's l1: 0.215698\tvalid_1's l1: 0.238812\n",
      "[800]\ttraining's l1: 0.207101\tvalid_1's l1: 0.232071\n",
      "[900]\ttraining's l1: 0.199093\tvalid_1's l1: 0.225976\n",
      "[1000]\ttraining's l1: 0.191771\tvalid_1's l1: 0.22049\n",
      "[1100]\ttraining's l1: 0.185124\tvalid_1's l1: 0.215486\n",
      "[1200]\ttraining's l1: 0.179214\tvalid_1's l1: 0.211201\n",
      "[1300]\ttraining's l1: 0.173549\tvalid_1's l1: 0.207192\n",
      "[1400]\ttraining's l1: 0.168369\tvalid_1's l1: 0.203457\n",
      "[1500]\ttraining's l1: 0.163846\tvalid_1's l1: 0.200414\n",
      "[1600]\ttraining's l1: 0.159487\tvalid_1's l1: 0.197266\n",
      "[1700]\ttraining's l1: 0.155458\tvalid_1's l1: 0.194667\n",
      "[1800]\ttraining's l1: 0.151774\tvalid_1's l1: 0.192226\n",
      "[1900]\ttraining's l1: 0.148324\tvalid_1's l1: 0.190016\n",
      "[2000]\ttraining's l1: 0.145043\tvalid_1's l1: 0.188035\n",
      "[2100]\ttraining's l1: 0.142012\tvalid_1's l1: 0.186101\n",
      "[2200]\ttraining's l1: 0.139031\tvalid_1's l1: 0.184209\n",
      "[2300]\ttraining's l1: 0.136275\tvalid_1's l1: 0.182499\n",
      "[2400]\ttraining's l1: 0.133702\tvalid_1's l1: 0.18096\n",
      "[2500]\ttraining's l1: 0.131301\tvalid_1's l1: 0.179593\n",
      "[2600]\ttraining's l1: 0.12893\tvalid_1's l1: 0.178264\n",
      "[2700]\ttraining's l1: 0.126537\tvalid_1's l1: 0.176844\n",
      "[2800]\ttraining's l1: 0.124285\tvalid_1's l1: 0.175621\n",
      "[2900]\ttraining's l1: 0.122206\tvalid_1's l1: 0.174464\n",
      "[3000]\ttraining's l1: 0.120202\tvalid_1's l1: 0.173395\n",
      "[3100]\ttraining's l1: 0.118243\tvalid_1's l1: 0.172289\n",
      "[3200]\ttraining's l1: 0.116431\tvalid_1's l1: 0.171306\n",
      "[3300]\ttraining's l1: 0.114583\tvalid_1's l1: 0.170277\n",
      "[3400]\ttraining's l1: 0.112839\tvalid_1's l1: 0.169347\n",
      "[3500]\ttraining's l1: 0.111232\tvalid_1's l1: 0.168509\n",
      "[3600]\ttraining's l1: 0.109577\tvalid_1's l1: 0.167601\n",
      "[3700]\ttraining's l1: 0.107975\tvalid_1's l1: 0.166738\n",
      "[3800]\ttraining's l1: 0.106422\tvalid_1's l1: 0.165969\n",
      "[3900]\ttraining's l1: 0.104967\tvalid_1's l1: 0.165282\n",
      "[4000]\ttraining's l1: 0.103586\tvalid_1's l1: 0.164618\n",
      "[4100]\ttraining's l1: 0.102202\tvalid_1's l1: 0.163965\n",
      "[4200]\ttraining's l1: 0.100926\tvalid_1's l1: 0.163375\n",
      "[4300]\ttraining's l1: 0.09971\tvalid_1's l1: 0.162841\n",
      "[4400]\ttraining's l1: 0.0983948\tvalid_1's l1: 0.162226\n",
      "[4500]\ttraining's l1: 0.0971551\tvalid_1's l1: 0.161646\n",
      "[4600]\ttraining's l1: 0.0959584\tvalid_1's l1: 0.16105\n",
      "[4700]\ttraining's l1: 0.0947373\tvalid_1's l1: 0.160469\n",
      "[4800]\ttraining's l1: 0.0935658\tvalid_1's l1: 0.159929\n",
      "[4900]\ttraining's l1: 0.0924812\tvalid_1's l1: 0.159448\n",
      "[5000]\ttraining's l1: 0.0913883\tvalid_1's l1: 0.158973\n",
      "[5100]\ttraining's l1: 0.0903189\tvalid_1's l1: 0.158506\n",
      "[5200]\ttraining's l1: 0.0892755\tvalid_1's l1: 0.158067\n",
      "[5300]\ttraining's l1: 0.0882331\tvalid_1's l1: 0.157622\n",
      "[5400]\ttraining's l1: 0.0872274\tvalid_1's l1: 0.157161\n",
      "[5500]\ttraining's l1: 0.0862197\tvalid_1's l1: 0.156728\n",
      "[5600]\ttraining's l1: 0.0852619\tvalid_1's l1: 0.156321\n",
      "[5700]\ttraining's l1: 0.0843537\tvalid_1's l1: 0.155957\n",
      "[5800]\ttraining's l1: 0.0833998\tvalid_1's l1: 0.155531\n",
      "[5900]\ttraining's l1: 0.0825214\tvalid_1's l1: 0.155165\n",
      "[6000]\ttraining's l1: 0.0816504\tvalid_1's l1: 0.154809\n",
      "[6100]\ttraining's l1: 0.0808032\tvalid_1's l1: 0.15446\n",
      "[6200]\ttraining's l1: 0.0799253\tvalid_1's l1: 0.154071\n",
      "[6300]\ttraining's l1: 0.0790718\tvalid_1's l1: 0.15369\n",
      "[6400]\ttraining's l1: 0.0782816\tvalid_1's l1: 0.153388\n",
      "[6500]\ttraining's l1: 0.0775026\tvalid_1's l1: 0.153096\n",
      "[6600]\ttraining's l1: 0.0767313\tvalid_1's l1: 0.152794\n",
      "[6700]\ttraining's l1: 0.075973\tvalid_1's l1: 0.152485\n",
      "[6800]\ttraining's l1: 0.0751967\tvalid_1's l1: 0.152162\n",
      "[6900]\ttraining's l1: 0.0744383\tvalid_1's l1: 0.151866\n",
      "[7000]\ttraining's l1: 0.0737057\tvalid_1's l1: 0.151588\n",
      "[7100]\ttraining's l1: 0.0729922\tvalid_1's l1: 0.151324\n",
      "[7200]\ttraining's l1: 0.0722788\tvalid_1's l1: 0.15108\n",
      "[7300]\ttraining's l1: 0.0716047\tvalid_1's l1: 0.150824\n",
      "[7400]\ttraining's l1: 0.0708913\tvalid_1's l1: 0.150568\n",
      "[7500]\ttraining's l1: 0.0702371\tvalid_1's l1: 0.150302\n",
      "[7600]\ttraining's l1: 0.0696179\tvalid_1's l1: 0.150072\n",
      "[7700]\ttraining's l1: 0.0689114\tvalid_1's l1: 0.14978\n",
      "[7800]\ttraining's l1: 0.0682854\tvalid_1's l1: 0.149565\n",
      "[7900]\ttraining's l1: 0.0676531\tvalid_1's l1: 0.149336\n",
      "[8000]\ttraining's l1: 0.0670269\tvalid_1's l1: 0.149121\n",
      "[8100]\ttraining's l1: 0.0664338\tvalid_1's l1: 0.148919\n",
      "[8200]\ttraining's l1: 0.0658099\tvalid_1's l1: 0.148705\n",
      "[8300]\ttraining's l1: 0.0651974\tvalid_1's l1: 0.148473\n",
      "[8400]\ttraining's l1: 0.0646273\tvalid_1's l1: 0.148283\n",
      "[8500]\ttraining's l1: 0.0640422\tvalid_1's l1: 0.14806\n",
      "[8600]\ttraining's l1: 0.0634739\tvalid_1's l1: 0.147851\n",
      "[8700]\ttraining's l1: 0.0628876\tvalid_1's l1: 0.147643\n",
      "[8800]\ttraining's l1: 0.0623229\tvalid_1's l1: 0.147445\n",
      "[8900]\ttraining's l1: 0.0617822\tvalid_1's l1: 0.147264\n",
      "[9000]\ttraining's l1: 0.061269\tvalid_1's l1: 0.147089\n",
      "[9100]\ttraining's l1: 0.0607315\tvalid_1's l1: 0.146911\n",
      "[9200]\ttraining's l1: 0.0602183\tvalid_1's l1: 0.146731\n",
      "[9300]\ttraining's l1: 0.0596917\tvalid_1's l1: 0.146541\n",
      "[9400]\ttraining's l1: 0.059196\tvalid_1's l1: 0.146401\n",
      "[9500]\ttraining's l1: 0.0587148\tvalid_1's l1: 0.146254\n",
      "[9600]\ttraining's l1: 0.0582224\tvalid_1's l1: 0.146114\n",
      "[9700]\ttraining's l1: 0.0577512\tvalid_1's l1: 0.145955\n",
      "[9800]\ttraining's l1: 0.0572712\tvalid_1's l1: 0.145808\n",
      "[9900]\ttraining's l1: 0.0568098\tvalid_1's l1: 0.145647\n",
      "[10000]\ttraining's l1: 0.0563521\tvalid_1's l1: 0.145518\n",
      "[10100]\ttraining's l1: 0.0559003\tvalid_1's l1: 0.145373\n",
      "[10200]\ttraining's l1: 0.0554498\tvalid_1's l1: 0.145244\n",
      "[10300]\ttraining's l1: 0.0550014\tvalid_1's l1: 0.1451\n",
      "[10400]\ttraining's l1: 0.0545643\tvalid_1's l1: 0.144983\n",
      "[10500]\ttraining's l1: 0.0541251\tvalid_1's l1: 0.144852\n",
      "[10600]\ttraining's l1: 0.0536729\tvalid_1's l1: 0.144707\n",
      "[10700]\ttraining's l1: 0.0532566\tvalid_1's l1: 0.144588\n",
      "[10800]\ttraining's l1: 0.0528298\tvalid_1's l1: 0.144457\n",
      "[10900]\ttraining's l1: 0.0524163\tvalid_1's l1: 0.144335\n",
      "[11000]\ttraining's l1: 0.0520195\tvalid_1's l1: 0.144206\n",
      "[11100]\ttraining's l1: 0.0516203\tvalid_1's l1: 0.144103\n",
      "[11200]\ttraining's l1: 0.0511998\tvalid_1's l1: 0.143971\n",
      "[11300]\ttraining's l1: 0.0507792\tvalid_1's l1: 0.143835\n",
      "[11400]\ttraining's l1: 0.0503982\tvalid_1's l1: 0.143734\n",
      "[11500]\ttraining's l1: 0.0499844\tvalid_1's l1: 0.143587\n",
      "[11600]\ttraining's l1: 0.0496177\tvalid_1's l1: 0.143498\n",
      "[11700]\ttraining's l1: 0.0492486\tvalid_1's l1: 0.143393\n",
      "[11800]\ttraining's l1: 0.0488698\tvalid_1's l1: 0.14329\n",
      "[11900]\ttraining's l1: 0.0485096\tvalid_1's l1: 0.1432\n",
      "[12000]\ttraining's l1: 0.0481532\tvalid_1's l1: 0.143105\n",
      "[12100]\ttraining's l1: 0.0477863\tvalid_1's l1: 0.143003\n",
      "[12200]\ttraining's l1: 0.0474362\tvalid_1's l1: 0.142908\n",
      "[12300]\ttraining's l1: 0.0470929\tvalid_1's l1: 0.142813\n",
      "[12400]\ttraining's l1: 0.0467474\tvalid_1's l1: 0.142714\n",
      "[12500]\ttraining's l1: 0.0464003\tvalid_1's l1: 0.142613\n",
      "[12600]\ttraining's l1: 0.0460536\tvalid_1's l1: 0.14253\n",
      "[12700]\ttraining's l1: 0.0456955\tvalid_1's l1: 0.142433\n",
      "[12800]\ttraining's l1: 0.0453625\tvalid_1's l1: 0.142348\n",
      "[12900]\ttraining's l1: 0.0450383\tvalid_1's l1: 0.142272\n",
      "[13000]\ttraining's l1: 0.0447034\tvalid_1's l1: 0.142183\n",
      "[13100]\ttraining's l1: 0.0443862\tvalid_1's l1: 0.142101\n",
      "[13200]\ttraining's l1: 0.0440596\tvalid_1's l1: 0.142021\n",
      "[13300]\ttraining's l1: 0.0437374\tvalid_1's l1: 0.141934\n",
      "[13400]\ttraining's l1: 0.0434268\tvalid_1's l1: 0.14185\n",
      "[13500]\ttraining's l1: 0.0431231\tvalid_1's l1: 0.141769\n",
      "[13600]\ttraining's l1: 0.0428316\tvalid_1's l1: 0.141702\n",
      "[13700]\ttraining's l1: 0.0425333\tvalid_1's l1: 0.141634\n",
      "[13800]\ttraining's l1: 0.0422478\tvalid_1's l1: 0.141571\n",
      "[13900]\ttraining's l1: 0.0419504\tvalid_1's l1: 0.141494\n",
      "[14000]\ttraining's l1: 0.0416552\tvalid_1's l1: 0.14141\n",
      "[14100]\ttraining's l1: 0.0413614\tvalid_1's l1: 0.141339\n",
      "[14200]\ttraining's l1: 0.0410717\tvalid_1's l1: 0.141268\n",
      "[14300]\ttraining's l1: 0.0407735\tvalid_1's l1: 0.141186\n",
      "[14400]\ttraining's l1: 0.0404994\tvalid_1's l1: 0.141115\n",
      "[14500]\ttraining's l1: 0.0402267\tvalid_1's l1: 0.141046\n",
      "[14600]\ttraining's l1: 0.0399623\tvalid_1's l1: 0.140987\n",
      "[14700]\ttraining's l1: 0.0396896\tvalid_1's l1: 0.140916\n",
      "[14800]\ttraining's l1: 0.0394268\tvalid_1's l1: 0.140848\n",
      "[14900]\ttraining's l1: 0.0391593\tvalid_1's l1: 0.14078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0389059\tvalid_1's l1: 0.140708\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0389059\tvalid_1's l1: 0.140708\n",
      "2JHH Fold 2, logMAE: -1.9610710555116164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.360045\tvalid_1's l1: 0.362661\n",
      "[200]\ttraining's l1: 0.303622\tvalid_1's l1: 0.309649\n",
      "[300]\ttraining's l1: 0.273358\tvalid_1's l1: 0.282689\n",
      "[400]\ttraining's l1: 0.252351\tvalid_1's l1: 0.264814\n",
      "[500]\ttraining's l1: 0.23692\tvalid_1's l1: 0.252023\n",
      "[600]\ttraining's l1: 0.224176\tvalid_1's l1: 0.241757\n",
      "[700]\ttraining's l1: 0.213393\tvalid_1's l1: 0.233283\n",
      "[800]\ttraining's l1: 0.204586\tvalid_1's l1: 0.226642\n",
      "[900]\ttraining's l1: 0.196531\tvalid_1's l1: 0.220532\n",
      "[1000]\ttraining's l1: 0.189526\tvalid_1's l1: 0.215383\n",
      "[1100]\ttraining's l1: 0.182765\tvalid_1's l1: 0.210425\n",
      "[1200]\ttraining's l1: 0.176545\tvalid_1's l1: 0.205853\n",
      "[1300]\ttraining's l1: 0.171055\tvalid_1's l1: 0.202003\n",
      "[1400]\ttraining's l1: 0.166461\tvalid_1's l1: 0.198891\n",
      "[1500]\ttraining's l1: 0.161939\tvalid_1's l1: 0.195855\n",
      "[1600]\ttraining's l1: 0.157628\tvalid_1's l1: 0.193033\n",
      "[1700]\ttraining's l1: 0.153772\tvalid_1's l1: 0.190506\n",
      "[1800]\ttraining's l1: 0.150171\tvalid_1's l1: 0.188149\n",
      "[1900]\ttraining's l1: 0.146776\tvalid_1's l1: 0.186028\n",
      "[2000]\ttraining's l1: 0.143753\tvalid_1's l1: 0.184154\n",
      "[2100]\ttraining's l1: 0.140599\tvalid_1's l1: 0.182198\n",
      "[2200]\ttraining's l1: 0.137833\tvalid_1's l1: 0.180609\n",
      "[2300]\ttraining's l1: 0.135048\tvalid_1's l1: 0.178934\n",
      "[2400]\ttraining's l1: 0.132613\tvalid_1's l1: 0.177542\n",
      "[2500]\ttraining's l1: 0.130226\tvalid_1's l1: 0.176177\n",
      "[2600]\ttraining's l1: 0.127821\tvalid_1's l1: 0.174705\n",
      "[2700]\ttraining's l1: 0.125641\tvalid_1's l1: 0.173538\n",
      "[2800]\ttraining's l1: 0.123547\tvalid_1's l1: 0.172371\n",
      "[2900]\ttraining's l1: 0.121386\tvalid_1's l1: 0.171147\n",
      "[3000]\ttraining's l1: 0.119317\tvalid_1's l1: 0.169983\n",
      "[3100]\ttraining's l1: 0.117436\tvalid_1's l1: 0.168978\n",
      "[3200]\ttraining's l1: 0.115699\tvalid_1's l1: 0.168128\n",
      "[3300]\ttraining's l1: 0.11404\tvalid_1's l1: 0.167294\n",
      "[3400]\ttraining's l1: 0.112283\tvalid_1's l1: 0.166374\n",
      "[3500]\ttraining's l1: 0.110658\tvalid_1's l1: 0.165557\n",
      "[3600]\ttraining's l1: 0.109102\tvalid_1's l1: 0.164807\n",
      "[3700]\ttraining's l1: 0.107578\tvalid_1's l1: 0.164028\n",
      "[3800]\ttraining's l1: 0.106058\tvalid_1's l1: 0.163248\n",
      "[3900]\ttraining's l1: 0.104613\tvalid_1's l1: 0.162531\n",
      "[4000]\ttraining's l1: 0.10318\tvalid_1's l1: 0.161828\n",
      "[4100]\ttraining's l1: 0.101734\tvalid_1's l1: 0.161096\n",
      "[4200]\ttraining's l1: 0.100451\tvalid_1's l1: 0.160462\n",
      "[4300]\ttraining's l1: 0.0991891\tvalid_1's l1: 0.159875\n",
      "[4400]\ttraining's l1: 0.097959\tvalid_1's l1: 0.15936\n",
      "[4500]\ttraining's l1: 0.0967086\tvalid_1's l1: 0.158773\n",
      "[4600]\ttraining's l1: 0.0954824\tvalid_1's l1: 0.158212\n",
      "[4700]\ttraining's l1: 0.0943168\tvalid_1's l1: 0.157724\n",
      "[4800]\ttraining's l1: 0.0931941\tvalid_1's l1: 0.157235\n",
      "[4900]\ttraining's l1: 0.0920656\tvalid_1's l1: 0.15676\n",
      "[5000]\ttraining's l1: 0.0910178\tvalid_1's l1: 0.156339\n",
      "[5100]\ttraining's l1: 0.0899495\tvalid_1's l1: 0.155871\n",
      "[5200]\ttraining's l1: 0.0889185\tvalid_1's l1: 0.155459\n",
      "[5300]\ttraining's l1: 0.08784\tvalid_1's l1: 0.154949\n",
      "[5400]\ttraining's l1: 0.0868606\tvalid_1's l1: 0.15452\n",
      "[5500]\ttraining's l1: 0.0859598\tvalid_1's l1: 0.154169\n",
      "[5600]\ttraining's l1: 0.0850522\tvalid_1's l1: 0.153793\n",
      "[5700]\ttraining's l1: 0.0841146\tvalid_1's l1: 0.153398\n",
      "[5800]\ttraining's l1: 0.0831834\tvalid_1's l1: 0.153007\n",
      "[5900]\ttraining's l1: 0.082265\tvalid_1's l1: 0.152642\n",
      "[6000]\ttraining's l1: 0.0814079\tvalid_1's l1: 0.152309\n",
      "[6100]\ttraining's l1: 0.0805409\tvalid_1's l1: 0.151957\n",
      "[6200]\ttraining's l1: 0.0796572\tvalid_1's l1: 0.151552\n",
      "[6300]\ttraining's l1: 0.0788077\tvalid_1's l1: 0.151208\n",
      "[6400]\ttraining's l1: 0.077989\tvalid_1's l1: 0.150845\n",
      "[6500]\ttraining's l1: 0.0772122\tvalid_1's l1: 0.150547\n",
      "[6600]\ttraining's l1: 0.0764804\tvalid_1's l1: 0.15027\n",
      "[6700]\ttraining's l1: 0.0756975\tvalid_1's l1: 0.149974\n",
      "[6800]\ttraining's l1: 0.0749701\tvalid_1's l1: 0.14968\n",
      "[6900]\ttraining's l1: 0.0742168\tvalid_1's l1: 0.149381\n",
      "[7000]\ttraining's l1: 0.0735184\tvalid_1's l1: 0.149151\n",
      "[7100]\ttraining's l1: 0.072791\tvalid_1's l1: 0.148882\n",
      "[7200]\ttraining's l1: 0.0720972\tvalid_1's l1: 0.14863\n",
      "[7300]\ttraining's l1: 0.0714048\tvalid_1's l1: 0.148384\n",
      "[7400]\ttraining's l1: 0.0707231\tvalid_1's l1: 0.148142\n",
      "[7500]\ttraining's l1: 0.0700386\tvalid_1's l1: 0.147882\n",
      "[7600]\ttraining's l1: 0.0693762\tvalid_1's l1: 0.147644\n",
      "[7700]\ttraining's l1: 0.0687345\tvalid_1's l1: 0.147414\n",
      "[7800]\ttraining's l1: 0.0680717\tvalid_1's l1: 0.147156\n",
      "[7900]\ttraining's l1: 0.0674408\tvalid_1's l1: 0.146949\n",
      "[8000]\ttraining's l1: 0.0668306\tvalid_1's l1: 0.146695\n",
      "[8100]\ttraining's l1: 0.0662165\tvalid_1's l1: 0.146466\n",
      "[8200]\ttraining's l1: 0.0656189\tvalid_1's l1: 0.146256\n",
      "[8300]\ttraining's l1: 0.0650205\tvalid_1's l1: 0.146034\n",
      "[8400]\ttraining's l1: 0.0644077\tvalid_1's l1: 0.145828\n",
      "[8500]\ttraining's l1: 0.0638249\tvalid_1's l1: 0.145624\n",
      "[8600]\ttraining's l1: 0.0632606\tvalid_1's l1: 0.145424\n",
      "[8700]\ttraining's l1: 0.0626918\tvalid_1's l1: 0.145229\n",
      "[8800]\ttraining's l1: 0.062145\tvalid_1's l1: 0.14506\n",
      "[8900]\ttraining's l1: 0.0616245\tvalid_1's l1: 0.144895\n",
      "[9000]\ttraining's l1: 0.0610721\tvalid_1's l1: 0.144701\n",
      "[9100]\ttraining's l1: 0.0605466\tvalid_1's l1: 0.144517\n",
      "[9200]\ttraining's l1: 0.0600329\tvalid_1's l1: 0.144351\n",
      "[9300]\ttraining's l1: 0.0595293\tvalid_1's l1: 0.144212\n",
      "[9400]\ttraining's l1: 0.0590155\tvalid_1's l1: 0.144042\n",
      "[9500]\ttraining's l1: 0.0585223\tvalid_1's l1: 0.143886\n",
      "[9600]\ttraining's l1: 0.0580286\tvalid_1's l1: 0.143711\n",
      "[9700]\ttraining's l1: 0.0575396\tvalid_1's l1: 0.143559\n",
      "[9800]\ttraining's l1: 0.0570456\tvalid_1's l1: 0.143414\n",
      "[9900]\ttraining's l1: 0.0565804\tvalid_1's l1: 0.143273\n",
      "[10000]\ttraining's l1: 0.0561177\tvalid_1's l1: 0.143134\n",
      "[10100]\ttraining's l1: 0.055654\tvalid_1's l1: 0.142986\n",
      "[10200]\ttraining's l1: 0.0551951\tvalid_1's l1: 0.142829\n",
      "[10300]\ttraining's l1: 0.0547482\tvalid_1's l1: 0.142712\n",
      "[10400]\ttraining's l1: 0.0542944\tvalid_1's l1: 0.142568\n",
      "[10500]\ttraining's l1: 0.0538439\tvalid_1's l1: 0.142422\n",
      "[10600]\ttraining's l1: 0.0534029\tvalid_1's l1: 0.142285\n",
      "[10700]\ttraining's l1: 0.0529896\tvalid_1's l1: 0.142163\n",
      "[10800]\ttraining's l1: 0.0525494\tvalid_1's l1: 0.142031\n",
      "[10900]\ttraining's l1: 0.0521497\tvalid_1's l1: 0.141901\n",
      "[11000]\ttraining's l1: 0.0517556\tvalid_1's l1: 0.141793\n",
      "[11100]\ttraining's l1: 0.0513399\tvalid_1's l1: 0.141665\n",
      "[11200]\ttraining's l1: 0.0509366\tvalid_1's l1: 0.141559\n",
      "[11300]\ttraining's l1: 0.050542\tvalid_1's l1: 0.141455\n",
      "[11400]\ttraining's l1: 0.0501531\tvalid_1's l1: 0.141329\n",
      "[11500]\ttraining's l1: 0.0497685\tvalid_1's l1: 0.14121\n",
      "[11600]\ttraining's l1: 0.0493713\tvalid_1's l1: 0.141091\n",
      "[11700]\ttraining's l1: 0.0489868\tvalid_1's l1: 0.140988\n",
      "[11800]\ttraining's l1: 0.0486164\tvalid_1's l1: 0.140886\n",
      "[11900]\ttraining's l1: 0.0482498\tvalid_1's l1: 0.140788\n",
      "[12000]\ttraining's l1: 0.047883\tvalid_1's l1: 0.140687\n",
      "[12100]\ttraining's l1: 0.0475249\tvalid_1's l1: 0.140598\n",
      "[12200]\ttraining's l1: 0.0471683\tvalid_1's l1: 0.140511\n",
      "[12300]\ttraining's l1: 0.0467833\tvalid_1's l1: 0.140397\n",
      "[12400]\ttraining's l1: 0.0464378\tvalid_1's l1: 0.140307\n",
      "[12500]\ttraining's l1: 0.0461053\tvalid_1's l1: 0.140218\n",
      "[12600]\ttraining's l1: 0.0457822\tvalid_1's l1: 0.140126\n",
      "[12700]\ttraining's l1: 0.0454407\tvalid_1's l1: 0.140033\n",
      "[12800]\ttraining's l1: 0.045104\tvalid_1's l1: 0.139942\n",
      "[12900]\ttraining's l1: 0.0447746\tvalid_1's l1: 0.139863\n",
      "[13000]\ttraining's l1: 0.0444439\tvalid_1's l1: 0.139775\n",
      "[13100]\ttraining's l1: 0.0441163\tvalid_1's l1: 0.13969\n",
      "[13200]\ttraining's l1: 0.0437946\tvalid_1's l1: 0.139583\n",
      "[13300]\ttraining's l1: 0.0434849\tvalid_1's l1: 0.139514\n",
      "[13400]\ttraining's l1: 0.0431732\tvalid_1's l1: 0.13944\n",
      "[13500]\ttraining's l1: 0.0428622\tvalid_1's l1: 0.139366\n",
      "[13600]\ttraining's l1: 0.0425671\tvalid_1's l1: 0.139275\n",
      "[13700]\ttraining's l1: 0.0422639\tvalid_1's l1: 0.139206\n",
      "[13800]\ttraining's l1: 0.0419707\tvalid_1's l1: 0.139129\n",
      "[13900]\ttraining's l1: 0.0416837\tvalid_1's l1: 0.139057\n",
      "[14000]\ttraining's l1: 0.0413979\tvalid_1's l1: 0.138982\n",
      "[14100]\ttraining's l1: 0.0410994\tvalid_1's l1: 0.138905\n",
      "[14200]\ttraining's l1: 0.0408006\tvalid_1's l1: 0.138815\n",
      "[14300]\ttraining's l1: 0.0405179\tvalid_1's l1: 0.138744\n",
      "[14400]\ttraining's l1: 0.0402501\tvalid_1's l1: 0.138674\n",
      "[14500]\ttraining's l1: 0.039961\tvalid_1's l1: 0.138597\n",
      "[14600]\ttraining's l1: 0.0396862\tvalid_1's l1: 0.138523\n",
      "[14700]\ttraining's l1: 0.0394099\tvalid_1's l1: 0.138456\n",
      "[14800]\ttraining's l1: 0.0391344\tvalid_1's l1: 0.138376\n",
      "[14900]\ttraining's l1: 0.0388654\tvalid_1's l1: 0.138305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0386056\tvalid_1's l1: 0.138246\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0386056\tvalid_1's l1: 0.138246\n",
      "2JHH Fold 3, logMAE: -1.9787239564917325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.362183\tvalid_1's l1: 0.368712\n",
      "[200]\ttraining's l1: 0.304345\tvalid_1's l1: 0.313535\n",
      "[300]\ttraining's l1: 0.274865\tvalid_1's l1: 0.286753\n",
      "[400]\ttraining's l1: 0.254846\tvalid_1's l1: 0.269462\n",
      "[500]\ttraining's l1: 0.238928\tvalid_1's l1: 0.255866\n",
      "[600]\ttraining's l1: 0.22612\tvalid_1's l1: 0.245671\n",
      "[700]\ttraining's l1: 0.215641\tvalid_1's l1: 0.237405\n",
      "[800]\ttraining's l1: 0.206129\tvalid_1's l1: 0.230028\n",
      "[900]\ttraining's l1: 0.198057\tvalid_1's l1: 0.223971\n",
      "[1000]\ttraining's l1: 0.190414\tvalid_1's l1: 0.218278\n",
      "[1100]\ttraining's l1: 0.183723\tvalid_1's l1: 0.213367\n",
      "[1200]\ttraining's l1: 0.177838\tvalid_1's l1: 0.209174\n",
      "[1300]\ttraining's l1: 0.172413\tvalid_1's l1: 0.20525\n",
      "[1400]\ttraining's l1: 0.167405\tvalid_1's l1: 0.201728\n",
      "[1500]\ttraining's l1: 0.163034\tvalid_1's l1: 0.198925\n",
      "[1600]\ttraining's l1: 0.158977\tvalid_1's l1: 0.196256\n",
      "[1700]\ttraining's l1: 0.155181\tvalid_1's l1: 0.193801\n",
      "[1800]\ttraining's l1: 0.15145\tvalid_1's l1: 0.191359\n",
      "[1900]\ttraining's l1: 0.148057\tvalid_1's l1: 0.189136\n",
      "[2000]\ttraining's l1: 0.144671\tvalid_1's l1: 0.186938\n",
      "[2100]\ttraining's l1: 0.141566\tvalid_1's l1: 0.184994\n",
      "[2200]\ttraining's l1: 0.138744\tvalid_1's l1: 0.183318\n",
      "[2300]\ttraining's l1: 0.136094\tvalid_1's l1: 0.181734\n",
      "[2400]\ttraining's l1: 0.133611\tvalid_1's l1: 0.180296\n",
      "[2500]\ttraining's l1: 0.130952\tvalid_1's l1: 0.178684\n",
      "[2600]\ttraining's l1: 0.128622\tvalid_1's l1: 0.177386\n",
      "[2700]\ttraining's l1: 0.126381\tvalid_1's l1: 0.176201\n",
      "[2800]\ttraining's l1: 0.124259\tvalid_1's l1: 0.175045\n",
      "[2900]\ttraining's l1: 0.12224\tvalid_1's l1: 0.173962\n",
      "[3000]\ttraining's l1: 0.120301\tvalid_1's l1: 0.172946\n",
      "[3100]\ttraining's l1: 0.118414\tvalid_1's l1: 0.171959\n",
      "[3200]\ttraining's l1: 0.116604\tvalid_1's l1: 0.171028\n",
      "[3300]\ttraining's l1: 0.114848\tvalid_1's l1: 0.170091\n",
      "[3400]\ttraining's l1: 0.113152\tvalid_1's l1: 0.169227\n",
      "[3500]\ttraining's l1: 0.111501\tvalid_1's l1: 0.168418\n",
      "[3600]\ttraining's l1: 0.1099\tvalid_1's l1: 0.167596\n",
      "[3700]\ttraining's l1: 0.108328\tvalid_1's l1: 0.166862\n",
      "[3800]\ttraining's l1: 0.106708\tvalid_1's l1: 0.165988\n",
      "[3900]\ttraining's l1: 0.105238\tvalid_1's l1: 0.165258\n",
      "[4000]\ttraining's l1: 0.10375\tvalid_1's l1: 0.164523\n",
      "[4100]\ttraining's l1: 0.102429\tvalid_1's l1: 0.163929\n",
      "[4200]\ttraining's l1: 0.101123\tvalid_1's l1: 0.163323\n",
      "[4300]\ttraining's l1: 0.0998059\tvalid_1's l1: 0.162727\n",
      "[4400]\ttraining's l1: 0.0985867\tvalid_1's l1: 0.162164\n",
      "[4500]\ttraining's l1: 0.0973532\tvalid_1's l1: 0.161591\n",
      "[4600]\ttraining's l1: 0.0961289\tvalid_1's l1: 0.160985\n",
      "[4700]\ttraining's l1: 0.0950189\tvalid_1's l1: 0.160483\n",
      "[4800]\ttraining's l1: 0.0938282\tvalid_1's l1: 0.159948\n",
      "[4900]\ttraining's l1: 0.0926721\tvalid_1's l1: 0.159386\n",
      "[5000]\ttraining's l1: 0.0915994\tvalid_1's l1: 0.158922\n",
      "[5100]\ttraining's l1: 0.0905426\tvalid_1's l1: 0.15847\n",
      "[5200]\ttraining's l1: 0.0895013\tvalid_1's l1: 0.158031\n",
      "[5300]\ttraining's l1: 0.0884805\tvalid_1's l1: 0.157625\n",
      "[5400]\ttraining's l1: 0.0874837\tvalid_1's l1: 0.157183\n",
      "[5500]\ttraining's l1: 0.0863947\tvalid_1's l1: 0.156643\n",
      "[5600]\ttraining's l1: 0.0854124\tvalid_1's l1: 0.156208\n",
      "[5700]\ttraining's l1: 0.084472\tvalid_1's l1: 0.155856\n",
      "[5800]\ttraining's l1: 0.0835284\tvalid_1's l1: 0.155431\n",
      "[5900]\ttraining's l1: 0.0826375\tvalid_1's l1: 0.155042\n",
      "[6000]\ttraining's l1: 0.0817638\tvalid_1's l1: 0.154676\n",
      "[6100]\ttraining's l1: 0.0808992\tvalid_1's l1: 0.154326\n",
      "[6200]\ttraining's l1: 0.0800351\tvalid_1's l1: 0.153979\n",
      "[6300]\ttraining's l1: 0.079249\tvalid_1's l1: 0.153707\n",
      "[6400]\ttraining's l1: 0.0784175\tvalid_1's l1: 0.153383\n",
      "[6500]\ttraining's l1: 0.0776227\tvalid_1's l1: 0.153074\n",
      "[6600]\ttraining's l1: 0.0768488\tvalid_1's l1: 0.15279\n",
      "[6700]\ttraining's l1: 0.076093\tvalid_1's l1: 0.152517\n",
      "[6800]\ttraining's l1: 0.0753232\tvalid_1's l1: 0.152199\n",
      "[6900]\ttraining's l1: 0.0745896\tvalid_1's l1: 0.151912\n",
      "[7000]\ttraining's l1: 0.0738614\tvalid_1's l1: 0.151627\n",
      "[7100]\ttraining's l1: 0.0731275\tvalid_1's l1: 0.151342\n",
      "[7200]\ttraining's l1: 0.0724438\tvalid_1's l1: 0.151073\n",
      "[7300]\ttraining's l1: 0.0717444\tvalid_1's l1: 0.150816\n",
      "[7400]\ttraining's l1: 0.0710436\tvalid_1's l1: 0.150572\n",
      "[7500]\ttraining's l1: 0.0703818\tvalid_1's l1: 0.150327\n",
      "[7600]\ttraining's l1: 0.0697116\tvalid_1's l1: 0.150069\n",
      "[7700]\ttraining's l1: 0.0690775\tvalid_1's l1: 0.149815\n",
      "[7800]\ttraining's l1: 0.0684222\tvalid_1's l1: 0.149562\n",
      "[7900]\ttraining's l1: 0.0677921\tvalid_1's l1: 0.149327\n",
      "[8000]\ttraining's l1: 0.0672047\tvalid_1's l1: 0.149135\n",
      "[8100]\ttraining's l1: 0.0665872\tvalid_1's l1: 0.148889\n",
      "[8200]\ttraining's l1: 0.0660194\tvalid_1's l1: 0.148681\n",
      "[8300]\ttraining's l1: 0.0654337\tvalid_1's l1: 0.148481\n",
      "[8400]\ttraining's l1: 0.0648504\tvalid_1's l1: 0.148261\n",
      "[8500]\ttraining's l1: 0.0642885\tvalid_1's l1: 0.148079\n",
      "[8600]\ttraining's l1: 0.0637313\tvalid_1's l1: 0.147879\n",
      "[8700]\ttraining's l1: 0.063183\tvalid_1's l1: 0.147691\n",
      "[8800]\ttraining's l1: 0.06263\tvalid_1's l1: 0.1475\n",
      "[8900]\ttraining's l1: 0.0621019\tvalid_1's l1: 0.147306\n",
      "[9000]\ttraining's l1: 0.0615643\tvalid_1's l1: 0.147106\n",
      "[9100]\ttraining's l1: 0.0610408\tvalid_1's l1: 0.146921\n",
      "[9200]\ttraining's l1: 0.0605487\tvalid_1's l1: 0.14677\n",
      "[9300]\ttraining's l1: 0.0600176\tvalid_1's l1: 0.146582\n",
      "[9400]\ttraining's l1: 0.0595199\tvalid_1's l1: 0.146413\n",
      "[9500]\ttraining's l1: 0.0590104\tvalid_1's l1: 0.146254\n",
      "[9600]\ttraining's l1: 0.0584946\tvalid_1's l1: 0.146094\n",
      "[9700]\ttraining's l1: 0.0580116\tvalid_1's l1: 0.145928\n",
      "[9800]\ttraining's l1: 0.0575297\tvalid_1's l1: 0.145751\n",
      "[9900]\ttraining's l1: 0.0570694\tvalid_1's l1: 0.145619\n",
      "[10000]\ttraining's l1: 0.0566042\tvalid_1's l1: 0.145473\n",
      "[10100]\ttraining's l1: 0.0561377\tvalid_1's l1: 0.145307\n",
      "[10200]\ttraining's l1: 0.0556882\tvalid_1's l1: 0.145165\n",
      "[10300]\ttraining's l1: 0.0552158\tvalid_1's l1: 0.145016\n",
      "[10400]\ttraining's l1: 0.0547544\tvalid_1's l1: 0.144876\n",
      "[10500]\ttraining's l1: 0.0543136\tvalid_1's l1: 0.144723\n",
      "[10600]\ttraining's l1: 0.0538715\tvalid_1's l1: 0.144572\n",
      "[10700]\ttraining's l1: 0.0534375\tvalid_1's l1: 0.144442\n",
      "[10800]\ttraining's l1: 0.0530126\tvalid_1's l1: 0.144305\n",
      "[10900]\ttraining's l1: 0.0525851\tvalid_1's l1: 0.144165\n",
      "[11000]\ttraining's l1: 0.0521795\tvalid_1's l1: 0.144052\n",
      "[11100]\ttraining's l1: 0.051751\tvalid_1's l1: 0.14393\n",
      "[11200]\ttraining's l1: 0.051355\tvalid_1's l1: 0.143814\n",
      "[11300]\ttraining's l1: 0.0509628\tvalid_1's l1: 0.1437\n",
      "[11400]\ttraining's l1: 0.0505675\tvalid_1's l1: 0.143586\n",
      "[11500]\ttraining's l1: 0.0502066\tvalid_1's l1: 0.143498\n",
      "[11600]\ttraining's l1: 0.0498136\tvalid_1's l1: 0.143389\n",
      "[11700]\ttraining's l1: 0.049441\tvalid_1's l1: 0.14328\n",
      "[11800]\ttraining's l1: 0.0490663\tvalid_1's l1: 0.143163\n",
      "[11900]\ttraining's l1: 0.0486973\tvalid_1's l1: 0.143061\n",
      "[12000]\ttraining's l1: 0.0483346\tvalid_1's l1: 0.142963\n",
      "[12100]\ttraining's l1: 0.0479755\tvalid_1's l1: 0.142863\n",
      "[12200]\ttraining's l1: 0.047626\tvalid_1's l1: 0.142762\n",
      "[12300]\ttraining's l1: 0.0472688\tvalid_1's l1: 0.142658\n",
      "[12400]\ttraining's l1: 0.0469119\tvalid_1's l1: 0.142556\n",
      "[12500]\ttraining's l1: 0.0465743\tvalid_1's l1: 0.142466\n",
      "[12600]\ttraining's l1: 0.0462384\tvalid_1's l1: 0.142362\n",
      "[12700]\ttraining's l1: 0.0459052\tvalid_1's l1: 0.142269\n",
      "[12800]\ttraining's l1: 0.0455694\tvalid_1's l1: 0.14218\n",
      "[12900]\ttraining's l1: 0.0452409\tvalid_1's l1: 0.142081\n",
      "[13000]\ttraining's l1: 0.044921\tvalid_1's l1: 0.141999\n",
      "[13100]\ttraining's l1: 0.0446051\tvalid_1's l1: 0.141907\n",
      "[13200]\ttraining's l1: 0.0442775\tvalid_1's l1: 0.141813\n",
      "[13300]\ttraining's l1: 0.0439518\tvalid_1's l1: 0.141717\n",
      "[13400]\ttraining's l1: 0.0436319\tvalid_1's l1: 0.141626\n",
      "[13500]\ttraining's l1: 0.0433126\tvalid_1's l1: 0.141536\n",
      "[13600]\ttraining's l1: 0.0429988\tvalid_1's l1: 0.141453\n",
      "[13700]\ttraining's l1: 0.0426972\tvalid_1's l1: 0.141368\n",
      "[13800]\ttraining's l1: 0.0423887\tvalid_1's l1: 0.141287\n",
      "[13900]\ttraining's l1: 0.0420914\tvalid_1's l1: 0.141207\n",
      "[14000]\ttraining's l1: 0.0418046\tvalid_1's l1: 0.14115\n",
      "[14100]\ttraining's l1: 0.041504\tvalid_1's l1: 0.141064\n",
      "[14200]\ttraining's l1: 0.0412022\tvalid_1's l1: 0.140984\n",
      "[14300]\ttraining's l1: 0.0409102\tvalid_1's l1: 0.140922\n",
      "[14400]\ttraining's l1: 0.0406183\tvalid_1's l1: 0.140837\n",
      "[14500]\ttraining's l1: 0.0403349\tvalid_1's l1: 0.14076\n",
      "[14600]\ttraining's l1: 0.0400621\tvalid_1's l1: 0.140683\n",
      "[14700]\ttraining's l1: 0.039786\tvalid_1's l1: 0.140612\n",
      "[14800]\ttraining's l1: 0.0395143\tvalid_1's l1: 0.140548\n",
      "[14900]\ttraining's l1: 0.0392431\tvalid_1's l1: 0.140476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0389709\tvalid_1's l1: 0.140399\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0389709\tvalid_1's l1: 0.140399\n",
      "2JHH Fold 4, logMAE: -1.9632670791269073\n",
      "*** Training Model for 2JHN ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.294263\tvalid_1's l1: 0.309568\n",
      "[200]\ttraining's l1: 0.233842\tvalid_1's l1: 0.25418\n",
      "[300]\ttraining's l1: 0.203387\tvalid_1's l1: 0.228954\n",
      "[400]\ttraining's l1: 0.182961\tvalid_1's l1: 0.213009\n",
      "[500]\ttraining's l1: 0.167326\tvalid_1's l1: 0.201378\n",
      "[600]\ttraining's l1: 0.155744\tvalid_1's l1: 0.193167\n",
      "[700]\ttraining's l1: 0.146\tvalid_1's l1: 0.186554\n",
      "[800]\ttraining's l1: 0.137481\tvalid_1's l1: 0.181009\n",
      "[900]\ttraining's l1: 0.13011\tvalid_1's l1: 0.176558\n",
      "[1000]\ttraining's l1: 0.123702\tvalid_1's l1: 0.172623\n",
      "[1100]\ttraining's l1: 0.1179\tvalid_1's l1: 0.169248\n",
      "[1200]\ttraining's l1: 0.112611\tvalid_1's l1: 0.166248\n",
      "[1300]\ttraining's l1: 0.108001\tvalid_1's l1: 0.163831\n",
      "[1400]\ttraining's l1: 0.103822\tvalid_1's l1: 0.161477\n",
      "[1500]\ttraining's l1: 0.0997944\tvalid_1's l1: 0.159423\n",
      "[1600]\ttraining's l1: 0.0961111\tvalid_1's l1: 0.157454\n",
      "[1700]\ttraining's l1: 0.0926849\tvalid_1's l1: 0.15583\n",
      "[1800]\ttraining's l1: 0.0895408\tvalid_1's l1: 0.154203\n",
      "[1900]\ttraining's l1: 0.0867696\tvalid_1's l1: 0.152856\n",
      "[2000]\ttraining's l1: 0.0840539\tvalid_1's l1: 0.15161\n",
      "[2100]\ttraining's l1: 0.0815141\tvalid_1's l1: 0.150329\n",
      "[2200]\ttraining's l1: 0.079079\tvalid_1's l1: 0.149248\n",
      "[2300]\ttraining's l1: 0.0767163\tvalid_1's l1: 0.148162\n",
      "[2400]\ttraining's l1: 0.0745312\tvalid_1's l1: 0.14732\n",
      "[2500]\ttraining's l1: 0.072446\tvalid_1's l1: 0.14638\n",
      "[2600]\ttraining's l1: 0.0705383\tvalid_1's l1: 0.145561\n",
      "[2700]\ttraining's l1: 0.0686539\tvalid_1's l1: 0.144819\n",
      "[2800]\ttraining's l1: 0.0668774\tvalid_1's l1: 0.144134\n",
      "[2900]\ttraining's l1: 0.0651648\tvalid_1's l1: 0.143511\n",
      "[3000]\ttraining's l1: 0.0634804\tvalid_1's l1: 0.142927\n",
      "[3100]\ttraining's l1: 0.0619225\tvalid_1's l1: 0.142303\n",
      "[3200]\ttraining's l1: 0.0604233\tvalid_1's l1: 0.141749\n",
      "[3300]\ttraining's l1: 0.0590098\tvalid_1's l1: 0.141261\n",
      "[3400]\ttraining's l1: 0.0576256\tvalid_1's l1: 0.14077\n",
      "[3500]\ttraining's l1: 0.0563131\tvalid_1's l1: 0.140344\n",
      "[3600]\ttraining's l1: 0.0550246\tvalid_1's l1: 0.139871\n",
      "[3700]\ttraining's l1: 0.0538148\tvalid_1's l1: 0.139467\n",
      "[3800]\ttraining's l1: 0.0526357\tvalid_1's l1: 0.139053\n",
      "[3900]\ttraining's l1: 0.0514879\tvalid_1's l1: 0.138652\n",
      "[4000]\ttraining's l1: 0.0503868\tvalid_1's l1: 0.138314\n",
      "[4100]\ttraining's l1: 0.0493287\tvalid_1's l1: 0.137997\n",
      "[4200]\ttraining's l1: 0.0483242\tvalid_1's l1: 0.137691\n",
      "[4300]\ttraining's l1: 0.0473481\tvalid_1's l1: 0.137363\n",
      "[4400]\ttraining's l1: 0.0464029\tvalid_1's l1: 0.137075\n",
      "[4500]\ttraining's l1: 0.0454475\tvalid_1's l1: 0.13674\n",
      "[4600]\ttraining's l1: 0.0445551\tvalid_1's l1: 0.136449\n",
      "[4700]\ttraining's l1: 0.0436648\tvalid_1's l1: 0.136207\n",
      "[4800]\ttraining's l1: 0.0428315\tvalid_1's l1: 0.13597\n",
      "[4900]\ttraining's l1: 0.0420311\tvalid_1's l1: 0.135736\n",
      "[5000]\ttraining's l1: 0.0412274\tvalid_1's l1: 0.135506\n",
      "[5100]\ttraining's l1: 0.0404367\tvalid_1's l1: 0.135308\n",
      "[5200]\ttraining's l1: 0.0396769\tvalid_1's l1: 0.135083\n",
      "[5300]\ttraining's l1: 0.0389568\tvalid_1's l1: 0.134879\n",
      "[5400]\ttraining's l1: 0.0382644\tvalid_1's l1: 0.134694\n",
      "[5500]\ttraining's l1: 0.0375607\tvalid_1's l1: 0.134491\n",
      "[5600]\ttraining's l1: 0.0368845\tvalid_1's l1: 0.134334\n",
      "[5700]\ttraining's l1: 0.0362164\tvalid_1's l1: 0.134167\n",
      "[5800]\ttraining's l1: 0.0355618\tvalid_1's l1: 0.133972\n",
      "[5900]\ttraining's l1: 0.0349171\tvalid_1's l1: 0.133789\n",
      "[6000]\ttraining's l1: 0.0343086\tvalid_1's l1: 0.133591\n",
      "[6100]\ttraining's l1: 0.0337043\tvalid_1's l1: 0.133451\n",
      "[6200]\ttraining's l1: 0.0331141\tvalid_1's l1: 0.133314\n",
      "[6300]\ttraining's l1: 0.0325628\tvalid_1's l1: 0.133178\n",
      "[6400]\ttraining's l1: 0.0320176\tvalid_1's l1: 0.133037\n",
      "[6500]\ttraining's l1: 0.0314641\tvalid_1's l1: 0.132918\n",
      "[6600]\ttraining's l1: 0.0309178\tvalid_1's l1: 0.132794\n",
      "[6700]\ttraining's l1: 0.0304066\tvalid_1's l1: 0.132665\n",
      "[6800]\ttraining's l1: 0.0298915\tvalid_1's l1: 0.132532\n",
      "[6900]\ttraining's l1: 0.0293923\tvalid_1's l1: 0.132413\n",
      "[7000]\ttraining's l1: 0.0289118\tvalid_1's l1: 0.132289\n",
      "[7100]\ttraining's l1: 0.0284451\tvalid_1's l1: 0.132166\n",
      "[7200]\ttraining's l1: 0.0279953\tvalid_1's l1: 0.132069\n",
      "[7300]\ttraining's l1: 0.0275583\tvalid_1's l1: 0.13196\n",
      "[7400]\ttraining's l1: 0.0271301\tvalid_1's l1: 0.131871\n",
      "[7500]\ttraining's l1: 0.0266929\tvalid_1's l1: 0.131778\n",
      "[7600]\ttraining's l1: 0.0262539\tvalid_1's l1: 0.131683\n",
      "[7700]\ttraining's l1: 0.025854\tvalid_1's l1: 0.131582\n",
      "[7800]\ttraining's l1: 0.0254581\tvalid_1's l1: 0.131497\n",
      "[7900]\ttraining's l1: 0.0250653\tvalid_1's l1: 0.131417\n",
      "[8000]\ttraining's l1: 0.0246583\tvalid_1's l1: 0.13135\n",
      "[8100]\ttraining's l1: 0.0242697\tvalid_1's l1: 0.131251\n",
      "[8200]\ttraining's l1: 0.0238946\tvalid_1's l1: 0.131176\n",
      "[8300]\ttraining's l1: 0.0235218\tvalid_1's l1: 0.131085\n",
      "[8400]\ttraining's l1: 0.023167\tvalid_1's l1: 0.13102\n",
      "[8500]\ttraining's l1: 0.022844\tvalid_1's l1: 0.130956\n",
      "[8600]\ttraining's l1: 0.0225158\tvalid_1's l1: 0.130885\n",
      "[8700]\ttraining's l1: 0.0221735\tvalid_1's l1: 0.130821\n",
      "[8800]\ttraining's l1: 0.0218613\tvalid_1's l1: 0.130763\n",
      "[8900]\ttraining's l1: 0.0215478\tvalid_1's l1: 0.130712\n",
      "[9000]\ttraining's l1: 0.0212453\tvalid_1's l1: 0.130665\n",
      "[9100]\ttraining's l1: 0.0209459\tvalid_1's l1: 0.130616\n",
      "[9200]\ttraining's l1: 0.0206591\tvalid_1's l1: 0.130561\n",
      "[9300]\ttraining's l1: 0.0203687\tvalid_1's l1: 0.130518\n",
      "[9400]\ttraining's l1: 0.0200756\tvalid_1's l1: 0.130473\n",
      "[9500]\ttraining's l1: 0.0197953\tvalid_1's l1: 0.130427\n",
      "[9600]\ttraining's l1: 0.0195174\tvalid_1's l1: 0.130365\n",
      "[9700]\ttraining's l1: 0.0192349\tvalid_1's l1: 0.130314\n",
      "[9800]\ttraining's l1: 0.0189714\tvalid_1's l1: 0.130273\n",
      "[9900]\ttraining's l1: 0.0186946\tvalid_1's l1: 0.130231\n",
      "[10000]\ttraining's l1: 0.0184417\tvalid_1's l1: 0.13018\n",
      "[10100]\ttraining's l1: 0.018196\tvalid_1's l1: 0.130127\n",
      "[10200]\ttraining's l1: 0.0179436\tvalid_1's l1: 0.130081\n",
      "[10300]\ttraining's l1: 0.0177024\tvalid_1's l1: 0.130034\n",
      "[10400]\ttraining's l1: 0.017468\tvalid_1's l1: 0.13001\n",
      "[10500]\ttraining's l1: 0.0172352\tvalid_1's l1: 0.129965\n",
      "[10600]\ttraining's l1: 0.0170001\tvalid_1's l1: 0.129922\n",
      "[10700]\ttraining's l1: 0.0167603\tvalid_1's l1: 0.12988\n",
      "[10800]\ttraining's l1: 0.0165289\tvalid_1's l1: 0.129847\n",
      "[10900]\ttraining's l1: 0.0163077\tvalid_1's l1: 0.129811\n",
      "[11000]\ttraining's l1: 0.0160964\tvalid_1's l1: 0.129779\n",
      "[11100]\ttraining's l1: 0.0158882\tvalid_1's l1: 0.129746\n",
      "[11200]\ttraining's l1: 0.015676\tvalid_1's l1: 0.129718\n",
      "[11300]\ttraining's l1: 0.0154744\tvalid_1's l1: 0.129677\n",
      "[11400]\ttraining's l1: 0.0152704\tvalid_1's l1: 0.129658\n",
      "[11500]\ttraining's l1: 0.0150752\tvalid_1's l1: 0.129628\n",
      "[11600]\ttraining's l1: 0.0148803\tvalid_1's l1: 0.1296\n",
      "[11700]\ttraining's l1: 0.0146908\tvalid_1's l1: 0.129571\n",
      "[11800]\ttraining's l1: 0.0145043\tvalid_1's l1: 0.12954\n",
      "[11900]\ttraining's l1: 0.0143153\tvalid_1's l1: 0.12951\n",
      "[12000]\ttraining's l1: 0.0141271\tvalid_1's l1: 0.129482\n",
      "[12100]\ttraining's l1: 0.0139377\tvalid_1's l1: 0.129448\n",
      "[12200]\ttraining's l1: 0.0137597\tvalid_1's l1: 0.129418\n",
      "[12300]\ttraining's l1: 0.0135817\tvalid_1's l1: 0.129387\n",
      "[12400]\ttraining's l1: 0.013411\tvalid_1's l1: 0.129357\n",
      "[12500]\ttraining's l1: 0.0132387\tvalid_1's l1: 0.129322\n",
      "[12600]\ttraining's l1: 0.0130735\tvalid_1's l1: 0.129293\n",
      "[12700]\ttraining's l1: 0.0129152\tvalid_1's l1: 0.129268\n",
      "[12800]\ttraining's l1: 0.0127576\tvalid_1's l1: 0.129243\n",
      "[12900]\ttraining's l1: 0.0125969\tvalid_1's l1: 0.129228\n",
      "[13000]\ttraining's l1: 0.0124426\tvalid_1's l1: 0.129204\n",
      "[13100]\ttraining's l1: 0.0122913\tvalid_1's l1: 0.129177\n",
      "[13200]\ttraining's l1: 0.0121414\tvalid_1's l1: 0.129155\n",
      "[13300]\ttraining's l1: 0.0119924\tvalid_1's l1: 0.129135\n",
      "[13400]\ttraining's l1: 0.0118412\tvalid_1's l1: 0.129108\n",
      "[13500]\ttraining's l1: 0.0116963\tvalid_1's l1: 0.129091\n",
      "[13600]\ttraining's l1: 0.0115536\tvalid_1's l1: 0.129074\n",
      "[13700]\ttraining's l1: 0.0114217\tvalid_1's l1: 0.129057\n",
      "[13800]\ttraining's l1: 0.0112788\tvalid_1's l1: 0.129041\n",
      "[13900]\ttraining's l1: 0.0111382\tvalid_1's l1: 0.129011\n",
      "[14000]\ttraining's l1: 0.0110011\tvalid_1's l1: 0.128995\n",
      "[14100]\ttraining's l1: 0.010864\tvalid_1's l1: 0.128985\n",
      "[14200]\ttraining's l1: 0.01073\tvalid_1's l1: 0.128963\n",
      "[14300]\ttraining's l1: 0.0106071\tvalid_1's l1: 0.128938\n",
      "[14400]\ttraining's l1: 0.0104828\tvalid_1's l1: 0.128914\n",
      "[14500]\ttraining's l1: 0.0103612\tvalid_1's l1: 0.128892\n",
      "[14600]\ttraining's l1: 0.0102379\tvalid_1's l1: 0.128873\n",
      "[14700]\ttraining's l1: 0.0101177\tvalid_1's l1: 0.128854\n",
      "[14800]\ttraining's l1: 0.0100014\tvalid_1's l1: 0.12884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14900]\ttraining's l1: 0.00988509\tvalid_1's l1: 0.128831\n",
      "[15000]\ttraining's l1: 0.00977215\tvalid_1's l1: 0.128813\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00977215\tvalid_1's l1: 0.128813\n",
      "2JHN Fold 0, logMAE: -2.04939736163057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.293693\tvalid_1's l1: 0.308355\n",
      "[200]\ttraining's l1: 0.232371\tvalid_1's l1: 0.252298\n",
      "[300]\ttraining's l1: 0.201909\tvalid_1's l1: 0.226227\n",
      "[400]\ttraining's l1: 0.182202\tvalid_1's l1: 0.210435\n",
      "[500]\ttraining's l1: 0.166592\tvalid_1's l1: 0.198744\n",
      "[600]\ttraining's l1: 0.154382\tvalid_1's l1: 0.189826\n",
      "[700]\ttraining's l1: 0.144763\tvalid_1's l1: 0.183336\n",
      "[800]\ttraining's l1: 0.136241\tvalid_1's l1: 0.177626\n",
      "[900]\ttraining's l1: 0.129128\tvalid_1's l1: 0.173168\n",
      "[1000]\ttraining's l1: 0.122868\tvalid_1's l1: 0.169319\n",
      "[1100]\ttraining's l1: 0.117227\tvalid_1's l1: 0.165929\n",
      "[1200]\ttraining's l1: 0.112008\tvalid_1's l1: 0.162796\n",
      "[1300]\ttraining's l1: 0.107371\tvalid_1's l1: 0.160227\n",
      "[1400]\ttraining's l1: 0.103152\tvalid_1's l1: 0.157905\n",
      "[1500]\ttraining's l1: 0.0991848\tvalid_1's l1: 0.155647\n",
      "[1600]\ttraining's l1: 0.0955429\tvalid_1's l1: 0.153742\n",
      "[1700]\ttraining's l1: 0.0922347\tvalid_1's l1: 0.152052\n",
      "[1800]\ttraining's l1: 0.0891955\tvalid_1's l1: 0.150732\n",
      "[1900]\ttraining's l1: 0.0862642\tvalid_1's l1: 0.149304\n",
      "[2000]\ttraining's l1: 0.0835843\tvalid_1's l1: 0.148232\n",
      "[2100]\ttraining's l1: 0.0810718\tvalid_1's l1: 0.147101\n",
      "[2200]\ttraining's l1: 0.0786757\tvalid_1's l1: 0.145982\n",
      "[2300]\ttraining's l1: 0.0763969\tvalid_1's l1: 0.14497\n",
      "[2400]\ttraining's l1: 0.0741794\tvalid_1's l1: 0.144024\n",
      "[2500]\ttraining's l1: 0.0721313\tvalid_1's l1: 0.143148\n",
      "[2600]\ttraining's l1: 0.0701717\tvalid_1's l1: 0.14235\n",
      "[2700]\ttraining's l1: 0.0683507\tvalid_1's l1: 0.141605\n",
      "[2800]\ttraining's l1: 0.066554\tvalid_1's l1: 0.140842\n",
      "[2900]\ttraining's l1: 0.0649074\tvalid_1's l1: 0.140181\n",
      "[3000]\ttraining's l1: 0.063296\tvalid_1's l1: 0.139447\n",
      "[3100]\ttraining's l1: 0.0617143\tvalid_1's l1: 0.13884\n",
      "[3200]\ttraining's l1: 0.0602144\tvalid_1's l1: 0.138222\n",
      "[3300]\ttraining's l1: 0.0587848\tvalid_1's l1: 0.137654\n",
      "[3400]\ttraining's l1: 0.0574083\tvalid_1's l1: 0.137185\n",
      "[3500]\ttraining's l1: 0.0560992\tvalid_1's l1: 0.136698\n",
      "[3600]\ttraining's l1: 0.0548424\tvalid_1's l1: 0.136248\n",
      "[3700]\ttraining's l1: 0.0536714\tvalid_1's l1: 0.135879\n",
      "[3800]\ttraining's l1: 0.0524861\tvalid_1's l1: 0.135494\n",
      "[3900]\ttraining's l1: 0.0513235\tvalid_1's l1: 0.135075\n",
      "[4000]\ttraining's l1: 0.0501991\tvalid_1's l1: 0.13468\n",
      "[4100]\ttraining's l1: 0.0491596\tvalid_1's l1: 0.134353\n",
      "[4200]\ttraining's l1: 0.0481156\tvalid_1's l1: 0.133996\n",
      "[4300]\ttraining's l1: 0.0471392\tvalid_1's l1: 0.133691\n",
      "[4400]\ttraining's l1: 0.0461879\tvalid_1's l1: 0.1334\n",
      "[4500]\ttraining's l1: 0.0453017\tvalid_1's l1: 0.133092\n",
      "[4600]\ttraining's l1: 0.0444415\tvalid_1's l1: 0.132803\n",
      "[4700]\ttraining's l1: 0.0435721\tvalid_1's l1: 0.132508\n",
      "[4800]\ttraining's l1: 0.0427155\tvalid_1's l1: 0.132248\n",
      "[4900]\ttraining's l1: 0.0418876\tvalid_1's l1: 0.13202\n",
      "[5000]\ttraining's l1: 0.0410767\tvalid_1's l1: 0.13176\n",
      "[5100]\ttraining's l1: 0.0402884\tvalid_1's l1: 0.131551\n",
      "[5200]\ttraining's l1: 0.0395469\tvalid_1's l1: 0.131352\n",
      "[5300]\ttraining's l1: 0.0388475\tvalid_1's l1: 0.131165\n",
      "[5400]\ttraining's l1: 0.0381475\tvalid_1's l1: 0.130966\n",
      "[5500]\ttraining's l1: 0.0374503\tvalid_1's l1: 0.130798\n",
      "[5600]\ttraining's l1: 0.0367738\tvalid_1's l1: 0.130608\n",
      "[5700]\ttraining's l1: 0.0361096\tvalid_1's l1: 0.130453\n",
      "[5800]\ttraining's l1: 0.0354543\tvalid_1's l1: 0.130256\n",
      "[5900]\ttraining's l1: 0.0348457\tvalid_1's l1: 0.13007\n",
      "[6000]\ttraining's l1: 0.0342358\tvalid_1's l1: 0.12989\n",
      "[6100]\ttraining's l1: 0.0336462\tvalid_1's l1: 0.129719\n",
      "[6200]\ttraining's l1: 0.0330661\tvalid_1's l1: 0.129586\n",
      "[6300]\ttraining's l1: 0.0324919\tvalid_1's l1: 0.129458\n",
      "[6400]\ttraining's l1: 0.0319236\tvalid_1's l1: 0.129341\n",
      "[6500]\ttraining's l1: 0.0313809\tvalid_1's l1: 0.129234\n",
      "[6600]\ttraining's l1: 0.0308365\tvalid_1's l1: 0.129093\n",
      "[6700]\ttraining's l1: 0.0303303\tvalid_1's l1: 0.12898\n",
      "[6800]\ttraining's l1: 0.0298295\tvalid_1's l1: 0.12886\n",
      "[6900]\ttraining's l1: 0.0293275\tvalid_1's l1: 0.128738\n",
      "[7000]\ttraining's l1: 0.0288479\tvalid_1's l1: 0.128629\n",
      "[7100]\ttraining's l1: 0.0283761\tvalid_1's l1: 0.128509\n",
      "[7200]\ttraining's l1: 0.0279173\tvalid_1's l1: 0.128403\n",
      "[7300]\ttraining's l1: 0.0274744\tvalid_1's l1: 0.128314\n",
      "[7400]\ttraining's l1: 0.0270379\tvalid_1's l1: 0.128215\n",
      "[7500]\ttraining's l1: 0.0266118\tvalid_1's l1: 0.12811\n",
      "[7600]\ttraining's l1: 0.0261931\tvalid_1's l1: 0.128018\n",
      "[7700]\ttraining's l1: 0.0257908\tvalid_1's l1: 0.127934\n",
      "[7800]\ttraining's l1: 0.025384\tvalid_1's l1: 0.127842\n",
      "[7900]\ttraining's l1: 0.024996\tvalid_1's l1: 0.127769\n",
      "[8000]\ttraining's l1: 0.0246119\tvalid_1's l1: 0.127688\n",
      "[8100]\ttraining's l1: 0.0242435\tvalid_1's l1: 0.127612\n",
      "[8200]\ttraining's l1: 0.0238714\tvalid_1's l1: 0.127538\n",
      "[8300]\ttraining's l1: 0.0235069\tvalid_1's l1: 0.127464\n",
      "[8400]\ttraining's l1: 0.0231601\tvalid_1's l1: 0.127413\n",
      "[8500]\ttraining's l1: 0.0228122\tvalid_1's l1: 0.127354\n",
      "[8600]\ttraining's l1: 0.0224686\tvalid_1's l1: 0.127273\n",
      "[8700]\ttraining's l1: 0.0221364\tvalid_1's l1: 0.12721\n",
      "[8800]\ttraining's l1: 0.0218205\tvalid_1's l1: 0.127158\n",
      "[8900]\ttraining's l1: 0.0215063\tvalid_1's l1: 0.127097\n",
      "[9000]\ttraining's l1: 0.0211815\tvalid_1's l1: 0.127049\n",
      "[9100]\ttraining's l1: 0.020873\tvalid_1's l1: 0.12698\n",
      "[9200]\ttraining's l1: 0.0205777\tvalid_1's l1: 0.12694\n",
      "[9300]\ttraining's l1: 0.0202761\tvalid_1's l1: 0.126893\n",
      "[9400]\ttraining's l1: 0.0199815\tvalid_1's l1: 0.126836\n",
      "[9500]\ttraining's l1: 0.0196884\tvalid_1's l1: 0.12678\n",
      "[9600]\ttraining's l1: 0.0194034\tvalid_1's l1: 0.126732\n",
      "[9700]\ttraining's l1: 0.019124\tvalid_1's l1: 0.12668\n",
      "[9800]\ttraining's l1: 0.0188556\tvalid_1's l1: 0.126622\n",
      "[9900]\ttraining's l1: 0.0186016\tvalid_1's l1: 0.126586\n",
      "[10000]\ttraining's l1: 0.0183552\tvalid_1's l1: 0.126536\n",
      "[10100]\ttraining's l1: 0.0181172\tvalid_1's l1: 0.126493\n",
      "[10200]\ttraining's l1: 0.0178638\tvalid_1's l1: 0.126446\n",
      "[10300]\ttraining's l1: 0.0176122\tvalid_1's l1: 0.126416\n",
      "[10400]\ttraining's l1: 0.0173751\tvalid_1's l1: 0.126371\n",
      "[10500]\ttraining's l1: 0.0171375\tvalid_1's l1: 0.126339\n",
      "[10600]\ttraining's l1: 0.0169105\tvalid_1's l1: 0.126313\n",
      "[10700]\ttraining's l1: 0.0166794\tvalid_1's l1: 0.126278\n",
      "[10800]\ttraining's l1: 0.016457\tvalid_1's l1: 0.126254\n",
      "[10900]\ttraining's l1: 0.0162264\tvalid_1's l1: 0.126222\n",
      "[11000]\ttraining's l1: 0.0160056\tvalid_1's l1: 0.126182\n",
      "[11100]\ttraining's l1: 0.0157888\tvalid_1's l1: 0.126143\n",
      "[11200]\ttraining's l1: 0.0155738\tvalid_1's l1: 0.126108\n",
      "[11300]\ttraining's l1: 0.0153724\tvalid_1's l1: 0.126076\n",
      "[11400]\ttraining's l1: 0.0151604\tvalid_1's l1: 0.126036\n",
      "[11500]\ttraining's l1: 0.0149611\tvalid_1's l1: 0.126007\n",
      "[11600]\ttraining's l1: 0.0147598\tvalid_1's l1: 0.125969\n",
      "[11700]\ttraining's l1: 0.0145698\tvalid_1's l1: 0.125941\n",
      "[11800]\ttraining's l1: 0.0143841\tvalid_1's l1: 0.125906\n",
      "[11900]\ttraining's l1: 0.0141992\tvalid_1's l1: 0.125865\n",
      "[12000]\ttraining's l1: 0.0140197\tvalid_1's l1: 0.125829\n",
      "[12100]\ttraining's l1: 0.0138427\tvalid_1's l1: 0.125803\n",
      "[12200]\ttraining's l1: 0.0136499\tvalid_1's l1: 0.125775\n",
      "[12300]\ttraining's l1: 0.0134784\tvalid_1's l1: 0.125762\n",
      "[12400]\ttraining's l1: 0.0133101\tvalid_1's l1: 0.125738\n",
      "[12500]\ttraining's l1: 0.0131418\tvalid_1's l1: 0.125709\n",
      "[12600]\ttraining's l1: 0.0129824\tvalid_1's l1: 0.12569\n",
      "[12700]\ttraining's l1: 0.0128269\tvalid_1's l1: 0.125668\n",
      "[12800]\ttraining's l1: 0.0126692\tvalid_1's l1: 0.125644\n",
      "[12900]\ttraining's l1: 0.0125076\tvalid_1's l1: 0.125618\n",
      "[13000]\ttraining's l1: 0.0123566\tvalid_1's l1: 0.125593\n",
      "[13100]\ttraining's l1: 0.0122147\tvalid_1's l1: 0.125574\n",
      "[13200]\ttraining's l1: 0.0120612\tvalid_1's l1: 0.125552\n",
      "[13300]\ttraining's l1: 0.011904\tvalid_1's l1: 0.125525\n",
      "[13400]\ttraining's l1: 0.0117475\tvalid_1's l1: 0.125508\n",
      "[13500]\ttraining's l1: 0.0115989\tvalid_1's l1: 0.125491\n",
      "[13600]\ttraining's l1: 0.0114591\tvalid_1's l1: 0.125468\n",
      "[13700]\ttraining's l1: 0.0113181\tvalid_1's l1: 0.125446\n",
      "[13800]\ttraining's l1: 0.0111818\tvalid_1's l1: 0.125429\n",
      "[13900]\ttraining's l1: 0.0110506\tvalid_1's l1: 0.125403\n",
      "[14000]\ttraining's l1: 0.0109181\tvalid_1's l1: 0.125378\n",
      "[14100]\ttraining's l1: 0.0107792\tvalid_1's l1: 0.125354\n",
      "[14200]\ttraining's l1: 0.0106521\tvalid_1's l1: 0.125339\n",
      "[14300]\ttraining's l1: 0.0105297\tvalid_1's l1: 0.125324\n",
      "[14400]\ttraining's l1: 0.0104015\tvalid_1's l1: 0.125309\n",
      "[14500]\ttraining's l1: 0.0102799\tvalid_1's l1: 0.125293\n",
      "[14600]\ttraining's l1: 0.0101553\tvalid_1's l1: 0.125276\n",
      "[14700]\ttraining's l1: 0.0100346\tvalid_1's l1: 0.125262\n",
      "[14800]\ttraining's l1: 0.00991316\tvalid_1's l1: 0.125248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14900]\ttraining's l1: 0.00979522\tvalid_1's l1: 0.125235\n",
      "[15000]\ttraining's l1: 0.00967708\tvalid_1's l1: 0.125217\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00967708\tvalid_1's l1: 0.125217\n",
      "2JHN Fold 1, logMAE: -2.0777074253530787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.289187\tvalid_1's l1: 0.301203\n",
      "[200]\ttraining's l1: 0.23084\tvalid_1's l1: 0.248381\n",
      "[300]\ttraining's l1: 0.198761\tvalid_1's l1: 0.220746\n",
      "[400]\ttraining's l1: 0.178666\tvalid_1's l1: 0.20492\n",
      "[500]\ttraining's l1: 0.163141\tvalid_1's l1: 0.193389\n",
      "[600]\ttraining's l1: 0.151775\tvalid_1's l1: 0.185594\n",
      "[700]\ttraining's l1: 0.142196\tvalid_1's l1: 0.17924\n",
      "[800]\ttraining's l1: 0.134363\tvalid_1's l1: 0.174394\n",
      "[900]\ttraining's l1: 0.12712\tvalid_1's l1: 0.16983\n",
      "[1000]\ttraining's l1: 0.121073\tvalid_1's l1: 0.16613\n",
      "[1100]\ttraining's l1: 0.115666\tvalid_1's l1: 0.163195\n",
      "[1200]\ttraining's l1: 0.110702\tvalid_1's l1: 0.16043\n",
      "[1300]\ttraining's l1: 0.106116\tvalid_1's l1: 0.15805\n",
      "[1400]\ttraining's l1: 0.10195\tvalid_1's l1: 0.155847\n",
      "[1500]\ttraining's l1: 0.0981757\tvalid_1's l1: 0.153941\n",
      "[1600]\ttraining's l1: 0.0946193\tvalid_1's l1: 0.152159\n",
      "[1700]\ttraining's l1: 0.0914294\tvalid_1's l1: 0.150659\n",
      "[1800]\ttraining's l1: 0.088394\tvalid_1's l1: 0.149237\n",
      "[1900]\ttraining's l1: 0.0855229\tvalid_1's l1: 0.147784\n",
      "[2000]\ttraining's l1: 0.0828442\tvalid_1's l1: 0.146628\n",
      "[2100]\ttraining's l1: 0.080222\tvalid_1's l1: 0.145403\n",
      "[2200]\ttraining's l1: 0.077882\tvalid_1's l1: 0.144366\n",
      "[2300]\ttraining's l1: 0.0755248\tvalid_1's l1: 0.143387\n",
      "[2400]\ttraining's l1: 0.0734316\tvalid_1's l1: 0.142496\n",
      "[2500]\ttraining's l1: 0.071353\tvalid_1's l1: 0.141697\n",
      "[2600]\ttraining's l1: 0.0694916\tvalid_1's l1: 0.140922\n",
      "[2700]\ttraining's l1: 0.0676872\tvalid_1's l1: 0.140168\n",
      "[2800]\ttraining's l1: 0.0660042\tvalid_1's l1: 0.139558\n",
      "[2900]\ttraining's l1: 0.0643391\tvalid_1's l1: 0.138949\n",
      "[3000]\ttraining's l1: 0.0627842\tvalid_1's l1: 0.138401\n",
      "[3100]\ttraining's l1: 0.0612875\tvalid_1's l1: 0.137812\n",
      "[3200]\ttraining's l1: 0.0598712\tvalid_1's l1: 0.137318\n",
      "[3300]\ttraining's l1: 0.0584873\tvalid_1's l1: 0.136843\n",
      "[3400]\ttraining's l1: 0.0571309\tvalid_1's l1: 0.136338\n",
      "[3500]\ttraining's l1: 0.055828\tvalid_1's l1: 0.135907\n",
      "[3600]\ttraining's l1: 0.0545751\tvalid_1's l1: 0.135463\n",
      "[3700]\ttraining's l1: 0.053382\tvalid_1's l1: 0.135066\n",
      "[3800]\ttraining's l1: 0.0522137\tvalid_1's l1: 0.134754\n",
      "[3900]\ttraining's l1: 0.0511237\tvalid_1's l1: 0.134379\n",
      "[4000]\ttraining's l1: 0.0500675\tvalid_1's l1: 0.134004\n",
      "[4100]\ttraining's l1: 0.0490256\tvalid_1's l1: 0.133671\n",
      "[4200]\ttraining's l1: 0.0480053\tvalid_1's l1: 0.133341\n",
      "[4300]\ttraining's l1: 0.0470226\tvalid_1's l1: 0.133065\n",
      "[4400]\ttraining's l1: 0.0460488\tvalid_1's l1: 0.132792\n",
      "[4500]\ttraining's l1: 0.0451382\tvalid_1's l1: 0.132518\n",
      "[4600]\ttraining's l1: 0.0442397\tvalid_1's l1: 0.132253\n",
      "[4700]\ttraining's l1: 0.0433919\tvalid_1's l1: 0.132042\n",
      "[4800]\ttraining's l1: 0.0425507\tvalid_1's l1: 0.131764\n",
      "[4900]\ttraining's l1: 0.0417304\tvalid_1's l1: 0.131511\n",
      "[5000]\ttraining's l1: 0.0409117\tvalid_1's l1: 0.131255\n",
      "[5100]\ttraining's l1: 0.0401282\tvalid_1's l1: 0.131044\n",
      "[5200]\ttraining's l1: 0.0393815\tvalid_1's l1: 0.130817\n",
      "[5300]\ttraining's l1: 0.0386574\tvalid_1's l1: 0.130649\n",
      "[5400]\ttraining's l1: 0.0379382\tvalid_1's l1: 0.130431\n",
      "[5500]\ttraining's l1: 0.0372275\tvalid_1's l1: 0.130261\n",
      "[5600]\ttraining's l1: 0.0365331\tvalid_1's l1: 0.130066\n",
      "[5700]\ttraining's l1: 0.0358661\tvalid_1's l1: 0.129905\n",
      "[5800]\ttraining's l1: 0.0352366\tvalid_1's l1: 0.129762\n",
      "[5900]\ttraining's l1: 0.0346256\tvalid_1's l1: 0.12962\n",
      "[6000]\ttraining's l1: 0.0340076\tvalid_1's l1: 0.129472\n",
      "[6100]\ttraining's l1: 0.0334413\tvalid_1's l1: 0.129347\n",
      "[6200]\ttraining's l1: 0.0328564\tvalid_1's l1: 0.129202\n",
      "[6300]\ttraining's l1: 0.032284\tvalid_1's l1: 0.129079\n",
      "[6400]\ttraining's l1: 0.0317421\tvalid_1's l1: 0.128952\n",
      "[6500]\ttraining's l1: 0.0312351\tvalid_1's l1: 0.128823\n",
      "[6600]\ttraining's l1: 0.0307282\tvalid_1's l1: 0.128702\n",
      "[6700]\ttraining's l1: 0.0302175\tvalid_1's l1: 0.128594\n",
      "[6800]\ttraining's l1: 0.029723\tvalid_1's l1: 0.128483\n",
      "[6900]\ttraining's l1: 0.0292294\tvalid_1's l1: 0.128388\n",
      "[7000]\ttraining's l1: 0.0287667\tvalid_1's l1: 0.128307\n",
      "[7100]\ttraining's l1: 0.0282944\tvalid_1's l1: 0.128195\n",
      "[7200]\ttraining's l1: 0.0278479\tvalid_1's l1: 0.128121\n",
      "[7300]\ttraining's l1: 0.027404\tvalid_1's l1: 0.128024\n",
      "[7400]\ttraining's l1: 0.0269773\tvalid_1's l1: 0.127939\n",
      "[7500]\ttraining's l1: 0.0265572\tvalid_1's l1: 0.127873\n",
      "[7600]\ttraining's l1: 0.0261326\tvalid_1's l1: 0.127797\n",
      "[7700]\ttraining's l1: 0.025718\tvalid_1's l1: 0.127711\n",
      "[7800]\ttraining's l1: 0.0253201\tvalid_1's l1: 0.127631\n",
      "[7900]\ttraining's l1: 0.0249187\tvalid_1's l1: 0.127539\n",
      "[8000]\ttraining's l1: 0.0245402\tvalid_1's l1: 0.127465\n",
      "[8100]\ttraining's l1: 0.0241644\tvalid_1's l1: 0.12738\n",
      "[8200]\ttraining's l1: 0.0238007\tvalid_1's l1: 0.127323\n",
      "[8300]\ttraining's l1: 0.023434\tvalid_1's l1: 0.12726\n",
      "[8400]\ttraining's l1: 0.0230812\tvalid_1's l1: 0.127175\n",
      "[8500]\ttraining's l1: 0.0227455\tvalid_1's l1: 0.12712\n",
      "[8600]\ttraining's l1: 0.0224106\tvalid_1's l1: 0.127074\n",
      "[8700]\ttraining's l1: 0.0220699\tvalid_1's l1: 0.127013\n",
      "[8800]\ttraining's l1: 0.0217483\tvalid_1's l1: 0.126956\n",
      "[8900]\ttraining's l1: 0.0214318\tvalid_1's l1: 0.126908\n",
      "[9000]\ttraining's l1: 0.0211276\tvalid_1's l1: 0.126856\n",
      "[9100]\ttraining's l1: 0.0208147\tvalid_1's l1: 0.126793\n",
      "[9200]\ttraining's l1: 0.0205104\tvalid_1's l1: 0.12672\n",
      "[9300]\ttraining's l1: 0.0202238\tvalid_1's l1: 0.12666\n",
      "[9400]\ttraining's l1: 0.0199399\tvalid_1's l1: 0.126606\n",
      "[9500]\ttraining's l1: 0.0196617\tvalid_1's l1: 0.126556\n",
      "[9600]\ttraining's l1: 0.0193804\tvalid_1's l1: 0.126501\n",
      "[9700]\ttraining's l1: 0.0190952\tvalid_1's l1: 0.126451\n",
      "[9800]\ttraining's l1: 0.0188211\tvalid_1's l1: 0.126407\n",
      "[9900]\ttraining's l1: 0.018566\tvalid_1's l1: 0.126367\n",
      "[10000]\ttraining's l1: 0.0183051\tvalid_1's l1: 0.126323\n",
      "[10100]\ttraining's l1: 0.0180447\tvalid_1's l1: 0.126274\n",
      "[10200]\ttraining's l1: 0.0178034\tvalid_1's l1: 0.126239\n",
      "[10300]\ttraining's l1: 0.0175499\tvalid_1's l1: 0.126192\n",
      "[10400]\ttraining's l1: 0.0173029\tvalid_1's l1: 0.126141\n",
      "[10500]\ttraining's l1: 0.0170687\tvalid_1's l1: 0.126109\n",
      "[10600]\ttraining's l1: 0.0168371\tvalid_1's l1: 0.126075\n",
      "[10700]\ttraining's l1: 0.0166006\tvalid_1's l1: 0.126037\n",
      "[10800]\ttraining's l1: 0.016375\tvalid_1's l1: 0.125999\n",
      "[10900]\ttraining's l1: 0.0161524\tvalid_1's l1: 0.125961\n",
      "[11000]\ttraining's l1: 0.0159376\tvalid_1's l1: 0.125927\n",
      "[11100]\ttraining's l1: 0.0157224\tvalid_1's l1: 0.125893\n",
      "[11200]\ttraining's l1: 0.0155171\tvalid_1's l1: 0.125858\n",
      "[11300]\ttraining's l1: 0.0153098\tvalid_1's l1: 0.125821\n",
      "[11400]\ttraining's l1: 0.0151131\tvalid_1's l1: 0.125795\n",
      "[11500]\ttraining's l1: 0.0149261\tvalid_1's l1: 0.12577\n",
      "[11600]\ttraining's l1: 0.014738\tvalid_1's l1: 0.125754\n",
      "[11700]\ttraining's l1: 0.0145457\tvalid_1's l1: 0.125719\n",
      "[11800]\ttraining's l1: 0.0143595\tvalid_1's l1: 0.12569\n",
      "[11900]\ttraining's l1: 0.0141804\tvalid_1's l1: 0.125669\n",
      "[12000]\ttraining's l1: 0.0139972\tvalid_1's l1: 0.12565\n",
      "[12100]\ttraining's l1: 0.0138121\tvalid_1's l1: 0.125623\n",
      "[12200]\ttraining's l1: 0.0136295\tvalid_1's l1: 0.125594\n",
      "[12300]\ttraining's l1: 0.0134583\tvalid_1's l1: 0.125574\n",
      "[12400]\ttraining's l1: 0.0132825\tvalid_1's l1: 0.125551\n",
      "[12500]\ttraining's l1: 0.0131095\tvalid_1's l1: 0.125531\n",
      "[12600]\ttraining's l1: 0.0129443\tvalid_1's l1: 0.125512\n",
      "[12700]\ttraining's l1: 0.0127838\tvalid_1's l1: 0.125494\n",
      "[12800]\ttraining's l1: 0.0126225\tvalid_1's l1: 0.125467\n",
      "[12900]\ttraining's l1: 0.0124707\tvalid_1's l1: 0.125445\n",
      "[13000]\ttraining's l1: 0.0123217\tvalid_1's l1: 0.125417\n",
      "[13100]\ttraining's l1: 0.0121622\tvalid_1's l1: 0.125395\n",
      "[13200]\ttraining's l1: 0.0120023\tvalid_1's l1: 0.125379\n",
      "[13300]\ttraining's l1: 0.0118581\tvalid_1's l1: 0.125366\n",
      "[13400]\ttraining's l1: 0.0117198\tvalid_1's l1: 0.125343\n",
      "[13500]\ttraining's l1: 0.0115698\tvalid_1's l1: 0.125319\n",
      "[13600]\ttraining's l1: 0.0114276\tvalid_1's l1: 0.125293\n",
      "[13700]\ttraining's l1: 0.0112917\tvalid_1's l1: 0.125282\n",
      "[13800]\ttraining's l1: 0.0111567\tvalid_1's l1: 0.125261\n",
      "[13900]\ttraining's l1: 0.0110173\tvalid_1's l1: 0.125249\n",
      "[14000]\ttraining's l1: 0.0108828\tvalid_1's l1: 0.125223\n",
      "[14100]\ttraining's l1: 0.0107524\tvalid_1's l1: 0.125209\n",
      "[14200]\ttraining's l1: 0.0106202\tvalid_1's l1: 0.125192\n",
      "[14300]\ttraining's l1: 0.0104932\tvalid_1's l1: 0.125172\n",
      "[14400]\ttraining's l1: 0.010371\tvalid_1's l1: 0.125157\n",
      "[14500]\ttraining's l1: 0.0102479\tvalid_1's l1: 0.125144\n",
      "[14600]\ttraining's l1: 0.0101291\tvalid_1's l1: 0.125134\n",
      "[14700]\ttraining's l1: 0.0100133\tvalid_1's l1: 0.125119\n",
      "[14800]\ttraining's l1: 0.00989884\tvalid_1's l1: 0.125105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14900]\ttraining's l1: 0.00978653\tvalid_1's l1: 0.125095\n",
      "[15000]\ttraining's l1: 0.00967355\tvalid_1's l1: 0.125078\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00967355\tvalid_1's l1: 0.125078\n",
      "2JHN Fold 2, logMAE: -2.0785464598950787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.291448\tvalid_1's l1: 0.301236\n",
      "[200]\ttraining's l1: 0.232199\tvalid_1's l1: 0.247741\n",
      "[300]\ttraining's l1: 0.201336\tvalid_1's l1: 0.222023\n",
      "[400]\ttraining's l1: 0.180649\tvalid_1's l1: 0.205702\n",
      "[500]\ttraining's l1: 0.165788\tvalid_1's l1: 0.194832\n",
      "[600]\ttraining's l1: 0.15395\tvalid_1's l1: 0.186491\n",
      "[700]\ttraining's l1: 0.144255\tvalid_1's l1: 0.179971\n",
      "[800]\ttraining's l1: 0.135812\tvalid_1's l1: 0.174349\n",
      "[900]\ttraining's l1: 0.128672\tvalid_1's l1: 0.169684\n",
      "[1000]\ttraining's l1: 0.122202\tvalid_1's l1: 0.165806\n",
      "[1100]\ttraining's l1: 0.11674\tvalid_1's l1: 0.162654\n",
      "[1200]\ttraining's l1: 0.111797\tvalid_1's l1: 0.159971\n",
      "[1300]\ttraining's l1: 0.107176\tvalid_1's l1: 0.157417\n",
      "[1400]\ttraining's l1: 0.102988\tvalid_1's l1: 0.155175\n",
      "[1500]\ttraining's l1: 0.0992051\tvalid_1's l1: 0.153382\n",
      "[1600]\ttraining's l1: 0.0956394\tvalid_1's l1: 0.151598\n",
      "[1700]\ttraining's l1: 0.0923424\tvalid_1's l1: 0.150111\n",
      "[1800]\ttraining's l1: 0.0892044\tvalid_1's l1: 0.148596\n",
      "[1900]\ttraining's l1: 0.086301\tvalid_1's l1: 0.147148\n",
      "[2000]\ttraining's l1: 0.0836164\tvalid_1's l1: 0.145853\n",
      "[2100]\ttraining's l1: 0.0810168\tvalid_1's l1: 0.14467\n",
      "[2200]\ttraining's l1: 0.0786319\tvalid_1's l1: 0.143687\n",
      "[2300]\ttraining's l1: 0.0763121\tvalid_1's l1: 0.14271\n",
      "[2400]\ttraining's l1: 0.0741062\tvalid_1's l1: 0.14176\n",
      "[2500]\ttraining's l1: 0.0719966\tvalid_1's l1: 0.140808\n",
      "[2600]\ttraining's l1: 0.0700258\tvalid_1's l1: 0.140073\n",
      "[2700]\ttraining's l1: 0.0682234\tvalid_1's l1: 0.13936\n",
      "[2800]\ttraining's l1: 0.0664598\tvalid_1's l1: 0.138738\n",
      "[2900]\ttraining's l1: 0.0648454\tvalid_1's l1: 0.138186\n",
      "[3000]\ttraining's l1: 0.0632262\tvalid_1's l1: 0.137573\n",
      "[3100]\ttraining's l1: 0.0617388\tvalid_1's l1: 0.137067\n",
      "[3200]\ttraining's l1: 0.0602703\tvalid_1's l1: 0.136545\n",
      "[3300]\ttraining's l1: 0.05889\tvalid_1's l1: 0.136035\n",
      "[3400]\ttraining's l1: 0.0575023\tvalid_1's l1: 0.135538\n",
      "[3500]\ttraining's l1: 0.0562326\tvalid_1's l1: 0.135112\n",
      "[3600]\ttraining's l1: 0.0549376\tvalid_1's l1: 0.134622\n",
      "[3700]\ttraining's l1: 0.0537\tvalid_1's l1: 0.13425\n",
      "[3800]\ttraining's l1: 0.0524881\tvalid_1's l1: 0.133862\n",
      "[3900]\ttraining's l1: 0.0513518\tvalid_1's l1: 0.133534\n",
      "[4000]\ttraining's l1: 0.050226\tvalid_1's l1: 0.133214\n",
      "[4100]\ttraining's l1: 0.0491788\tvalid_1's l1: 0.132867\n",
      "[4200]\ttraining's l1: 0.0481567\tvalid_1's l1: 0.132587\n",
      "[4300]\ttraining's l1: 0.0471497\tvalid_1's l1: 0.132287\n",
      "[4400]\ttraining's l1: 0.046168\tvalid_1's l1: 0.132022\n",
      "[4500]\ttraining's l1: 0.0452292\tvalid_1's l1: 0.131762\n",
      "[4600]\ttraining's l1: 0.0443512\tvalid_1's l1: 0.131525\n",
      "[4700]\ttraining's l1: 0.0434941\tvalid_1's l1: 0.131306\n",
      "[4800]\ttraining's l1: 0.0426579\tvalid_1's l1: 0.131065\n",
      "[4900]\ttraining's l1: 0.0418463\tvalid_1's l1: 0.130857\n",
      "[5000]\ttraining's l1: 0.041046\tvalid_1's l1: 0.130621\n",
      "[5100]\ttraining's l1: 0.0402864\tvalid_1's l1: 0.130444\n",
      "[5200]\ttraining's l1: 0.0395484\tvalid_1's l1: 0.130277\n",
      "[5300]\ttraining's l1: 0.0388313\tvalid_1's l1: 0.130073\n",
      "[5400]\ttraining's l1: 0.0381407\tvalid_1's l1: 0.129899\n",
      "[5500]\ttraining's l1: 0.0374606\tvalid_1's l1: 0.12975\n",
      "[5600]\ttraining's l1: 0.0368057\tvalid_1's l1: 0.129602\n",
      "[5700]\ttraining's l1: 0.0361475\tvalid_1's l1: 0.12943\n",
      "[5800]\ttraining's l1: 0.0354911\tvalid_1's l1: 0.129292\n",
      "[5900]\ttraining's l1: 0.0348608\tvalid_1's l1: 0.129123\n",
      "[6000]\ttraining's l1: 0.034256\tvalid_1's l1: 0.128977\n",
      "[6100]\ttraining's l1: 0.0336723\tvalid_1's l1: 0.128843\n",
      "[6200]\ttraining's l1: 0.0330921\tvalid_1's l1: 0.128721\n",
      "[6300]\ttraining's l1: 0.0325123\tvalid_1's l1: 0.128617\n",
      "[6400]\ttraining's l1: 0.0319699\tvalid_1's l1: 0.128492\n",
      "[6500]\ttraining's l1: 0.0314431\tvalid_1's l1: 0.128362\n",
      "[6600]\ttraining's l1: 0.0308992\tvalid_1's l1: 0.128237\n",
      "[6700]\ttraining's l1: 0.0303803\tvalid_1's l1: 0.128104\n",
      "[6800]\ttraining's l1: 0.0298867\tvalid_1's l1: 0.128007\n",
      "[6900]\ttraining's l1: 0.0293904\tvalid_1's l1: 0.127885\n",
      "[7000]\ttraining's l1: 0.0289152\tvalid_1's l1: 0.127811\n",
      "[7100]\ttraining's l1: 0.0284498\tvalid_1's l1: 0.127694\n",
      "[7200]\ttraining's l1: 0.0279827\tvalid_1's l1: 0.12758\n",
      "[7300]\ttraining's l1: 0.0275338\tvalid_1's l1: 0.127485\n",
      "[7400]\ttraining's l1: 0.0270893\tvalid_1's l1: 0.127396\n",
      "[7500]\ttraining's l1: 0.0266525\tvalid_1's l1: 0.127297\n",
      "[7600]\ttraining's l1: 0.0262516\tvalid_1's l1: 0.127233\n",
      "[7700]\ttraining's l1: 0.025847\tvalid_1's l1: 0.127133\n",
      "[7800]\ttraining's l1: 0.0254412\tvalid_1's l1: 0.127027\n",
      "[7900]\ttraining's l1: 0.0250527\tvalid_1's l1: 0.126945\n",
      "[8000]\ttraining's l1: 0.0246623\tvalid_1's l1: 0.126865\n",
      "[8100]\ttraining's l1: 0.0242858\tvalid_1's l1: 0.1268\n",
      "[8200]\ttraining's l1: 0.023906\tvalid_1's l1: 0.126736\n",
      "[8300]\ttraining's l1: 0.0235355\tvalid_1's l1: 0.126659\n",
      "[8400]\ttraining's l1: 0.0231858\tvalid_1's l1: 0.126605\n",
      "[8500]\ttraining's l1: 0.0228329\tvalid_1's l1: 0.126524\n",
      "[8600]\ttraining's l1: 0.0225024\tvalid_1's l1: 0.126455\n",
      "[8700]\ttraining's l1: 0.0221814\tvalid_1's l1: 0.1264\n",
      "[8800]\ttraining's l1: 0.021858\tvalid_1's l1: 0.126341\n",
      "[8900]\ttraining's l1: 0.0215273\tvalid_1's l1: 0.126264\n",
      "[9000]\ttraining's l1: 0.0212201\tvalid_1's l1: 0.126196\n",
      "[9100]\ttraining's l1: 0.0209122\tvalid_1's l1: 0.12614\n",
      "[9200]\ttraining's l1: 0.0206211\tvalid_1's l1: 0.126086\n",
      "[9300]\ttraining's l1: 0.0203245\tvalid_1's l1: 0.126045\n",
      "[9400]\ttraining's l1: 0.0200334\tvalid_1's l1: 0.125992\n",
      "[9500]\ttraining's l1: 0.0197394\tvalid_1's l1: 0.125925\n",
      "[9600]\ttraining's l1: 0.019449\tvalid_1's l1: 0.125873\n",
      "[9700]\ttraining's l1: 0.0191675\tvalid_1's l1: 0.12582\n",
      "[9800]\ttraining's l1: 0.0189065\tvalid_1's l1: 0.125771\n",
      "[9900]\ttraining's l1: 0.0186409\tvalid_1's l1: 0.125717\n",
      "[10000]\ttraining's l1: 0.0183787\tvalid_1's l1: 0.12567\n",
      "[10100]\ttraining's l1: 0.0181191\tvalid_1's l1: 0.125628\n",
      "[10200]\ttraining's l1: 0.0178635\tvalid_1's l1: 0.125594\n",
      "[10300]\ttraining's l1: 0.0176188\tvalid_1's l1: 0.12555\n",
      "[10400]\ttraining's l1: 0.0173817\tvalid_1's l1: 0.125512\n",
      "[10500]\ttraining's l1: 0.0171374\tvalid_1's l1: 0.125477\n",
      "[10600]\ttraining's l1: 0.0169127\tvalid_1's l1: 0.125429\n",
      "[10700]\ttraining's l1: 0.0166933\tvalid_1's l1: 0.125393\n",
      "[10800]\ttraining's l1: 0.0164676\tvalid_1's l1: 0.125346\n",
      "[10900]\ttraining's l1: 0.0162508\tvalid_1's l1: 0.125313\n",
      "[11000]\ttraining's l1: 0.0160413\tvalid_1's l1: 0.125267\n",
      "[11100]\ttraining's l1: 0.015815\tvalid_1's l1: 0.125227\n",
      "[11200]\ttraining's l1: 0.0156012\tvalid_1's l1: 0.125194\n",
      "[11300]\ttraining's l1: 0.0153928\tvalid_1's l1: 0.125158\n",
      "[11400]\ttraining's l1: 0.0151877\tvalid_1's l1: 0.125129\n",
      "[11500]\ttraining's l1: 0.0149893\tvalid_1's l1: 0.125097\n",
      "[11600]\ttraining's l1: 0.0147944\tvalid_1's l1: 0.125063\n",
      "[11700]\ttraining's l1: 0.0146019\tvalid_1's l1: 0.125023\n",
      "[11800]\ttraining's l1: 0.0144107\tvalid_1's l1: 0.124985\n",
      "[11900]\ttraining's l1: 0.0142252\tvalid_1's l1: 0.12495\n",
      "[12000]\ttraining's l1: 0.0140483\tvalid_1's l1: 0.124927\n",
      "[12100]\ttraining's l1: 0.013866\tvalid_1's l1: 0.124897\n",
      "[12200]\ttraining's l1: 0.0136898\tvalid_1's l1: 0.124859\n",
      "[12300]\ttraining's l1: 0.0135148\tvalid_1's l1: 0.12483\n",
      "[12400]\ttraining's l1: 0.0133501\tvalid_1's l1: 0.124805\n",
      "[12500]\ttraining's l1: 0.0131832\tvalid_1's l1: 0.124775\n",
      "[12600]\ttraining's l1: 0.0130082\tvalid_1's l1: 0.124757\n",
      "[12700]\ttraining's l1: 0.0128441\tvalid_1's l1: 0.124729\n",
      "[12800]\ttraining's l1: 0.0126884\tvalid_1's l1: 0.124713\n",
      "[12900]\ttraining's l1: 0.0125319\tvalid_1's l1: 0.124686\n",
      "[13000]\ttraining's l1: 0.0123838\tvalid_1's l1: 0.124668\n",
      "[13100]\ttraining's l1: 0.0122255\tvalid_1's l1: 0.124642\n",
      "[13200]\ttraining's l1: 0.0120714\tvalid_1's l1: 0.124621\n",
      "[13300]\ttraining's l1: 0.0119223\tvalid_1's l1: 0.124602\n",
      "[13400]\ttraining's l1: 0.0117745\tvalid_1's l1: 0.124581\n",
      "[13500]\ttraining's l1: 0.0116287\tvalid_1's l1: 0.124558\n",
      "[13600]\ttraining's l1: 0.0114887\tvalid_1's l1: 0.124535\n",
      "[13700]\ttraining's l1: 0.0113505\tvalid_1's l1: 0.124514\n",
      "[13800]\ttraining's l1: 0.0112099\tvalid_1's l1: 0.124498\n",
      "[13900]\ttraining's l1: 0.0110753\tvalid_1's l1: 0.124475\n",
      "[14000]\ttraining's l1: 0.0109411\tvalid_1's l1: 0.12446\n",
      "[14100]\ttraining's l1: 0.010811\tvalid_1's l1: 0.124438\n",
      "[14200]\ttraining's l1: 0.0106863\tvalid_1's l1: 0.124418\n",
      "[14300]\ttraining's l1: 0.0105559\tvalid_1's l1: 0.124392\n",
      "[14400]\ttraining's l1: 0.0104325\tvalid_1's l1: 0.124376\n",
      "[14500]\ttraining's l1: 0.010308\tvalid_1's l1: 0.124352\n",
      "[14600]\ttraining's l1: 0.0101876\tvalid_1's l1: 0.124339\n",
      "[14700]\ttraining's l1: 0.010066\tvalid_1's l1: 0.124321\n",
      "[14800]\ttraining's l1: 0.0099471\tvalid_1's l1: 0.124301\n",
      "[14900]\ttraining's l1: 0.00982678\tvalid_1's l1: 0.124282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.00970822\tvalid_1's l1: 0.124266\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00970822\tvalid_1's l1: 0.124266\n",
      "2JHN Fold 3, logMAE: -2.0853284440025797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.295427\tvalid_1's l1: 0.300505\n",
      "[200]\ttraining's l1: 0.233706\tvalid_1's l1: 0.245826\n",
      "[300]\ttraining's l1: 0.203577\tvalid_1's l1: 0.221676\n",
      "[400]\ttraining's l1: 0.182867\tvalid_1's l1: 0.205804\n",
      "[500]\ttraining's l1: 0.167389\tvalid_1's l1: 0.194544\n",
      "[600]\ttraining's l1: 0.154941\tvalid_1's l1: 0.185827\n",
      "[700]\ttraining's l1: 0.144675\tvalid_1's l1: 0.179047\n",
      "[800]\ttraining's l1: 0.136601\tvalid_1's l1: 0.173999\n",
      "[900]\ttraining's l1: 0.129423\tvalid_1's l1: 0.169825\n",
      "[1000]\ttraining's l1: 0.122839\tvalid_1's l1: 0.165738\n",
      "[1100]\ttraining's l1: 0.117006\tvalid_1's l1: 0.162351\n",
      "[1200]\ttraining's l1: 0.111955\tvalid_1's l1: 0.159548\n",
      "[1300]\ttraining's l1: 0.107526\tvalid_1's l1: 0.157255\n",
      "[1400]\ttraining's l1: 0.103311\tvalid_1's l1: 0.155119\n",
      "[1500]\ttraining's l1: 0.0993272\tvalid_1's l1: 0.153169\n",
      "[1600]\ttraining's l1: 0.0957087\tvalid_1's l1: 0.151437\n",
      "[1700]\ttraining's l1: 0.0922748\tvalid_1's l1: 0.149753\n",
      "[1800]\ttraining's l1: 0.0892135\tvalid_1's l1: 0.148331\n",
      "[1900]\ttraining's l1: 0.0862819\tvalid_1's l1: 0.147019\n",
      "[2000]\ttraining's l1: 0.0835838\tvalid_1's l1: 0.145822\n",
      "[2100]\ttraining's l1: 0.0810398\tvalid_1's l1: 0.144693\n",
      "[2200]\ttraining's l1: 0.0786436\tvalid_1's l1: 0.143588\n",
      "[2300]\ttraining's l1: 0.0762668\tvalid_1's l1: 0.142583\n",
      "[2400]\ttraining's l1: 0.074137\tvalid_1's l1: 0.141699\n",
      "[2500]\ttraining's l1: 0.0720848\tvalid_1's l1: 0.14087\n",
      "[2600]\ttraining's l1: 0.0701043\tvalid_1's l1: 0.140137\n",
      "[2700]\ttraining's l1: 0.0682667\tvalid_1's l1: 0.13948\n",
      "[2800]\ttraining's l1: 0.0665046\tvalid_1's l1: 0.138801\n",
      "[2900]\ttraining's l1: 0.0648301\tvalid_1's l1: 0.138223\n",
      "[3000]\ttraining's l1: 0.063243\tvalid_1's l1: 0.137636\n",
      "[3100]\ttraining's l1: 0.0616635\tvalid_1's l1: 0.137079\n",
      "[3200]\ttraining's l1: 0.0601718\tvalid_1's l1: 0.13655\n",
      "[3300]\ttraining's l1: 0.0587777\tvalid_1's l1: 0.136064\n",
      "[3400]\ttraining's l1: 0.0574183\tvalid_1's l1: 0.13555\n",
      "[3500]\ttraining's l1: 0.0561913\tvalid_1's l1: 0.135162\n",
      "[3600]\ttraining's l1: 0.0549484\tvalid_1's l1: 0.134675\n",
      "[3700]\ttraining's l1: 0.0537437\tvalid_1's l1: 0.134266\n",
      "[3800]\ttraining's l1: 0.0525998\tvalid_1's l1: 0.133883\n",
      "[3900]\ttraining's l1: 0.0514789\tvalid_1's l1: 0.133498\n",
      "[4000]\ttraining's l1: 0.0503628\tvalid_1's l1: 0.133123\n",
      "[4100]\ttraining's l1: 0.049296\tvalid_1's l1: 0.132784\n",
      "[4200]\ttraining's l1: 0.0482451\tvalid_1's l1: 0.132457\n",
      "[4300]\ttraining's l1: 0.0472545\tvalid_1's l1: 0.132197\n",
      "[4400]\ttraining's l1: 0.0463293\tvalid_1's l1: 0.131909\n",
      "[4500]\ttraining's l1: 0.0454131\tvalid_1's l1: 0.131638\n",
      "[4600]\ttraining's l1: 0.0445158\tvalid_1's l1: 0.131393\n",
      "[4700]\ttraining's l1: 0.0436712\tvalid_1's l1: 0.131121\n",
      "[4800]\ttraining's l1: 0.0428496\tvalid_1's l1: 0.130855\n",
      "[4900]\ttraining's l1: 0.0420339\tvalid_1's l1: 0.130592\n",
      "[5000]\ttraining's l1: 0.0412215\tvalid_1's l1: 0.130373\n",
      "[5100]\ttraining's l1: 0.0404652\tvalid_1's l1: 0.130143\n",
      "[5200]\ttraining's l1: 0.0397138\tvalid_1's l1: 0.129943\n",
      "[5300]\ttraining's l1: 0.0389813\tvalid_1's l1: 0.129753\n",
      "[5400]\ttraining's l1: 0.0382513\tvalid_1's l1: 0.129553\n",
      "[5500]\ttraining's l1: 0.0375409\tvalid_1's l1: 0.129353\n",
      "[5600]\ttraining's l1: 0.036835\tvalid_1's l1: 0.12917\n",
      "[5700]\ttraining's l1: 0.0361663\tvalid_1's l1: 0.129002\n",
      "[5800]\ttraining's l1: 0.0355376\tvalid_1's l1: 0.128845\n",
      "[5900]\ttraining's l1: 0.0348914\tvalid_1's l1: 0.128677\n",
      "[6000]\ttraining's l1: 0.0342957\tvalid_1's l1: 0.128528\n",
      "[6100]\ttraining's l1: 0.0336872\tvalid_1's l1: 0.12837\n",
      "[6200]\ttraining's l1: 0.0331229\tvalid_1's l1: 0.128238\n",
      "[6300]\ttraining's l1: 0.0325353\tvalid_1's l1: 0.128104\n",
      "[6400]\ttraining's l1: 0.031996\tvalid_1's l1: 0.127985\n",
      "[6500]\ttraining's l1: 0.0314747\tvalid_1's l1: 0.127872\n",
      "[6600]\ttraining's l1: 0.0309638\tvalid_1's l1: 0.127779\n",
      "[6700]\ttraining's l1: 0.0304522\tvalid_1's l1: 0.127661\n",
      "[6800]\ttraining's l1: 0.0299408\tvalid_1's l1: 0.127525\n",
      "[6900]\ttraining's l1: 0.0294606\tvalid_1's l1: 0.127391\n",
      "[7000]\ttraining's l1: 0.0290014\tvalid_1's l1: 0.12728\n",
      "[7100]\ttraining's l1: 0.0285304\tvalid_1's l1: 0.127169\n",
      "[7200]\ttraining's l1: 0.0280618\tvalid_1's l1: 0.127088\n",
      "[7300]\ttraining's l1: 0.0276133\tvalid_1's l1: 0.126996\n",
      "[7400]\ttraining's l1: 0.0271686\tvalid_1's l1: 0.126893\n",
      "[7500]\ttraining's l1: 0.0267493\tvalid_1's l1: 0.126791\n",
      "[7600]\ttraining's l1: 0.0263507\tvalid_1's l1: 0.126709\n",
      "[7700]\ttraining's l1: 0.0259366\tvalid_1's l1: 0.12662\n",
      "[7800]\ttraining's l1: 0.0255251\tvalid_1's l1: 0.126532\n",
      "[7900]\ttraining's l1: 0.0251346\tvalid_1's l1: 0.126447\n",
      "[8000]\ttraining's l1: 0.0247525\tvalid_1's l1: 0.126368\n",
      "[8100]\ttraining's l1: 0.024374\tvalid_1's l1: 0.126293\n",
      "[8200]\ttraining's l1: 0.0239995\tvalid_1's l1: 0.126212\n",
      "[8300]\ttraining's l1: 0.0236484\tvalid_1's l1: 0.12612\n",
      "[8400]\ttraining's l1: 0.023295\tvalid_1's l1: 0.126051\n",
      "[8500]\ttraining's l1: 0.0229296\tvalid_1's l1: 0.12596\n",
      "[8600]\ttraining's l1: 0.0225947\tvalid_1's l1: 0.125911\n",
      "[8700]\ttraining's l1: 0.0222607\tvalid_1's l1: 0.125832\n",
      "[8800]\ttraining's l1: 0.0219362\tvalid_1's l1: 0.125759\n",
      "[8900]\ttraining's l1: 0.0216082\tvalid_1's l1: 0.125696\n",
      "[9000]\ttraining's l1: 0.0212938\tvalid_1's l1: 0.125649\n",
      "[9100]\ttraining's l1: 0.0210027\tvalid_1's l1: 0.125599\n",
      "[9200]\ttraining's l1: 0.0206951\tvalid_1's l1: 0.125549\n",
      "[9300]\ttraining's l1: 0.0203995\tvalid_1's l1: 0.125494\n",
      "[9400]\ttraining's l1: 0.0200953\tvalid_1's l1: 0.125444\n",
      "[9500]\ttraining's l1: 0.0198108\tvalid_1's l1: 0.125395\n",
      "[9600]\ttraining's l1: 0.0195236\tvalid_1's l1: 0.125339\n",
      "[9700]\ttraining's l1: 0.0192363\tvalid_1's l1: 0.125281\n",
      "[9800]\ttraining's l1: 0.018957\tvalid_1's l1: 0.125238\n",
      "[9900]\ttraining's l1: 0.0186938\tvalid_1's l1: 0.125213\n",
      "[10000]\ttraining's l1: 0.0184302\tvalid_1's l1: 0.125157\n",
      "[10100]\ttraining's l1: 0.01818\tvalid_1's l1: 0.125111\n",
      "[10200]\ttraining's l1: 0.0179295\tvalid_1's l1: 0.125064\n",
      "[10300]\ttraining's l1: 0.0176753\tvalid_1's l1: 0.125009\n",
      "[10400]\ttraining's l1: 0.0174198\tvalid_1's l1: 0.124952\n",
      "[10500]\ttraining's l1: 0.0171823\tvalid_1's l1: 0.124911\n",
      "[10600]\ttraining's l1: 0.0169546\tvalid_1's l1: 0.124865\n",
      "[10700]\ttraining's l1: 0.0167262\tvalid_1's l1: 0.124828\n",
      "[10800]\ttraining's l1: 0.0165099\tvalid_1's l1: 0.124799\n",
      "[10900]\ttraining's l1: 0.0162914\tvalid_1's l1: 0.124766\n",
      "[11000]\ttraining's l1: 0.0160716\tvalid_1's l1: 0.124734\n",
      "[11100]\ttraining's l1: 0.0158683\tvalid_1's l1: 0.124707\n",
      "[11200]\ttraining's l1: 0.0156523\tvalid_1's l1: 0.124668\n",
      "[11300]\ttraining's l1: 0.0154323\tvalid_1's l1: 0.12463\n",
      "[11400]\ttraining's l1: 0.0152347\tvalid_1's l1: 0.124608\n",
      "[11500]\ttraining's l1: 0.0150427\tvalid_1's l1: 0.124573\n",
      "[11600]\ttraining's l1: 0.0148509\tvalid_1's l1: 0.12455\n",
      "[11700]\ttraining's l1: 0.0146665\tvalid_1's l1: 0.124523\n",
      "[11800]\ttraining's l1: 0.014477\tvalid_1's l1: 0.124487\n",
      "[11900]\ttraining's l1: 0.014289\tvalid_1's l1: 0.124449\n",
      "[12000]\ttraining's l1: 0.0141052\tvalid_1's l1: 0.124414\n",
      "[12100]\ttraining's l1: 0.0139173\tvalid_1's l1: 0.124378\n",
      "[12200]\ttraining's l1: 0.0137392\tvalid_1's l1: 0.12435\n",
      "[12300]\ttraining's l1: 0.0135651\tvalid_1's l1: 0.124321\n",
      "[12400]\ttraining's l1: 0.0133936\tvalid_1's l1: 0.124303\n",
      "[12500]\ttraining's l1: 0.0132224\tvalid_1's l1: 0.124283\n",
      "[12600]\ttraining's l1: 0.0130475\tvalid_1's l1: 0.124259\n",
      "[12700]\ttraining's l1: 0.0128825\tvalid_1's l1: 0.124233\n",
      "[12800]\ttraining's l1: 0.0127209\tvalid_1's l1: 0.12421\n",
      "[12900]\ttraining's l1: 0.012563\tvalid_1's l1: 0.124178\n",
      "[13000]\ttraining's l1: 0.0124072\tvalid_1's l1: 0.124154\n",
      "[13100]\ttraining's l1: 0.0122592\tvalid_1's l1: 0.124124\n",
      "[13200]\ttraining's l1: 0.0121109\tvalid_1's l1: 0.12411\n",
      "[13300]\ttraining's l1: 0.011957\tvalid_1's l1: 0.124084\n",
      "[13400]\ttraining's l1: 0.0118151\tvalid_1's l1: 0.124073\n",
      "[13500]\ttraining's l1: 0.0116644\tvalid_1's l1: 0.124055\n",
      "[13600]\ttraining's l1: 0.0115228\tvalid_1's l1: 0.124036\n",
      "[13700]\ttraining's l1: 0.0113738\tvalid_1's l1: 0.124018\n",
      "[13800]\ttraining's l1: 0.011236\tvalid_1's l1: 0.124001\n",
      "[13900]\ttraining's l1: 0.011106\tvalid_1's l1: 0.123981\n",
      "[14000]\ttraining's l1: 0.0109749\tvalid_1's l1: 0.123969\n",
      "[14100]\ttraining's l1: 0.0108423\tvalid_1's l1: 0.123953\n",
      "[14200]\ttraining's l1: 0.0107122\tvalid_1's l1: 0.123929\n",
      "[14300]\ttraining's l1: 0.0105864\tvalid_1's l1: 0.123909\n",
      "[14400]\ttraining's l1: 0.0104593\tvalid_1's l1: 0.123892\n",
      "[14500]\ttraining's l1: 0.0103324\tvalid_1's l1: 0.123876\n",
      "[14600]\ttraining's l1: 0.010206\tvalid_1's l1: 0.123848\n",
      "[14700]\ttraining's l1: 0.0100882\tvalid_1's l1: 0.123834\n",
      "[14800]\ttraining's l1: 0.0099674\tvalid_1's l1: 0.123811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14900]\ttraining's l1: 0.00985231\tvalid_1's l1: 0.123789\n",
      "[15000]\ttraining's l1: 0.00973819\tvalid_1's l1: 0.123774\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.00973819\tvalid_1's l1: 0.123774\n",
      "2JHN Fold 4, logMAE: -2.089298129231239\n",
      "*** Training Model for 2JHC ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.722765\tvalid_1's l1: 0.725662\n",
      "[200]\ttraining's l1: 0.611337\tvalid_1's l1: 0.616341\n",
      "[300]\ttraining's l1: 0.554353\tvalid_1's l1: 0.561442\n",
      "[400]\ttraining's l1: 0.517572\tvalid_1's l1: 0.526385\n",
      "[500]\ttraining's l1: 0.48935\tvalid_1's l1: 0.49993\n",
      "[600]\ttraining's l1: 0.467363\tvalid_1's l1: 0.479434\n",
      "[700]\ttraining's l1: 0.450428\tvalid_1's l1: 0.463985\n",
      "[800]\ttraining's l1: 0.435485\tvalid_1's l1: 0.450293\n",
      "[900]\ttraining's l1: 0.422653\tvalid_1's l1: 0.438781\n",
      "[1000]\ttraining's l1: 0.41122\tvalid_1's l1: 0.428453\n",
      "[1100]\ttraining's l1: 0.400796\tvalid_1's l1: 0.419149\n",
      "[1200]\ttraining's l1: 0.391203\tvalid_1's l1: 0.41063\n",
      "[1300]\ttraining's l1: 0.38248\tvalid_1's l1: 0.403\n",
      "[1400]\ttraining's l1: 0.374809\tvalid_1's l1: 0.396593\n",
      "[1500]\ttraining's l1: 0.367849\tvalid_1's l1: 0.390682\n",
      "[1600]\ttraining's l1: 0.361078\tvalid_1's l1: 0.38498\n",
      "[1700]\ttraining's l1: 0.355076\tvalid_1's l1: 0.380077\n",
      "[1800]\ttraining's l1: 0.349288\tvalid_1's l1: 0.375397\n",
      "[1900]\ttraining's l1: 0.343851\tvalid_1's l1: 0.371034\n",
      "[2000]\ttraining's l1: 0.33878\tvalid_1's l1: 0.367022\n",
      "[2100]\ttraining's l1: 0.334153\tvalid_1's l1: 0.363403\n",
      "[2200]\ttraining's l1: 0.329274\tvalid_1's l1: 0.359443\n",
      "[2300]\ttraining's l1: 0.324823\tvalid_1's l1: 0.355914\n",
      "[2400]\ttraining's l1: 0.320561\tvalid_1's l1: 0.352639\n",
      "[2500]\ttraining's l1: 0.316466\tvalid_1's l1: 0.349438\n",
      "[2600]\ttraining's l1: 0.312485\tvalid_1's l1: 0.346323\n",
      "[2700]\ttraining's l1: 0.308805\tvalid_1's l1: 0.343493\n",
      "[2800]\ttraining's l1: 0.305231\tvalid_1's l1: 0.340841\n",
      "[2900]\ttraining's l1: 0.301733\tvalid_1's l1: 0.338208\n",
      "[3000]\ttraining's l1: 0.29851\tvalid_1's l1: 0.335947\n",
      "[3100]\ttraining's l1: 0.295276\tvalid_1's l1: 0.333482\n",
      "[3200]\ttraining's l1: 0.292286\tvalid_1's l1: 0.331303\n",
      "[3300]\ttraining's l1: 0.289462\tvalid_1's l1: 0.329261\n",
      "[3400]\ttraining's l1: 0.286468\tvalid_1's l1: 0.327047\n",
      "[3500]\ttraining's l1: 0.28379\tvalid_1's l1: 0.325179\n",
      "[3600]\ttraining's l1: 0.28122\tvalid_1's l1: 0.323377\n",
      "[3700]\ttraining's l1: 0.278687\tvalid_1's l1: 0.321608\n",
      "[3800]\ttraining's l1: 0.276138\tvalid_1's l1: 0.319809\n",
      "[3900]\ttraining's l1: 0.273588\tvalid_1's l1: 0.318024\n",
      "[4000]\ttraining's l1: 0.271243\tvalid_1's l1: 0.316424\n",
      "[4100]\ttraining's l1: 0.268996\tvalid_1's l1: 0.314936\n",
      "[4200]\ttraining's l1: 0.266596\tvalid_1's l1: 0.31328\n",
      "[4300]\ttraining's l1: 0.264355\tvalid_1's l1: 0.311781\n",
      "[4400]\ttraining's l1: 0.262267\tvalid_1's l1: 0.310378\n",
      "[4500]\ttraining's l1: 0.260156\tvalid_1's l1: 0.309004\n",
      "[4600]\ttraining's l1: 0.258093\tvalid_1's l1: 0.307655\n",
      "[4700]\ttraining's l1: 0.256227\tvalid_1's l1: 0.306552\n",
      "[4800]\ttraining's l1: 0.254163\tvalid_1's l1: 0.30519\n",
      "[4900]\ttraining's l1: 0.252147\tvalid_1's l1: 0.303865\n",
      "[5000]\ttraining's l1: 0.250303\tvalid_1's l1: 0.302672\n",
      "[5100]\ttraining's l1: 0.248439\tvalid_1's l1: 0.301456\n",
      "[5200]\ttraining's l1: 0.2466\tvalid_1's l1: 0.3003\n",
      "[5300]\ttraining's l1: 0.244864\tvalid_1's l1: 0.299198\n",
      "[5400]\ttraining's l1: 0.243133\tvalid_1's l1: 0.298099\n",
      "[5500]\ttraining's l1: 0.241397\tvalid_1's l1: 0.29697\n",
      "[5600]\ttraining's l1: 0.239758\tvalid_1's l1: 0.295961\n",
      "[5700]\ttraining's l1: 0.238031\tvalid_1's l1: 0.294844\n",
      "[5800]\ttraining's l1: 0.236394\tvalid_1's l1: 0.293854\n",
      "[5900]\ttraining's l1: 0.234799\tvalid_1's l1: 0.292831\n",
      "[6000]\ttraining's l1: 0.233237\tvalid_1's l1: 0.29188\n",
      "[6100]\ttraining's l1: 0.231731\tvalid_1's l1: 0.290986\n",
      "[6200]\ttraining's l1: 0.230258\tvalid_1's l1: 0.290087\n",
      "[6300]\ttraining's l1: 0.228806\tvalid_1's l1: 0.289217\n",
      "[6400]\ttraining's l1: 0.227415\tvalid_1's l1: 0.288372\n",
      "[6500]\ttraining's l1: 0.225918\tvalid_1's l1: 0.287434\n",
      "[6600]\ttraining's l1: 0.224477\tvalid_1's l1: 0.286533\n",
      "[6700]\ttraining's l1: 0.223058\tvalid_1's l1: 0.285654\n",
      "[6800]\ttraining's l1: 0.221687\tvalid_1's l1: 0.28481\n",
      "[6900]\ttraining's l1: 0.220341\tvalid_1's l1: 0.284013\n",
      "[7000]\ttraining's l1: 0.219008\tvalid_1's l1: 0.283189\n",
      "[7100]\ttraining's l1: 0.217743\tvalid_1's l1: 0.282448\n",
      "[7200]\ttraining's l1: 0.216506\tvalid_1's l1: 0.281728\n",
      "[7300]\ttraining's l1: 0.215256\tvalid_1's l1: 0.281019\n",
      "[7400]\ttraining's l1: 0.214017\tvalid_1's l1: 0.280341\n",
      "[7500]\ttraining's l1: 0.212813\tvalid_1's l1: 0.279706\n",
      "[7600]\ttraining's l1: 0.21164\tvalid_1's l1: 0.279062\n",
      "[7700]\ttraining's l1: 0.210469\tvalid_1's l1: 0.278413\n",
      "[7800]\ttraining's l1: 0.2093\tvalid_1's l1: 0.277748\n",
      "[7900]\ttraining's l1: 0.208195\tvalid_1's l1: 0.277128\n",
      "[8000]\ttraining's l1: 0.207035\tvalid_1's l1: 0.276464\n",
      "[8100]\ttraining's l1: 0.205888\tvalid_1's l1: 0.275799\n",
      "[8200]\ttraining's l1: 0.204778\tvalid_1's l1: 0.275182\n",
      "[8300]\ttraining's l1: 0.203638\tvalid_1's l1: 0.274506\n",
      "[8400]\ttraining's l1: 0.202551\tvalid_1's l1: 0.273909\n",
      "[8500]\ttraining's l1: 0.201534\tvalid_1's l1: 0.273379\n",
      "[8600]\ttraining's l1: 0.200465\tvalid_1's l1: 0.27277\n",
      "[8700]\ttraining's l1: 0.199415\tvalid_1's l1: 0.272185\n",
      "[8800]\ttraining's l1: 0.198392\tvalid_1's l1: 0.27161\n",
      "[8900]\ttraining's l1: 0.197345\tvalid_1's l1: 0.271041\n",
      "[9000]\ttraining's l1: 0.196408\tvalid_1's l1: 0.27057\n",
      "[9100]\ttraining's l1: 0.195419\tvalid_1's l1: 0.270056\n",
      "[9200]\ttraining's l1: 0.194475\tvalid_1's l1: 0.269568\n",
      "[9300]\ttraining's l1: 0.193513\tvalid_1's l1: 0.269059\n",
      "[9400]\ttraining's l1: 0.192539\tvalid_1's l1: 0.268568\n",
      "[9500]\ttraining's l1: 0.191596\tvalid_1's l1: 0.268039\n",
      "[9600]\ttraining's l1: 0.190662\tvalid_1's l1: 0.267554\n",
      "[9700]\ttraining's l1: 0.189737\tvalid_1's l1: 0.267065\n",
      "[9800]\ttraining's l1: 0.188814\tvalid_1's l1: 0.266581\n",
      "[9900]\ttraining's l1: 0.187906\tvalid_1's l1: 0.266105\n",
      "[10000]\ttraining's l1: 0.187032\tvalid_1's l1: 0.265665\n",
      "[10100]\ttraining's l1: 0.186157\tvalid_1's l1: 0.265186\n",
      "[10200]\ttraining's l1: 0.185281\tvalid_1's l1: 0.264716\n",
      "[10300]\ttraining's l1: 0.184397\tvalid_1's l1: 0.264242\n",
      "[10400]\ttraining's l1: 0.183544\tvalid_1's l1: 0.263836\n",
      "[10500]\ttraining's l1: 0.182688\tvalid_1's l1: 0.263385\n",
      "[10600]\ttraining's l1: 0.181858\tvalid_1's l1: 0.262973\n",
      "[10700]\ttraining's l1: 0.181029\tvalid_1's l1: 0.262576\n",
      "[10800]\ttraining's l1: 0.180226\tvalid_1's l1: 0.262181\n",
      "[10900]\ttraining's l1: 0.179403\tvalid_1's l1: 0.261759\n",
      "[11000]\ttraining's l1: 0.178613\tvalid_1's l1: 0.261393\n",
      "[11100]\ttraining's l1: 0.177853\tvalid_1's l1: 0.261022\n",
      "[11200]\ttraining's l1: 0.177061\tvalid_1's l1: 0.26065\n",
      "[11300]\ttraining's l1: 0.176277\tvalid_1's l1: 0.260285\n",
      "[11400]\ttraining's l1: 0.175547\tvalid_1's l1: 0.259953\n",
      "[11500]\ttraining's l1: 0.174816\tvalid_1's l1: 0.259586\n",
      "[11600]\ttraining's l1: 0.17404\tvalid_1's l1: 0.259207\n",
      "[11700]\ttraining's l1: 0.173298\tvalid_1's l1: 0.258858\n",
      "[11800]\ttraining's l1: 0.172551\tvalid_1's l1: 0.258489\n",
      "[11900]\ttraining's l1: 0.171809\tvalid_1's l1: 0.258146\n",
      "[12000]\ttraining's l1: 0.171072\tvalid_1's l1: 0.257771\n",
      "[12100]\ttraining's l1: 0.170335\tvalid_1's l1: 0.257403\n",
      "[12200]\ttraining's l1: 0.169616\tvalid_1's l1: 0.257068\n",
      "[12300]\ttraining's l1: 0.16891\tvalid_1's l1: 0.256739\n",
      "[12400]\ttraining's l1: 0.168226\tvalid_1's l1: 0.256427\n",
      "[12500]\ttraining's l1: 0.167527\tvalid_1's l1: 0.2561\n",
      "[12600]\ttraining's l1: 0.16683\tvalid_1's l1: 0.25577\n",
      "[12700]\ttraining's l1: 0.166136\tvalid_1's l1: 0.255437\n",
      "[12800]\ttraining's l1: 0.165465\tvalid_1's l1: 0.255113\n",
      "[12900]\ttraining's l1: 0.164763\tvalid_1's l1: 0.254784\n",
      "[13000]\ttraining's l1: 0.16409\tvalid_1's l1: 0.254495\n",
      "[13100]\ttraining's l1: 0.16341\tvalid_1's l1: 0.254155\n",
      "[13200]\ttraining's l1: 0.162719\tvalid_1's l1: 0.253837\n",
      "[13300]\ttraining's l1: 0.162081\tvalid_1's l1: 0.253556\n",
      "[13400]\ttraining's l1: 0.161438\tvalid_1's l1: 0.253255\n",
      "[13500]\ttraining's l1: 0.160789\tvalid_1's l1: 0.252996\n",
      "[13600]\ttraining's l1: 0.16015\tvalid_1's l1: 0.252705\n",
      "[13700]\ttraining's l1: 0.159544\tvalid_1's l1: 0.252436\n",
      "[13800]\ttraining's l1: 0.158938\tvalid_1's l1: 0.252165\n",
      "[13900]\ttraining's l1: 0.158318\tvalid_1's l1: 0.251904\n",
      "[14000]\ttraining's l1: 0.157715\tvalid_1's l1: 0.251628\n",
      "[14100]\ttraining's l1: 0.15711\tvalid_1's l1: 0.251371\n",
      "[14200]\ttraining's l1: 0.156506\tvalid_1's l1: 0.251089\n",
      "[14300]\ttraining's l1: 0.155875\tvalid_1's l1: 0.250809\n",
      "[14400]\ttraining's l1: 0.155297\tvalid_1's l1: 0.250558\n",
      "[14500]\ttraining's l1: 0.1547\tvalid_1's l1: 0.250307\n",
      "[14600]\ttraining's l1: 0.154104\tvalid_1's l1: 0.25004\n",
      "[14700]\ttraining's l1: 0.153531\tvalid_1's l1: 0.249802\n",
      "[14800]\ttraining's l1: 0.152987\tvalid_1's l1: 0.249572\n",
      "[14900]\ttraining's l1: 0.152393\tvalid_1's l1: 0.249322\n",
      "[15000]\ttraining's l1: 0.151838\tvalid_1's l1: 0.249086\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.151838\tvalid_1's l1: 0.249086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2JHC Fold 0, logMAE: -1.3899488913519698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.730969\tvalid_1's l1: 0.733814\n",
      "[200]\ttraining's l1: 0.616368\tvalid_1's l1: 0.621529\n",
      "[300]\ttraining's l1: 0.55976\tvalid_1's l1: 0.566687\n",
      "[400]\ttraining's l1: 0.519902\tvalid_1's l1: 0.528359\n",
      "[500]\ttraining's l1: 0.492238\tvalid_1's l1: 0.502148\n",
      "[600]\ttraining's l1: 0.470232\tvalid_1's l1: 0.48166\n",
      "[700]\ttraining's l1: 0.450807\tvalid_1's l1: 0.463661\n",
      "[800]\ttraining's l1: 0.435713\tvalid_1's l1: 0.449952\n",
      "[900]\ttraining's l1: 0.422292\tvalid_1's l1: 0.437945\n",
      "[1000]\ttraining's l1: 0.410885\tvalid_1's l1: 0.42769\n",
      "[1100]\ttraining's l1: 0.400754\tvalid_1's l1: 0.418809\n",
      "[1200]\ttraining's l1: 0.391971\tvalid_1's l1: 0.411305\n",
      "[1300]\ttraining's l1: 0.383525\tvalid_1's l1: 0.404223\n",
      "[1400]\ttraining's l1: 0.375923\tvalid_1's l1: 0.397829\n",
      "[1500]\ttraining's l1: 0.368568\tvalid_1's l1: 0.391612\n",
      "[1600]\ttraining's l1: 0.362146\tvalid_1's l1: 0.386268\n",
      "[1700]\ttraining's l1: 0.35602\tvalid_1's l1: 0.381215\n",
      "[1800]\ttraining's l1: 0.349852\tvalid_1's l1: 0.376038\n",
      "[1900]\ttraining's l1: 0.343946\tvalid_1's l1: 0.371149\n",
      "[2000]\ttraining's l1: 0.338936\tvalid_1's l1: 0.367146\n",
      "[2100]\ttraining's l1: 0.334013\tvalid_1's l1: 0.363235\n",
      "[2200]\ttraining's l1: 0.328977\tvalid_1's l1: 0.35913\n",
      "[2300]\ttraining's l1: 0.324384\tvalid_1's l1: 0.3555\n",
      "[2400]\ttraining's l1: 0.320114\tvalid_1's l1: 0.3522\n",
      "[2500]\ttraining's l1: 0.316123\tvalid_1's l1: 0.349051\n",
      "[2600]\ttraining's l1: 0.312196\tvalid_1's l1: 0.346064\n",
      "[2700]\ttraining's l1: 0.30854\tvalid_1's l1: 0.343248\n",
      "[2800]\ttraining's l1: 0.30512\tvalid_1's l1: 0.340707\n",
      "[2900]\ttraining's l1: 0.3018\tvalid_1's l1: 0.338275\n",
      "[3000]\ttraining's l1: 0.29843\tvalid_1's l1: 0.335813\n",
      "[3100]\ttraining's l1: 0.295098\tvalid_1's l1: 0.333252\n",
      "[3200]\ttraining's l1: 0.2919\tvalid_1's l1: 0.330896\n",
      "[3300]\ttraining's l1: 0.289029\tvalid_1's l1: 0.32883\n",
      "[3400]\ttraining's l1: 0.286231\tvalid_1's l1: 0.326804\n",
      "[3500]\ttraining's l1: 0.283624\tvalid_1's l1: 0.325024\n",
      "[3600]\ttraining's l1: 0.280873\tvalid_1's l1: 0.323075\n",
      "[3700]\ttraining's l1: 0.278207\tvalid_1's l1: 0.321147\n",
      "[3800]\ttraining's l1: 0.275677\tvalid_1's l1: 0.319361\n",
      "[3900]\ttraining's l1: 0.273203\tvalid_1's l1: 0.317625\n",
      "[4000]\ttraining's l1: 0.270815\tvalid_1's l1: 0.315996\n",
      "[4100]\ttraining's l1: 0.268397\tvalid_1's l1: 0.314335\n",
      "[4200]\ttraining's l1: 0.266173\tvalid_1's l1: 0.312841\n",
      "[4300]\ttraining's l1: 0.264061\tvalid_1's l1: 0.31147\n",
      "[4400]\ttraining's l1: 0.261851\tvalid_1's l1: 0.309928\n",
      "[4500]\ttraining's l1: 0.259888\tvalid_1's l1: 0.308645\n",
      "[4600]\ttraining's l1: 0.257796\tvalid_1's l1: 0.307278\n",
      "[4700]\ttraining's l1: 0.25586\tvalid_1's l1: 0.306026\n",
      "[4800]\ttraining's l1: 0.253888\tvalid_1's l1: 0.30471\n",
      "[4900]\ttraining's l1: 0.251901\tvalid_1's l1: 0.303385\n",
      "[5000]\ttraining's l1: 0.249984\tvalid_1's l1: 0.302146\n",
      "[5100]\ttraining's l1: 0.248103\tvalid_1's l1: 0.300942\n",
      "[5200]\ttraining's l1: 0.246383\tvalid_1's l1: 0.299867\n",
      "[5300]\ttraining's l1: 0.244617\tvalid_1's l1: 0.298719\n",
      "[5400]\ttraining's l1: 0.242956\tvalid_1's l1: 0.297689\n",
      "[5500]\ttraining's l1: 0.241231\tvalid_1's l1: 0.296589\n",
      "[5600]\ttraining's l1: 0.23958\tvalid_1's l1: 0.295568\n",
      "[5700]\ttraining's l1: 0.237947\tvalid_1's l1: 0.294506\n",
      "[5800]\ttraining's l1: 0.23635\tvalid_1's l1: 0.293552\n",
      "[5900]\ttraining's l1: 0.234798\tvalid_1's l1: 0.292616\n",
      "[6000]\ttraining's l1: 0.233188\tvalid_1's l1: 0.29159\n",
      "[6100]\ttraining's l1: 0.231703\tvalid_1's l1: 0.290698\n",
      "[6200]\ttraining's l1: 0.230214\tvalid_1's l1: 0.289787\n",
      "[6300]\ttraining's l1: 0.228734\tvalid_1's l1: 0.288912\n",
      "[6400]\ttraining's l1: 0.227301\tvalid_1's l1: 0.288041\n",
      "[6500]\ttraining's l1: 0.225878\tvalid_1's l1: 0.287194\n",
      "[6600]\ttraining's l1: 0.22448\tvalid_1's l1: 0.286421\n",
      "[6700]\ttraining's l1: 0.223082\tvalid_1's l1: 0.2856\n",
      "[6800]\ttraining's l1: 0.221714\tvalid_1's l1: 0.284827\n",
      "[6900]\ttraining's l1: 0.220355\tvalid_1's l1: 0.284026\n",
      "[7000]\ttraining's l1: 0.219047\tvalid_1's l1: 0.283261\n",
      "[7100]\ttraining's l1: 0.217748\tvalid_1's l1: 0.282493\n",
      "[7200]\ttraining's l1: 0.216474\tvalid_1's l1: 0.281741\n",
      "[7300]\ttraining's l1: 0.21525\tvalid_1's l1: 0.28101\n",
      "[7400]\ttraining's l1: 0.213999\tvalid_1's l1: 0.280304\n",
      "[7500]\ttraining's l1: 0.212793\tvalid_1's l1: 0.279595\n",
      "[7600]\ttraining's l1: 0.211613\tvalid_1's l1: 0.278889\n",
      "[7700]\ttraining's l1: 0.210395\tvalid_1's l1: 0.278183\n",
      "[7800]\ttraining's l1: 0.209263\tvalid_1's l1: 0.277574\n",
      "[7900]\ttraining's l1: 0.208166\tvalid_1's l1: 0.276983\n",
      "[8000]\ttraining's l1: 0.20703\tvalid_1's l1: 0.276366\n",
      "[8100]\ttraining's l1: 0.205933\tvalid_1's l1: 0.275764\n",
      "[8200]\ttraining's l1: 0.204784\tvalid_1's l1: 0.27513\n",
      "[8300]\ttraining's l1: 0.203716\tvalid_1's l1: 0.274554\n",
      "[8400]\ttraining's l1: 0.20271\tvalid_1's l1: 0.274021\n",
      "[8500]\ttraining's l1: 0.201644\tvalid_1's l1: 0.273451\n",
      "[8600]\ttraining's l1: 0.200646\tvalid_1's l1: 0.272941\n",
      "[8700]\ttraining's l1: 0.19962\tvalid_1's l1: 0.272404\n",
      "[8800]\ttraining's l1: 0.198592\tvalid_1's l1: 0.27183\n",
      "[8900]\ttraining's l1: 0.197608\tvalid_1's l1: 0.271303\n",
      "[9000]\ttraining's l1: 0.196593\tvalid_1's l1: 0.27073\n",
      "[9100]\ttraining's l1: 0.195586\tvalid_1's l1: 0.27021\n",
      "[9200]\ttraining's l1: 0.194598\tvalid_1's l1: 0.269677\n",
      "[9300]\ttraining's l1: 0.193639\tvalid_1's l1: 0.269216\n",
      "[9400]\ttraining's l1: 0.192736\tvalid_1's l1: 0.268774\n",
      "[9500]\ttraining's l1: 0.191822\tvalid_1's l1: 0.268294\n",
      "[9600]\ttraining's l1: 0.190911\tvalid_1's l1: 0.267828\n",
      "[9700]\ttraining's l1: 0.189964\tvalid_1's l1: 0.26731\n",
      "[9800]\ttraining's l1: 0.189006\tvalid_1's l1: 0.266819\n",
      "[9900]\ttraining's l1: 0.188118\tvalid_1's l1: 0.266361\n",
      "[10000]\ttraining's l1: 0.187248\tvalid_1's l1: 0.265913\n",
      "[10100]\ttraining's l1: 0.186374\tvalid_1's l1: 0.265454\n",
      "[10200]\ttraining's l1: 0.18551\tvalid_1's l1: 0.264995\n",
      "[10300]\ttraining's l1: 0.184626\tvalid_1's l1: 0.264538\n",
      "[10400]\ttraining's l1: 0.183774\tvalid_1's l1: 0.264099\n",
      "[10500]\ttraining's l1: 0.182938\tvalid_1's l1: 0.263669\n",
      "[10600]\ttraining's l1: 0.182144\tvalid_1's l1: 0.263282\n",
      "[10700]\ttraining's l1: 0.181315\tvalid_1's l1: 0.262871\n",
      "[10800]\ttraining's l1: 0.180489\tvalid_1's l1: 0.26245\n",
      "[10900]\ttraining's l1: 0.179678\tvalid_1's l1: 0.262024\n",
      "[11000]\ttraining's l1: 0.178898\tvalid_1's l1: 0.261636\n",
      "[11100]\ttraining's l1: 0.178089\tvalid_1's l1: 0.261253\n",
      "[11200]\ttraining's l1: 0.177302\tvalid_1's l1: 0.260875\n",
      "[11300]\ttraining's l1: 0.176519\tvalid_1's l1: 0.260474\n",
      "[11400]\ttraining's l1: 0.175716\tvalid_1's l1: 0.260031\n",
      "[11500]\ttraining's l1: 0.174926\tvalid_1's l1: 0.259608\n",
      "[11600]\ttraining's l1: 0.174173\tvalid_1's l1: 0.259251\n",
      "[11700]\ttraining's l1: 0.173444\tvalid_1's l1: 0.258907\n",
      "[11800]\ttraining's l1: 0.172737\tvalid_1's l1: 0.258583\n",
      "[11900]\ttraining's l1: 0.172002\tvalid_1's l1: 0.258227\n",
      "[12000]\ttraining's l1: 0.171271\tvalid_1's l1: 0.257866\n",
      "[12100]\ttraining's l1: 0.170546\tvalid_1's l1: 0.257525\n",
      "[12200]\ttraining's l1: 0.169822\tvalid_1's l1: 0.257181\n",
      "[12300]\ttraining's l1: 0.16908\tvalid_1's l1: 0.256812\n",
      "[12400]\ttraining's l1: 0.168382\tvalid_1's l1: 0.256484\n",
      "[12500]\ttraining's l1: 0.167659\tvalid_1's l1: 0.256143\n",
      "[12600]\ttraining's l1: 0.166957\tvalid_1's l1: 0.255794\n",
      "[12700]\ttraining's l1: 0.16628\tvalid_1's l1: 0.255467\n",
      "[12800]\ttraining's l1: 0.165595\tvalid_1's l1: 0.255172\n",
      "[12900]\ttraining's l1: 0.16488\tvalid_1's l1: 0.254812\n",
      "[13000]\ttraining's l1: 0.164222\tvalid_1's l1: 0.254491\n",
      "[13100]\ttraining's l1: 0.16357\tvalid_1's l1: 0.254175\n",
      "[13200]\ttraining's l1: 0.162885\tvalid_1's l1: 0.253866\n",
      "[13300]\ttraining's l1: 0.162255\tvalid_1's l1: 0.253591\n",
      "[13400]\ttraining's l1: 0.161618\tvalid_1's l1: 0.25329\n",
      "[13500]\ttraining's l1: 0.160982\tvalid_1's l1: 0.252987\n",
      "[13600]\ttraining's l1: 0.160362\tvalid_1's l1: 0.252708\n",
      "[13700]\ttraining's l1: 0.159749\tvalid_1's l1: 0.252435\n",
      "[13800]\ttraining's l1: 0.159119\tvalid_1's l1: 0.252136\n",
      "[13900]\ttraining's l1: 0.158528\tvalid_1's l1: 0.251883\n",
      "[14000]\ttraining's l1: 0.157914\tvalid_1's l1: 0.25161\n",
      "[14100]\ttraining's l1: 0.157295\tvalid_1's l1: 0.251324\n",
      "[14200]\ttraining's l1: 0.156683\tvalid_1's l1: 0.251028\n",
      "[14300]\ttraining's l1: 0.156077\tvalid_1's l1: 0.250742\n",
      "[14400]\ttraining's l1: 0.155478\tvalid_1's l1: 0.250487\n",
      "[14500]\ttraining's l1: 0.154862\tvalid_1's l1: 0.250234\n",
      "[14600]\ttraining's l1: 0.154284\tvalid_1's l1: 0.250003\n",
      "[14700]\ttraining's l1: 0.15371\tvalid_1's l1: 0.249743\n",
      "[14800]\ttraining's l1: 0.153123\tvalid_1's l1: 0.249483\n",
      "[14900]\ttraining's l1: 0.152539\tvalid_1's l1: 0.249224\n",
      "[15000]\ttraining's l1: 0.151967\tvalid_1's l1: 0.248961\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.151967\tvalid_1's l1: 0.248961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2JHC Fold 1, logMAE: -1.390459768074527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.724327\tvalid_1's l1: 0.731958\n",
      "[200]\ttraining's l1: 0.615101\tvalid_1's l1: 0.624662\n",
      "[300]\ttraining's l1: 0.558112\tvalid_1's l1: 0.569308\n",
      "[400]\ttraining's l1: 0.51969\tvalid_1's l1: 0.532343\n",
      "[500]\ttraining's l1: 0.490878\tvalid_1's l1: 0.505137\n",
      "[600]\ttraining's l1: 0.468527\tvalid_1's l1: 0.484189\n",
      "[700]\ttraining's l1: 0.450229\tvalid_1's l1: 0.46734\n",
      "[800]\ttraining's l1: 0.434955\tvalid_1's l1: 0.453285\n",
      "[900]\ttraining's l1: 0.42191\tvalid_1's l1: 0.441622\n",
      "[1000]\ttraining's l1: 0.410027\tvalid_1's l1: 0.43106\n",
      "[1100]\ttraining's l1: 0.399281\tvalid_1's l1: 0.421596\n",
      "[1200]\ttraining's l1: 0.39023\tvalid_1's l1: 0.413795\n",
      "[1300]\ttraining's l1: 0.381592\tvalid_1's l1: 0.406355\n",
      "[1400]\ttraining's l1: 0.373775\tvalid_1's l1: 0.39983\n",
      "[1500]\ttraining's l1: 0.366988\tvalid_1's l1: 0.394196\n",
      "[1600]\ttraining's l1: 0.360232\tvalid_1's l1: 0.388527\n",
      "[1700]\ttraining's l1: 0.354525\tvalid_1's l1: 0.383868\n",
      "[1800]\ttraining's l1: 0.348793\tvalid_1's l1: 0.379273\n",
      "[1900]\ttraining's l1: 0.343372\tvalid_1's l1: 0.37487\n",
      "[2000]\ttraining's l1: 0.338077\tvalid_1's l1: 0.370507\n",
      "[2100]\ttraining's l1: 0.333222\tvalid_1's l1: 0.366652\n",
      "[2200]\ttraining's l1: 0.328663\tvalid_1's l1: 0.363087\n",
      "[2300]\ttraining's l1: 0.324038\tvalid_1's l1: 0.35939\n",
      "[2400]\ttraining's l1: 0.319778\tvalid_1's l1: 0.356105\n",
      "[2500]\ttraining's l1: 0.315993\tvalid_1's l1: 0.353273\n",
      "[2600]\ttraining's l1: 0.312138\tvalid_1's l1: 0.350364\n",
      "[2700]\ttraining's l1: 0.308398\tvalid_1's l1: 0.34758\n",
      "[2800]\ttraining's l1: 0.304945\tvalid_1's l1: 0.34493\n",
      "[2900]\ttraining's l1: 0.301507\tvalid_1's l1: 0.342336\n",
      "[3000]\ttraining's l1: 0.298147\tvalid_1's l1: 0.339894\n",
      "[3100]\ttraining's l1: 0.295078\tvalid_1's l1: 0.337643\n",
      "[3200]\ttraining's l1: 0.292041\tvalid_1's l1: 0.335464\n",
      "[3300]\ttraining's l1: 0.289131\tvalid_1's l1: 0.333459\n",
      "[3400]\ttraining's l1: 0.286318\tvalid_1's l1: 0.331457\n",
      "[3500]\ttraining's l1: 0.28367\tvalid_1's l1: 0.329581\n",
      "[3600]\ttraining's l1: 0.280951\tvalid_1's l1: 0.327622\n",
      "[3700]\ttraining's l1: 0.278275\tvalid_1's l1: 0.325724\n",
      "[3800]\ttraining's l1: 0.275721\tvalid_1's l1: 0.323928\n",
      "[3900]\ttraining's l1: 0.273164\tvalid_1's l1: 0.322093\n",
      "[4000]\ttraining's l1: 0.270718\tvalid_1's l1: 0.320412\n",
      "[4100]\ttraining's l1: 0.26839\tvalid_1's l1: 0.318788\n",
      "[4200]\ttraining's l1: 0.266132\tvalid_1's l1: 0.31723\n",
      "[4300]\ttraining's l1: 0.263862\tvalid_1's l1: 0.315716\n",
      "[4400]\ttraining's l1: 0.261753\tvalid_1's l1: 0.314342\n",
      "[4500]\ttraining's l1: 0.259732\tvalid_1's l1: 0.313041\n",
      "[4600]\ttraining's l1: 0.257697\tvalid_1's l1: 0.311665\n",
      "[4700]\ttraining's l1: 0.255741\tvalid_1's l1: 0.310375\n",
      "[4800]\ttraining's l1: 0.253835\tvalid_1's l1: 0.309098\n",
      "[4900]\ttraining's l1: 0.251924\tvalid_1's l1: 0.307865\n",
      "[5000]\ttraining's l1: 0.250021\tvalid_1's l1: 0.306581\n",
      "[5100]\ttraining's l1: 0.248075\tvalid_1's l1: 0.305272\n",
      "[5200]\ttraining's l1: 0.246151\tvalid_1's l1: 0.304\n",
      "[5300]\ttraining's l1: 0.244335\tvalid_1's l1: 0.30282\n",
      "[5400]\ttraining's l1: 0.242637\tvalid_1's l1: 0.301733\n",
      "[5500]\ttraining's l1: 0.240944\tvalid_1's l1: 0.300692\n",
      "[5600]\ttraining's l1: 0.239239\tvalid_1's l1: 0.299576\n",
      "[5700]\ttraining's l1: 0.237603\tvalid_1's l1: 0.298516\n",
      "[5800]\ttraining's l1: 0.23593\tvalid_1's l1: 0.297445\n",
      "[5900]\ttraining's l1: 0.234385\tvalid_1's l1: 0.296488\n",
      "[6000]\ttraining's l1: 0.232778\tvalid_1's l1: 0.29553\n",
      "[6100]\ttraining's l1: 0.231289\tvalid_1's l1: 0.294624\n",
      "[6200]\ttraining's l1: 0.229796\tvalid_1's l1: 0.293766\n",
      "[6300]\ttraining's l1: 0.228346\tvalid_1's l1: 0.292908\n",
      "[6400]\ttraining's l1: 0.226927\tvalid_1's l1: 0.292059\n",
      "[6500]\ttraining's l1: 0.225525\tvalid_1's l1: 0.291219\n",
      "[6600]\ttraining's l1: 0.224176\tvalid_1's l1: 0.290428\n",
      "[6700]\ttraining's l1: 0.22284\tvalid_1's l1: 0.289637\n",
      "[6800]\ttraining's l1: 0.22146\tvalid_1's l1: 0.288812\n",
      "[6900]\ttraining's l1: 0.220114\tvalid_1's l1: 0.287999\n",
      "[7000]\ttraining's l1: 0.218829\tvalid_1's l1: 0.287237\n",
      "[7100]\ttraining's l1: 0.217529\tvalid_1's l1: 0.286495\n",
      "[7200]\ttraining's l1: 0.216261\tvalid_1's l1: 0.285751\n",
      "[7300]\ttraining's l1: 0.215031\tvalid_1's l1: 0.285013\n",
      "[7400]\ttraining's l1: 0.213771\tvalid_1's l1: 0.28431\n",
      "[7500]\ttraining's l1: 0.212562\tvalid_1's l1: 0.283615\n",
      "[7600]\ttraining's l1: 0.211324\tvalid_1's l1: 0.282854\n",
      "[7700]\ttraining's l1: 0.210121\tvalid_1's l1: 0.282143\n",
      "[7800]\ttraining's l1: 0.208947\tvalid_1's l1: 0.281491\n",
      "[7900]\ttraining's l1: 0.207764\tvalid_1's l1: 0.280797\n",
      "[8000]\ttraining's l1: 0.20658\tvalid_1's l1: 0.280107\n",
      "[8100]\ttraining's l1: 0.205481\tvalid_1's l1: 0.279489\n",
      "[8200]\ttraining's l1: 0.204407\tvalid_1's l1: 0.278911\n",
      "[8300]\ttraining's l1: 0.203277\tvalid_1's l1: 0.278245\n",
      "[8400]\ttraining's l1: 0.202179\tvalid_1's l1: 0.277616\n",
      "[8500]\ttraining's l1: 0.201122\tvalid_1's l1: 0.277046\n",
      "[8600]\ttraining's l1: 0.200109\tvalid_1's l1: 0.276501\n",
      "[8700]\ttraining's l1: 0.199095\tvalid_1's l1: 0.275978\n",
      "[8800]\ttraining's l1: 0.198097\tvalid_1's l1: 0.27541\n",
      "[8900]\ttraining's l1: 0.197141\tvalid_1's l1: 0.274915\n",
      "[9000]\ttraining's l1: 0.196111\tvalid_1's l1: 0.274363\n",
      "[9100]\ttraining's l1: 0.19509\tvalid_1's l1: 0.273801\n",
      "[9200]\ttraining's l1: 0.194107\tvalid_1's l1: 0.273281\n",
      "[9300]\ttraining's l1: 0.193106\tvalid_1's l1: 0.272719\n",
      "[9400]\ttraining's l1: 0.192151\tvalid_1's l1: 0.272227\n",
      "[9500]\ttraining's l1: 0.191226\tvalid_1's l1: 0.271732\n",
      "[9600]\ttraining's l1: 0.190327\tvalid_1's l1: 0.271271\n",
      "[9700]\ttraining's l1: 0.189416\tvalid_1's l1: 0.270789\n",
      "[9800]\ttraining's l1: 0.1885\tvalid_1's l1: 0.270318\n",
      "[9900]\ttraining's l1: 0.187581\tvalid_1's l1: 0.269827\n",
      "[10000]\ttraining's l1: 0.186707\tvalid_1's l1: 0.269387\n",
      "[10100]\ttraining's l1: 0.185822\tvalid_1's l1: 0.268935\n",
      "[10200]\ttraining's l1: 0.184917\tvalid_1's l1: 0.268448\n",
      "[10300]\ttraining's l1: 0.184021\tvalid_1's l1: 0.267992\n",
      "[10400]\ttraining's l1: 0.183157\tvalid_1's l1: 0.267559\n",
      "[10500]\ttraining's l1: 0.182326\tvalid_1's l1: 0.267175\n",
      "[10600]\ttraining's l1: 0.181496\tvalid_1's l1: 0.266768\n",
      "[10700]\ttraining's l1: 0.180659\tvalid_1's l1: 0.266367\n",
      "[10800]\ttraining's l1: 0.179868\tvalid_1's l1: 0.266004\n",
      "[10900]\ttraining's l1: 0.179037\tvalid_1's l1: 0.265585\n",
      "[11000]\ttraining's l1: 0.178242\tvalid_1's l1: 0.265189\n",
      "[11100]\ttraining's l1: 0.177438\tvalid_1's l1: 0.264786\n",
      "[11200]\ttraining's l1: 0.176658\tvalid_1's l1: 0.26439\n",
      "[11300]\ttraining's l1: 0.175874\tvalid_1's l1: 0.263992\n",
      "[11400]\ttraining's l1: 0.175105\tvalid_1's l1: 0.263609\n",
      "[11500]\ttraining's l1: 0.174346\tvalid_1's l1: 0.263226\n",
      "[11600]\ttraining's l1: 0.173592\tvalid_1's l1: 0.262875\n",
      "[11700]\ttraining's l1: 0.172845\tvalid_1's l1: 0.262505\n",
      "[11800]\ttraining's l1: 0.172117\tvalid_1's l1: 0.262173\n",
      "[11900]\ttraining's l1: 0.171398\tvalid_1's l1: 0.261822\n",
      "[12000]\ttraining's l1: 0.170656\tvalid_1's l1: 0.261468\n",
      "[12100]\ttraining's l1: 0.169924\tvalid_1's l1: 0.261108\n",
      "[12200]\ttraining's l1: 0.169198\tvalid_1's l1: 0.260769\n",
      "[12300]\ttraining's l1: 0.168493\tvalid_1's l1: 0.260451\n",
      "[12400]\ttraining's l1: 0.167754\tvalid_1's l1: 0.26009\n",
      "[12500]\ttraining's l1: 0.167063\tvalid_1's l1: 0.25975\n",
      "[12600]\ttraining's l1: 0.166383\tvalid_1's l1: 0.259438\n",
      "[12700]\ttraining's l1: 0.16571\tvalid_1's l1: 0.259136\n",
      "[12800]\ttraining's l1: 0.165005\tvalid_1's l1: 0.258809\n",
      "[12900]\ttraining's l1: 0.164335\tvalid_1's l1: 0.258483\n",
      "[13000]\ttraining's l1: 0.163671\tvalid_1's l1: 0.258199\n",
      "[13100]\ttraining's l1: 0.163011\tvalid_1's l1: 0.257899\n",
      "[13200]\ttraining's l1: 0.162329\tvalid_1's l1: 0.257559\n",
      "[13300]\ttraining's l1: 0.16166\tvalid_1's l1: 0.25726\n",
      "[13400]\ttraining's l1: 0.161042\tvalid_1's l1: 0.256968\n",
      "[13500]\ttraining's l1: 0.160374\tvalid_1's l1: 0.256659\n",
      "[13600]\ttraining's l1: 0.159747\tvalid_1's l1: 0.256365\n",
      "[13700]\ttraining's l1: 0.159097\tvalid_1's l1: 0.256063\n",
      "[13800]\ttraining's l1: 0.158484\tvalid_1's l1: 0.255802\n",
      "[13900]\ttraining's l1: 0.157863\tvalid_1's l1: 0.255533\n",
      "[14000]\ttraining's l1: 0.157253\tvalid_1's l1: 0.255253\n",
      "[14100]\ttraining's l1: 0.156626\tvalid_1's l1: 0.254976\n",
      "[14200]\ttraining's l1: 0.156017\tvalid_1's l1: 0.254727\n",
      "[14300]\ttraining's l1: 0.155397\tvalid_1's l1: 0.254418\n",
      "[14400]\ttraining's l1: 0.154798\tvalid_1's l1: 0.254151\n",
      "[14500]\ttraining's l1: 0.154214\tvalid_1's l1: 0.253878\n",
      "[14600]\ttraining's l1: 0.153632\tvalid_1's l1: 0.253618\n",
      "[14700]\ttraining's l1: 0.153064\tvalid_1's l1: 0.253363\n",
      "[14800]\ttraining's l1: 0.152494\tvalid_1's l1: 0.253113\n",
      "[14900]\ttraining's l1: 0.151917\tvalid_1's l1: 0.252858\n",
      "[15000]\ttraining's l1: 0.151337\tvalid_1's l1: 0.252607\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.151337\tvalid_1's l1: 0.252607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2JHC Fold 2, logMAE: -1.3759209369904162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.721911\tvalid_1's l1: 0.72411\n",
      "[200]\ttraining's l1: 0.609787\tvalid_1's l1: 0.614\n",
      "[300]\ttraining's l1: 0.552723\tvalid_1's l1: 0.559245\n",
      "[400]\ttraining's l1: 0.515558\tvalid_1's l1: 0.524288\n",
      "[500]\ttraining's l1: 0.486711\tvalid_1's l1: 0.497482\n",
      "[600]\ttraining's l1: 0.464392\tvalid_1's l1: 0.476766\n",
      "[700]\ttraining's l1: 0.447348\tvalid_1's l1: 0.461425\n",
      "[800]\ttraining's l1: 0.432549\tvalid_1's l1: 0.447847\n",
      "[900]\ttraining's l1: 0.419058\tvalid_1's l1: 0.435811\n",
      "[1000]\ttraining's l1: 0.408308\tvalid_1's l1: 0.426418\n",
      "[1100]\ttraining's l1: 0.397804\tvalid_1's l1: 0.417209\n",
      "[1200]\ttraining's l1: 0.388846\tvalid_1's l1: 0.409338\n",
      "[1300]\ttraining's l1: 0.380631\tvalid_1's l1: 0.402397\n",
      "[1400]\ttraining's l1: 0.373156\tvalid_1's l1: 0.396188\n",
      "[1500]\ttraining's l1: 0.366033\tvalid_1's l1: 0.390165\n",
      "[1600]\ttraining's l1: 0.359244\tvalid_1's l1: 0.384387\n",
      "[1700]\ttraining's l1: 0.352851\tvalid_1's l1: 0.379051\n",
      "[1800]\ttraining's l1: 0.347232\tvalid_1's l1: 0.374467\n",
      "[1900]\ttraining's l1: 0.341984\tvalid_1's l1: 0.370275\n",
      "[2000]\ttraining's l1: 0.336997\tvalid_1's l1: 0.366246\n",
      "[2100]\ttraining's l1: 0.332024\tvalid_1's l1: 0.362235\n",
      "[2200]\ttraining's l1: 0.327438\tvalid_1's l1: 0.35857\n",
      "[2300]\ttraining's l1: 0.32301\tvalid_1's l1: 0.354973\n",
      "[2400]\ttraining's l1: 0.318907\tvalid_1's l1: 0.351781\n",
      "[2500]\ttraining's l1: 0.314869\tvalid_1's l1: 0.348643\n",
      "[2600]\ttraining's l1: 0.310863\tvalid_1's l1: 0.345594\n",
      "[2700]\ttraining's l1: 0.307139\tvalid_1's l1: 0.342721\n",
      "[2800]\ttraining's l1: 0.30357\tvalid_1's l1: 0.339999\n",
      "[2900]\ttraining's l1: 0.300266\tvalid_1's l1: 0.337575\n",
      "[3000]\ttraining's l1: 0.297201\tvalid_1's l1: 0.335385\n",
      "[3100]\ttraining's l1: 0.294029\tvalid_1's l1: 0.333046\n",
      "[3200]\ttraining's l1: 0.290996\tvalid_1's l1: 0.330811\n",
      "[3300]\ttraining's l1: 0.288032\tvalid_1's l1: 0.328611\n",
      "[3400]\ttraining's l1: 0.285199\tvalid_1's l1: 0.326538\n",
      "[3500]\ttraining's l1: 0.282518\tvalid_1's l1: 0.32461\n",
      "[3600]\ttraining's l1: 0.27978\tvalid_1's l1: 0.322638\n",
      "[3700]\ttraining's l1: 0.277202\tvalid_1's l1: 0.320838\n",
      "[3800]\ttraining's l1: 0.274696\tvalid_1's l1: 0.319084\n",
      "[3900]\ttraining's l1: 0.272174\tvalid_1's l1: 0.317265\n",
      "[4000]\ttraining's l1: 0.269742\tvalid_1's l1: 0.315581\n",
      "[4100]\ttraining's l1: 0.267447\tvalid_1's l1: 0.314018\n",
      "[4200]\ttraining's l1: 0.265268\tvalid_1's l1: 0.312539\n",
      "[4300]\ttraining's l1: 0.262966\tvalid_1's l1: 0.310937\n",
      "[4400]\ttraining's l1: 0.260856\tvalid_1's l1: 0.309572\n",
      "[4500]\ttraining's l1: 0.258654\tvalid_1's l1: 0.308067\n",
      "[4600]\ttraining's l1: 0.256701\tvalid_1's l1: 0.306805\n",
      "[4700]\ttraining's l1: 0.254669\tvalid_1's l1: 0.305426\n",
      "[4800]\ttraining's l1: 0.252684\tvalid_1's l1: 0.304118\n",
      "[4900]\ttraining's l1: 0.250811\tvalid_1's l1: 0.302885\n",
      "[5000]\ttraining's l1: 0.248921\tvalid_1's l1: 0.301652\n",
      "[5100]\ttraining's l1: 0.247\tvalid_1's l1: 0.300356\n",
      "[5200]\ttraining's l1: 0.245214\tvalid_1's l1: 0.29921\n",
      "[5300]\ttraining's l1: 0.243427\tvalid_1's l1: 0.298064\n",
      "[5400]\ttraining's l1: 0.241677\tvalid_1's l1: 0.296916\n",
      "[5500]\ttraining's l1: 0.240022\tvalid_1's l1: 0.295895\n",
      "[5600]\ttraining's l1: 0.238358\tvalid_1's l1: 0.294848\n",
      "[5700]\ttraining's l1: 0.236693\tvalid_1's l1: 0.29377\n",
      "[5800]\ttraining's l1: 0.235033\tvalid_1's l1: 0.292683\n",
      "[5900]\ttraining's l1: 0.233472\tvalid_1's l1: 0.291693\n",
      "[6000]\ttraining's l1: 0.231884\tvalid_1's l1: 0.290698\n",
      "[6100]\ttraining's l1: 0.230415\tvalid_1's l1: 0.289816\n",
      "[6200]\ttraining's l1: 0.228975\tvalid_1's l1: 0.288941\n",
      "[6300]\ttraining's l1: 0.227486\tvalid_1's l1: 0.288023\n",
      "[6400]\ttraining's l1: 0.226037\tvalid_1's l1: 0.287144\n",
      "[6500]\ttraining's l1: 0.22465\tvalid_1's l1: 0.286328\n",
      "[6600]\ttraining's l1: 0.223209\tvalid_1's l1: 0.285414\n",
      "[6700]\ttraining's l1: 0.221818\tvalid_1's l1: 0.284589\n",
      "[6800]\ttraining's l1: 0.220514\tvalid_1's l1: 0.283832\n",
      "[6900]\ttraining's l1: 0.219182\tvalid_1's l1: 0.283037\n",
      "[7000]\ttraining's l1: 0.217875\tvalid_1's l1: 0.282281\n",
      "[7100]\ttraining's l1: 0.216618\tvalid_1's l1: 0.281596\n",
      "[7200]\ttraining's l1: 0.215355\tvalid_1's l1: 0.280885\n",
      "[7300]\ttraining's l1: 0.214072\tvalid_1's l1: 0.280124\n",
      "[7400]\ttraining's l1: 0.212809\tvalid_1's l1: 0.279388\n",
      "[7500]\ttraining's l1: 0.21161\tvalid_1's l1: 0.278704\n",
      "[7600]\ttraining's l1: 0.210407\tvalid_1's l1: 0.278014\n",
      "[7700]\ttraining's l1: 0.209278\tvalid_1's l1: 0.277408\n",
      "[7800]\ttraining's l1: 0.208132\tvalid_1's l1: 0.276765\n",
      "[7900]\ttraining's l1: 0.206979\tvalid_1's l1: 0.276081\n",
      "[8000]\ttraining's l1: 0.205862\tvalid_1's l1: 0.275459\n",
      "[8100]\ttraining's l1: 0.204763\tvalid_1's l1: 0.274871\n",
      "[8200]\ttraining's l1: 0.203666\tvalid_1's l1: 0.274265\n",
      "[8300]\ttraining's l1: 0.202624\tvalid_1's l1: 0.273683\n",
      "[8400]\ttraining's l1: 0.201534\tvalid_1's l1: 0.273068\n",
      "[8500]\ttraining's l1: 0.200494\tvalid_1's l1: 0.272532\n",
      "[8600]\ttraining's l1: 0.199428\tvalid_1's l1: 0.271952\n",
      "[8700]\ttraining's l1: 0.198352\tvalid_1's l1: 0.271378\n",
      "[8800]\ttraining's l1: 0.197303\tvalid_1's l1: 0.270766\n",
      "[8900]\ttraining's l1: 0.196317\tvalid_1's l1: 0.270203\n",
      "[9000]\ttraining's l1: 0.195295\tvalid_1's l1: 0.269642\n",
      "[9100]\ttraining's l1: 0.19433\tvalid_1's l1: 0.269138\n",
      "[9200]\ttraining's l1: 0.193341\tvalid_1's l1: 0.268611\n",
      "[9300]\ttraining's l1: 0.192378\tvalid_1's l1: 0.2681\n",
      "[9400]\ttraining's l1: 0.191404\tvalid_1's l1: 0.26759\n",
      "[9500]\ttraining's l1: 0.190473\tvalid_1's l1: 0.267094\n",
      "[9600]\ttraining's l1: 0.189577\tvalid_1's l1: 0.266627\n",
      "[9700]\ttraining's l1: 0.188679\tvalid_1's l1: 0.266173\n",
      "[9800]\ttraining's l1: 0.187787\tvalid_1's l1: 0.265667\n",
      "[9900]\ttraining's l1: 0.186929\tvalid_1's l1: 0.265237\n",
      "[10000]\ttraining's l1: 0.18604\tvalid_1's l1: 0.264802\n",
      "[10100]\ttraining's l1: 0.18514\tvalid_1's l1: 0.264309\n",
      "[10200]\ttraining's l1: 0.184279\tvalid_1's l1: 0.263876\n",
      "[10300]\ttraining's l1: 0.183457\tvalid_1's l1: 0.263436\n",
      "[10400]\ttraining's l1: 0.182618\tvalid_1's l1: 0.263015\n",
      "[10500]\ttraining's l1: 0.181769\tvalid_1's l1: 0.26258\n",
      "[10600]\ttraining's l1: 0.180953\tvalid_1's l1: 0.262216\n",
      "[10700]\ttraining's l1: 0.180166\tvalid_1's l1: 0.261816\n",
      "[10800]\ttraining's l1: 0.179369\tvalid_1's l1: 0.261425\n",
      "[10900]\ttraining's l1: 0.17856\tvalid_1's l1: 0.261036\n",
      "[11000]\ttraining's l1: 0.177782\tvalid_1's l1: 0.260672\n",
      "[11100]\ttraining's l1: 0.176996\tvalid_1's l1: 0.260274\n",
      "[11200]\ttraining's l1: 0.176222\tvalid_1's l1: 0.25989\n",
      "[11300]\ttraining's l1: 0.175433\tvalid_1's l1: 0.259506\n",
      "[11400]\ttraining's l1: 0.174683\tvalid_1's l1: 0.259129\n",
      "[11500]\ttraining's l1: 0.17388\tvalid_1's l1: 0.258702\n",
      "[11600]\ttraining's l1: 0.173112\tvalid_1's l1: 0.258311\n",
      "[11700]\ttraining's l1: 0.172366\tvalid_1's l1: 0.257964\n",
      "[11800]\ttraining's l1: 0.171637\tvalid_1's l1: 0.257616\n",
      "[11900]\ttraining's l1: 0.170919\tvalid_1's l1: 0.257267\n",
      "[12000]\ttraining's l1: 0.170199\tvalid_1's l1: 0.256918\n",
      "[12100]\ttraining's l1: 0.169465\tvalid_1's l1: 0.256551\n",
      "[12200]\ttraining's l1: 0.16877\tvalid_1's l1: 0.256248\n",
      "[12300]\ttraining's l1: 0.168035\tvalid_1's l1: 0.255892\n",
      "[12400]\ttraining's l1: 0.167343\tvalid_1's l1: 0.255572\n",
      "[12500]\ttraining's l1: 0.166632\tvalid_1's l1: 0.255247\n",
      "[12600]\ttraining's l1: 0.165931\tvalid_1's l1: 0.254903\n",
      "[12700]\ttraining's l1: 0.165249\tvalid_1's l1: 0.254558\n",
      "[12800]\ttraining's l1: 0.164569\tvalid_1's l1: 0.254228\n",
      "[12900]\ttraining's l1: 0.163892\tvalid_1's l1: 0.253903\n",
      "[13000]\ttraining's l1: 0.163232\tvalid_1's l1: 0.253622\n",
      "[13100]\ttraining's l1: 0.162571\tvalid_1's l1: 0.253314\n",
      "[13200]\ttraining's l1: 0.161897\tvalid_1's l1: 0.252992\n",
      "[13300]\ttraining's l1: 0.161238\tvalid_1's l1: 0.252696\n",
      "[13400]\ttraining's l1: 0.160595\tvalid_1's l1: 0.252397\n",
      "[13500]\ttraining's l1: 0.159986\tvalid_1's l1: 0.252138\n",
      "[13600]\ttraining's l1: 0.159366\tvalid_1's l1: 0.251851\n",
      "[13700]\ttraining's l1: 0.158754\tvalid_1's l1: 0.25158\n",
      "[13800]\ttraining's l1: 0.158128\tvalid_1's l1: 0.251308\n",
      "[13900]\ttraining's l1: 0.157494\tvalid_1's l1: 0.25102\n",
      "[14000]\ttraining's l1: 0.1569\tvalid_1's l1: 0.250743\n",
      "[14100]\ttraining's l1: 0.15631\tvalid_1's l1: 0.250474\n",
      "[14200]\ttraining's l1: 0.155698\tvalid_1's l1: 0.250181\n",
      "[14300]\ttraining's l1: 0.155114\tvalid_1's l1: 0.249926\n",
      "[14400]\ttraining's l1: 0.154514\tvalid_1's l1: 0.249648\n",
      "[14500]\ttraining's l1: 0.153939\tvalid_1's l1: 0.249406\n",
      "[14600]\ttraining's l1: 0.153367\tvalid_1's l1: 0.249161\n",
      "[14700]\ttraining's l1: 0.15276\tvalid_1's l1: 0.248872\n",
      "[14800]\ttraining's l1: 0.152166\tvalid_1's l1: 0.248627\n",
      "[14900]\ttraining's l1: 0.151565\tvalid_1's l1: 0.248323\n",
      "[15000]\ttraining's l1: 0.151013\tvalid_1's l1: 0.24808\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.151013\tvalid_1's l1: 0.24808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2JHC Fold 3, logMAE: -1.3940046938679638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.723214\tvalid_1's l1: 0.727708\n",
      "[200]\ttraining's l1: 0.612751\tvalid_1's l1: 0.619351\n",
      "[300]\ttraining's l1: 0.55654\tvalid_1's l1: 0.564838\n",
      "[400]\ttraining's l1: 0.517202\tvalid_1's l1: 0.527238\n",
      "[500]\ttraining's l1: 0.490259\tvalid_1's l1: 0.5021\n",
      "[600]\ttraining's l1: 0.467402\tvalid_1's l1: 0.480635\n",
      "[700]\ttraining's l1: 0.450156\tvalid_1's l1: 0.464849\n",
      "[800]\ttraining's l1: 0.435136\tvalid_1's l1: 0.451409\n",
      "[900]\ttraining's l1: 0.422258\tvalid_1's l1: 0.439914\n",
      "[1000]\ttraining's l1: 0.410668\tvalid_1's l1: 0.429574\n",
      "[1100]\ttraining's l1: 0.400377\tvalid_1's l1: 0.420626\n",
      "[1200]\ttraining's l1: 0.391317\tvalid_1's l1: 0.412849\n",
      "[1300]\ttraining's l1: 0.382739\tvalid_1's l1: 0.405576\n",
      "[1400]\ttraining's l1: 0.374464\tvalid_1's l1: 0.398502\n",
      "[1500]\ttraining's l1: 0.367529\tvalid_1's l1: 0.392626\n",
      "[1600]\ttraining's l1: 0.360982\tvalid_1's l1: 0.387204\n",
      "[1700]\ttraining's l1: 0.354865\tvalid_1's l1: 0.3822\n",
      "[1800]\ttraining's l1: 0.348885\tvalid_1's l1: 0.377286\n",
      "[1900]\ttraining's l1: 0.343456\tvalid_1's l1: 0.372839\n",
      "[2000]\ttraining's l1: 0.338322\tvalid_1's l1: 0.368782\n",
      "[2100]\ttraining's l1: 0.333307\tvalid_1's l1: 0.364774\n",
      "[2200]\ttraining's l1: 0.328508\tvalid_1's l1: 0.360969\n",
      "[2300]\ttraining's l1: 0.324066\tvalid_1's l1: 0.357437\n",
      "[2400]\ttraining's l1: 0.319981\tvalid_1's l1: 0.354337\n",
      "[2500]\ttraining's l1: 0.315672\tvalid_1's l1: 0.350936\n",
      "[2600]\ttraining's l1: 0.311789\tvalid_1's l1: 0.347962\n",
      "[2700]\ttraining's l1: 0.308065\tvalid_1's l1: 0.34508\n",
      "[2800]\ttraining's l1: 0.304491\tvalid_1's l1: 0.342394\n",
      "[2900]\ttraining's l1: 0.300961\tvalid_1's l1: 0.339729\n",
      "[3000]\ttraining's l1: 0.297798\tvalid_1's l1: 0.337446\n",
      "[3100]\ttraining's l1: 0.294665\tvalid_1's l1: 0.335085\n",
      "[3200]\ttraining's l1: 0.291551\tvalid_1's l1: 0.332768\n",
      "[3300]\ttraining's l1: 0.288617\tvalid_1's l1: 0.330649\n",
      "[3400]\ttraining's l1: 0.285623\tvalid_1's l1: 0.328467\n",
      "[3500]\ttraining's l1: 0.282817\tvalid_1's l1: 0.326481\n",
      "[3600]\ttraining's l1: 0.280257\tvalid_1's l1: 0.324683\n",
      "[3700]\ttraining's l1: 0.277634\tvalid_1's l1: 0.322778\n",
      "[3800]\ttraining's l1: 0.275138\tvalid_1's l1: 0.321047\n",
      "[3900]\ttraining's l1: 0.272738\tvalid_1's l1: 0.319435\n",
      "[4000]\ttraining's l1: 0.27041\tvalid_1's l1: 0.317825\n",
      "[4100]\ttraining's l1: 0.268015\tvalid_1's l1: 0.31619\n",
      "[4200]\ttraining's l1: 0.265609\tvalid_1's l1: 0.314537\n",
      "[4300]\ttraining's l1: 0.263561\tvalid_1's l1: 0.31318\n",
      "[4400]\ttraining's l1: 0.261391\tvalid_1's l1: 0.311707\n",
      "[4500]\ttraining's l1: 0.259305\tvalid_1's l1: 0.310295\n",
      "[4600]\ttraining's l1: 0.257294\tvalid_1's l1: 0.308968\n",
      "[4700]\ttraining's l1: 0.255358\tvalid_1's l1: 0.307734\n",
      "[4800]\ttraining's l1: 0.253464\tvalid_1's l1: 0.306523\n",
      "[4900]\ttraining's l1: 0.251557\tvalid_1's l1: 0.305277\n",
      "[5000]\ttraining's l1: 0.249619\tvalid_1's l1: 0.303972\n",
      "[5100]\ttraining's l1: 0.247823\tvalid_1's l1: 0.302823\n",
      "[5200]\ttraining's l1: 0.246044\tvalid_1's l1: 0.301689\n",
      "[5300]\ttraining's l1: 0.244307\tvalid_1's l1: 0.300578\n",
      "[5400]\ttraining's l1: 0.242577\tvalid_1's l1: 0.299462\n",
      "[5500]\ttraining's l1: 0.240862\tvalid_1's l1: 0.298357\n",
      "[5600]\ttraining's l1: 0.239186\tvalid_1's l1: 0.297274\n",
      "[5700]\ttraining's l1: 0.237524\tvalid_1's l1: 0.296202\n",
      "[5800]\ttraining's l1: 0.235882\tvalid_1's l1: 0.295233\n",
      "[5900]\ttraining's l1: 0.234318\tvalid_1's l1: 0.294255\n",
      "[6000]\ttraining's l1: 0.232682\tvalid_1's l1: 0.293191\n",
      "[6100]\ttraining's l1: 0.231182\tvalid_1's l1: 0.292299\n",
      "[6200]\ttraining's l1: 0.229656\tvalid_1's l1: 0.29134\n",
      "[6300]\ttraining's l1: 0.22816\tvalid_1's l1: 0.290382\n",
      "[6400]\ttraining's l1: 0.226642\tvalid_1's l1: 0.289434\n",
      "[6500]\ttraining's l1: 0.225191\tvalid_1's l1: 0.288546\n",
      "[6600]\ttraining's l1: 0.223827\tvalid_1's l1: 0.28774\n",
      "[6700]\ttraining's l1: 0.222396\tvalid_1's l1: 0.286843\n",
      "[6800]\ttraining's l1: 0.221026\tvalid_1's l1: 0.286034\n",
      "[6900]\ttraining's l1: 0.219737\tvalid_1's l1: 0.285284\n",
      "[7000]\ttraining's l1: 0.218369\tvalid_1's l1: 0.284498\n",
      "[7100]\ttraining's l1: 0.217091\tvalid_1's l1: 0.28377\n",
      "[7200]\ttraining's l1: 0.215859\tvalid_1's l1: 0.283115\n",
      "[7300]\ttraining's l1: 0.214628\tvalid_1's l1: 0.282458\n",
      "[7400]\ttraining's l1: 0.213448\tvalid_1's l1: 0.281791\n",
      "[7500]\ttraining's l1: 0.212244\tvalid_1's l1: 0.28108\n",
      "[7600]\ttraining's l1: 0.211065\tvalid_1's l1: 0.280426\n",
      "[7700]\ttraining's l1: 0.209937\tvalid_1's l1: 0.279837\n",
      "[7800]\ttraining's l1: 0.208714\tvalid_1's l1: 0.27915\n",
      "[7900]\ttraining's l1: 0.207566\tvalid_1's l1: 0.27849\n",
      "[8000]\ttraining's l1: 0.206413\tvalid_1's l1: 0.277797\n",
      "[8100]\ttraining's l1: 0.205263\tvalid_1's l1: 0.277125\n",
      "[8200]\ttraining's l1: 0.204141\tvalid_1's l1: 0.276488\n",
      "[8300]\ttraining's l1: 0.203046\tvalid_1's l1: 0.275897\n",
      "[8400]\ttraining's l1: 0.20195\tvalid_1's l1: 0.275276\n",
      "[8500]\ttraining's l1: 0.200873\tvalid_1's l1: 0.274697\n",
      "[8600]\ttraining's l1: 0.199814\tvalid_1's l1: 0.274144\n",
      "[8700]\ttraining's l1: 0.198768\tvalid_1's l1: 0.273566\n",
      "[8800]\ttraining's l1: 0.197741\tvalid_1's l1: 0.273032\n",
      "[8900]\ttraining's l1: 0.196669\tvalid_1's l1: 0.272431\n",
      "[9000]\ttraining's l1: 0.19565\tvalid_1's l1: 0.271851\n",
      "[9100]\ttraining's l1: 0.194642\tvalid_1's l1: 0.271294\n",
      "[9200]\ttraining's l1: 0.193648\tvalid_1's l1: 0.270752\n",
      "[9300]\ttraining's l1: 0.19269\tvalid_1's l1: 0.270275\n",
      "[9400]\ttraining's l1: 0.191737\tvalid_1's l1: 0.269785\n",
      "[9500]\ttraining's l1: 0.190819\tvalid_1's l1: 0.26928\n",
      "[9600]\ttraining's l1: 0.189874\tvalid_1's l1: 0.268777\n",
      "[9700]\ttraining's l1: 0.188968\tvalid_1's l1: 0.26833\n",
      "[9800]\ttraining's l1: 0.188083\tvalid_1's l1: 0.267898\n",
      "[9900]\ttraining's l1: 0.187164\tvalid_1's l1: 0.267416\n",
      "[10000]\ttraining's l1: 0.186307\tvalid_1's l1: 0.26697\n",
      "[10100]\ttraining's l1: 0.185452\tvalid_1's l1: 0.266526\n",
      "[10200]\ttraining's l1: 0.184572\tvalid_1's l1: 0.266083\n",
      "[10300]\ttraining's l1: 0.183685\tvalid_1's l1: 0.265612\n",
      "[10400]\ttraining's l1: 0.182839\tvalid_1's l1: 0.265212\n",
      "[10500]\ttraining's l1: 0.181999\tvalid_1's l1: 0.264763\n",
      "[10600]\ttraining's l1: 0.181177\tvalid_1's l1: 0.264379\n",
      "[10700]\ttraining's l1: 0.180362\tvalid_1's l1: 0.263962\n",
      "[10800]\ttraining's l1: 0.179545\tvalid_1's l1: 0.263546\n",
      "[10900]\ttraining's l1: 0.178739\tvalid_1's l1: 0.263142\n",
      "[11000]\ttraining's l1: 0.177935\tvalid_1's l1: 0.262747\n",
      "[11100]\ttraining's l1: 0.177176\tvalid_1's l1: 0.26237\n",
      "[11200]\ttraining's l1: 0.176371\tvalid_1's l1: 0.261946\n",
      "[11300]\ttraining's l1: 0.175594\tvalid_1's l1: 0.261571\n",
      "[11400]\ttraining's l1: 0.17482\tvalid_1's l1: 0.261208\n",
      "[11500]\ttraining's l1: 0.174029\tvalid_1's l1: 0.260825\n",
      "[11600]\ttraining's l1: 0.173286\tvalid_1's l1: 0.260462\n",
      "[11700]\ttraining's l1: 0.172538\tvalid_1's l1: 0.260116\n",
      "[11800]\ttraining's l1: 0.171804\tvalid_1's l1: 0.259764\n",
      "[11900]\ttraining's l1: 0.171058\tvalid_1's l1: 0.259395\n",
      "[12000]\ttraining's l1: 0.170304\tvalid_1's l1: 0.259013\n",
      "[12100]\ttraining's l1: 0.169575\tvalid_1's l1: 0.258676\n",
      "[12200]\ttraining's l1: 0.168859\tvalid_1's l1: 0.258344\n",
      "[12300]\ttraining's l1: 0.168152\tvalid_1's l1: 0.258007\n",
      "[12400]\ttraining's l1: 0.167443\tvalid_1's l1: 0.257661\n",
      "[12500]\ttraining's l1: 0.166753\tvalid_1's l1: 0.257369\n",
      "[12600]\ttraining's l1: 0.166049\tvalid_1's l1: 0.257041\n",
      "[12700]\ttraining's l1: 0.165378\tvalid_1's l1: 0.256731\n",
      "[12800]\ttraining's l1: 0.16469\tvalid_1's l1: 0.256408\n",
      "[12900]\ttraining's l1: 0.163991\tvalid_1's l1: 0.256062\n",
      "[13000]\ttraining's l1: 0.163319\tvalid_1's l1: 0.255747\n",
      "[13100]\ttraining's l1: 0.16266\tvalid_1's l1: 0.255447\n",
      "[13200]\ttraining's l1: 0.162005\tvalid_1's l1: 0.255146\n",
      "[13300]\ttraining's l1: 0.161358\tvalid_1's l1: 0.254857\n",
      "[13400]\ttraining's l1: 0.160692\tvalid_1's l1: 0.254561\n",
      "[13500]\ttraining's l1: 0.160046\tvalid_1's l1: 0.254283\n",
      "[13600]\ttraining's l1: 0.159403\tvalid_1's l1: 0.254007\n",
      "[13700]\ttraining's l1: 0.158771\tvalid_1's l1: 0.253728\n",
      "[13800]\ttraining's l1: 0.158159\tvalid_1's l1: 0.25346\n",
      "[13900]\ttraining's l1: 0.157568\tvalid_1's l1: 0.253179\n",
      "[14000]\ttraining's l1: 0.156953\tvalid_1's l1: 0.252922\n",
      "[14100]\ttraining's l1: 0.156334\tvalid_1's l1: 0.252638\n",
      "[14200]\ttraining's l1: 0.155748\tvalid_1's l1: 0.252399\n",
      "[14300]\ttraining's l1: 0.155171\tvalid_1's l1: 0.252144\n",
      "[14400]\ttraining's l1: 0.15458\tvalid_1's l1: 0.251897\n",
      "[14500]\ttraining's l1: 0.15401\tvalid_1's l1: 0.251657\n",
      "[14600]\ttraining's l1: 0.153407\tvalid_1's l1: 0.251385\n",
      "[14700]\ttraining's l1: 0.152854\tvalid_1's l1: 0.251145\n",
      "[14800]\ttraining's l1: 0.152278\tvalid_1's l1: 0.250901\n",
      "[14900]\ttraining's l1: 0.15172\tvalid_1's l1: 0.250645\n",
      "[15000]\ttraining's l1: 0.151163\tvalid_1's l1: 0.250393\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.151163\tvalid_1's l1: 0.250393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2JHC Fold 4, logMAE: -1.3847231070787989\n",
      "*** Training Model for 3JHH ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.375805\tvalid_1's l1: 0.379748\n",
      "[200]\ttraining's l1: 0.317768\tvalid_1's l1: 0.323762\n",
      "[300]\ttraining's l1: 0.288205\tvalid_1's l1: 0.296272\n",
      "[400]\ttraining's l1: 0.268149\tvalid_1's l1: 0.278074\n",
      "[500]\ttraining's l1: 0.25255\tvalid_1's l1: 0.264137\n",
      "[600]\ttraining's l1: 0.240306\tvalid_1's l1: 0.253349\n",
      "[700]\ttraining's l1: 0.230075\tvalid_1's l1: 0.244456\n",
      "[800]\ttraining's l1: 0.221757\tvalid_1's l1: 0.237397\n",
      "[900]\ttraining's l1: 0.213934\tvalid_1's l1: 0.230974\n",
      "[1000]\ttraining's l1: 0.207429\tvalid_1's l1: 0.225697\n",
      "[1100]\ttraining's l1: 0.201795\tvalid_1's l1: 0.221322\n",
      "[1200]\ttraining's l1: 0.196162\tvalid_1's l1: 0.216884\n",
      "[1300]\ttraining's l1: 0.191147\tvalid_1's l1: 0.212921\n",
      "[1400]\ttraining's l1: 0.186738\tvalid_1's l1: 0.209559\n",
      "[1500]\ttraining's l1: 0.18249\tvalid_1's l1: 0.20628\n",
      "[1600]\ttraining's l1: 0.178511\tvalid_1's l1: 0.203251\n",
      "[1700]\ttraining's l1: 0.174917\tvalid_1's l1: 0.20059\n",
      "[1800]\ttraining's l1: 0.17129\tvalid_1's l1: 0.197848\n",
      "[1900]\ttraining's l1: 0.168135\tvalid_1's l1: 0.195592\n",
      "[2000]\ttraining's l1: 0.165044\tvalid_1's l1: 0.193353\n",
      "[2100]\ttraining's l1: 0.16222\tvalid_1's l1: 0.19134\n",
      "[2200]\ttraining's l1: 0.159557\tvalid_1's l1: 0.18949\n",
      "[2300]\ttraining's l1: 0.156825\tvalid_1's l1: 0.187511\n",
      "[2400]\ttraining's l1: 0.154264\tvalid_1's l1: 0.185756\n",
      "[2500]\ttraining's l1: 0.15206\tvalid_1's l1: 0.184341\n",
      "[2600]\ttraining's l1: 0.149703\tvalid_1's l1: 0.18269\n",
      "[2700]\ttraining's l1: 0.147568\tvalid_1's l1: 0.181266\n",
      "[2800]\ttraining's l1: 0.145382\tvalid_1's l1: 0.179768\n",
      "[2900]\ttraining's l1: 0.143396\tvalid_1's l1: 0.178449\n",
      "[3000]\ttraining's l1: 0.141448\tvalid_1's l1: 0.177179\n",
      "[3100]\ttraining's l1: 0.139585\tvalid_1's l1: 0.175991\n",
      "[3200]\ttraining's l1: 0.137677\tvalid_1's l1: 0.174724\n",
      "[3300]\ttraining's l1: 0.13599\tvalid_1's l1: 0.173672\n",
      "[3400]\ttraining's l1: 0.134315\tvalid_1's l1: 0.17262\n",
      "[3500]\ttraining's l1: 0.132681\tvalid_1's l1: 0.171595\n",
      "[3600]\ttraining's l1: 0.131117\tvalid_1's l1: 0.170606\n",
      "[3700]\ttraining's l1: 0.129591\tvalid_1's l1: 0.169656\n",
      "[3800]\ttraining's l1: 0.128137\tvalid_1's l1: 0.168782\n",
      "[3900]\ttraining's l1: 0.126664\tvalid_1's l1: 0.16789\n",
      "[4000]\ttraining's l1: 0.125226\tvalid_1's l1: 0.166984\n",
      "[4100]\ttraining's l1: 0.123895\tvalid_1's l1: 0.166193\n",
      "[4200]\ttraining's l1: 0.122579\tvalid_1's l1: 0.165445\n",
      "[4300]\ttraining's l1: 0.121228\tvalid_1's l1: 0.164669\n",
      "[4400]\ttraining's l1: 0.120009\tvalid_1's l1: 0.16399\n",
      "[4500]\ttraining's l1: 0.118771\tvalid_1's l1: 0.163285\n",
      "[4600]\ttraining's l1: 0.117571\tvalid_1's l1: 0.16266\n",
      "[4700]\ttraining's l1: 0.116443\tvalid_1's l1: 0.162078\n",
      "[4800]\ttraining's l1: 0.11532\tvalid_1's l1: 0.16143\n",
      "[4900]\ttraining's l1: 0.114217\tvalid_1's l1: 0.160801\n",
      "[5000]\ttraining's l1: 0.113065\tvalid_1's l1: 0.160149\n",
      "[5100]\ttraining's l1: 0.111986\tvalid_1's l1: 0.15954\n",
      "[5200]\ttraining's l1: 0.110963\tvalid_1's l1: 0.159029\n",
      "[5300]\ttraining's l1: 0.109924\tvalid_1's l1: 0.158432\n",
      "[5400]\ttraining's l1: 0.108941\tvalid_1's l1: 0.157923\n",
      "[5500]\ttraining's l1: 0.107984\tvalid_1's l1: 0.157402\n",
      "[5600]\ttraining's l1: 0.107028\tvalid_1's l1: 0.15688\n",
      "[5700]\ttraining's l1: 0.106075\tvalid_1's l1: 0.156381\n",
      "[5800]\ttraining's l1: 0.105146\tvalid_1's l1: 0.155879\n",
      "[5900]\ttraining's l1: 0.104253\tvalid_1's l1: 0.155426\n",
      "[6000]\ttraining's l1: 0.103315\tvalid_1's l1: 0.154901\n",
      "[6100]\ttraining's l1: 0.102428\tvalid_1's l1: 0.154422\n",
      "[6200]\ttraining's l1: 0.101519\tvalid_1's l1: 0.153946\n",
      "[6300]\ttraining's l1: 0.10069\tvalid_1's l1: 0.153532\n",
      "[6400]\ttraining's l1: 0.0998463\tvalid_1's l1: 0.153106\n",
      "[6500]\ttraining's l1: 0.0990053\tvalid_1's l1: 0.152683\n",
      "[6600]\ttraining's l1: 0.0981859\tvalid_1's l1: 0.152281\n",
      "[6700]\ttraining's l1: 0.0973647\tvalid_1's l1: 0.15183\n",
      "[6800]\ttraining's l1: 0.0965953\tvalid_1's l1: 0.151427\n",
      "[6900]\ttraining's l1: 0.0958129\tvalid_1's l1: 0.151045\n",
      "[7000]\ttraining's l1: 0.095042\tvalid_1's l1: 0.150662\n",
      "[7100]\ttraining's l1: 0.0942881\tvalid_1's l1: 0.150321\n",
      "[7200]\ttraining's l1: 0.0935633\tvalid_1's l1: 0.14999\n",
      "[7300]\ttraining's l1: 0.0928528\tvalid_1's l1: 0.14964\n",
      "[7400]\ttraining's l1: 0.0921632\tvalid_1's l1: 0.149299\n",
      "[7500]\ttraining's l1: 0.0914319\tvalid_1's l1: 0.148956\n",
      "[7600]\ttraining's l1: 0.0907735\tvalid_1's l1: 0.148659\n",
      "[7700]\ttraining's l1: 0.0900817\tvalid_1's l1: 0.148327\n",
      "[7800]\ttraining's l1: 0.0894246\tvalid_1's l1: 0.148028\n",
      "[7900]\ttraining's l1: 0.0887496\tvalid_1's l1: 0.147711\n",
      "[8000]\ttraining's l1: 0.0881188\tvalid_1's l1: 0.14744\n",
      "[8100]\ttraining's l1: 0.0874679\tvalid_1's l1: 0.147136\n",
      "[8200]\ttraining's l1: 0.0868164\tvalid_1's l1: 0.146834\n",
      "[8300]\ttraining's l1: 0.0861964\tvalid_1's l1: 0.146543\n",
      "[8400]\ttraining's l1: 0.0855974\tvalid_1's l1: 0.146258\n",
      "[8500]\ttraining's l1: 0.084974\tvalid_1's l1: 0.145949\n",
      "[8600]\ttraining's l1: 0.0843664\tvalid_1's l1: 0.145664\n",
      "[8700]\ttraining's l1: 0.0837742\tvalid_1's l1: 0.145387\n",
      "[8800]\ttraining's l1: 0.0832001\tvalid_1's l1: 0.145124\n",
      "[8900]\ttraining's l1: 0.0826576\tvalid_1's l1: 0.14489\n",
      "[9000]\ttraining's l1: 0.0820962\tvalid_1's l1: 0.144643\n",
      "[9100]\ttraining's l1: 0.081541\tvalid_1's l1: 0.144406\n",
      "[9200]\ttraining's l1: 0.0809791\tvalid_1's l1: 0.144154\n",
      "[9300]\ttraining's l1: 0.0804403\tvalid_1's l1: 0.143908\n",
      "[9400]\ttraining's l1: 0.0798818\tvalid_1's l1: 0.143655\n",
      "[9500]\ttraining's l1: 0.0793633\tvalid_1's l1: 0.143412\n",
      "[9600]\ttraining's l1: 0.078817\tvalid_1's l1: 0.143182\n",
      "[9700]\ttraining's l1: 0.0783264\tvalid_1's l1: 0.142966\n",
      "[9800]\ttraining's l1: 0.0778311\tvalid_1's l1: 0.142776\n",
      "[9900]\ttraining's l1: 0.0773432\tvalid_1's l1: 0.142567\n",
      "[10000]\ttraining's l1: 0.0768426\tvalid_1's l1: 0.142345\n",
      "[10100]\ttraining's l1: 0.0763563\tvalid_1's l1: 0.142144\n",
      "[10200]\ttraining's l1: 0.0758744\tvalid_1's l1: 0.141943\n",
      "[10300]\ttraining's l1: 0.0753963\tvalid_1's l1: 0.141748\n",
      "[10400]\ttraining's l1: 0.0749179\tvalid_1's l1: 0.141557\n",
      "[10500]\ttraining's l1: 0.0744533\tvalid_1's l1: 0.141349\n",
      "[10600]\ttraining's l1: 0.0739679\tvalid_1's l1: 0.141138\n",
      "[10700]\ttraining's l1: 0.0734995\tvalid_1's l1: 0.140945\n",
      "[10800]\ttraining's l1: 0.0730569\tvalid_1's l1: 0.140767\n",
      "[10900]\ttraining's l1: 0.072604\tvalid_1's l1: 0.140574\n",
      "[11000]\ttraining's l1: 0.0721646\tvalid_1's l1: 0.140396\n",
      "[11100]\ttraining's l1: 0.0717344\tvalid_1's l1: 0.140228\n",
      "[11200]\ttraining's l1: 0.071307\tvalid_1's l1: 0.140046\n",
      "[11300]\ttraining's l1: 0.0708737\tvalid_1's l1: 0.139853\n",
      "[11400]\ttraining's l1: 0.0704397\tvalid_1's l1: 0.139685\n",
      "[11500]\ttraining's l1: 0.070008\tvalid_1's l1: 0.139503\n",
      "[11600]\ttraining's l1: 0.0695823\tvalid_1's l1: 0.139331\n",
      "[11700]\ttraining's l1: 0.0691663\tvalid_1's l1: 0.139182\n",
      "[11800]\ttraining's l1: 0.0687773\tvalid_1's l1: 0.139035\n",
      "[11900]\ttraining's l1: 0.0683912\tvalid_1's l1: 0.138897\n",
      "[12000]\ttraining's l1: 0.0680114\tvalid_1's l1: 0.138738\n",
      "[12100]\ttraining's l1: 0.0676269\tvalid_1's l1: 0.13859\n",
      "[12200]\ttraining's l1: 0.0672507\tvalid_1's l1: 0.138444\n",
      "[12300]\ttraining's l1: 0.06685\tvalid_1's l1: 0.138272\n",
      "[12400]\ttraining's l1: 0.066462\tvalid_1's l1: 0.138134\n",
      "[12500]\ttraining's l1: 0.0660859\tvalid_1's l1: 0.137979\n",
      "[12600]\ttraining's l1: 0.0657034\tvalid_1's l1: 0.137827\n",
      "[12700]\ttraining's l1: 0.065334\tvalid_1's l1: 0.137676\n",
      "[12800]\ttraining's l1: 0.0649508\tvalid_1's l1: 0.137526\n",
      "[12900]\ttraining's l1: 0.064597\tvalid_1's l1: 0.1374\n",
      "[13000]\ttraining's l1: 0.0642383\tvalid_1's l1: 0.137246\n",
      "[13100]\ttraining's l1: 0.063875\tvalid_1's l1: 0.137108\n",
      "[13200]\ttraining's l1: 0.0635278\tvalid_1's l1: 0.136975\n",
      "[13300]\ttraining's l1: 0.0631796\tvalid_1's l1: 0.13685\n",
      "[13400]\ttraining's l1: 0.0628398\tvalid_1's l1: 0.136728\n",
      "[13500]\ttraining's l1: 0.0624925\tvalid_1's l1: 0.136597\n",
      "[13600]\ttraining's l1: 0.0621502\tvalid_1's l1: 0.136463\n",
      "[13700]\ttraining's l1: 0.0618212\tvalid_1's l1: 0.136333\n",
      "[13800]\ttraining's l1: 0.0615008\tvalid_1's l1: 0.136206\n",
      "[13900]\ttraining's l1: 0.061168\tvalid_1's l1: 0.136073\n",
      "[14000]\ttraining's l1: 0.0608378\tvalid_1's l1: 0.13596\n",
      "[14100]\ttraining's l1: 0.0605187\tvalid_1's l1: 0.135853\n",
      "[14200]\ttraining's l1: 0.06019\tvalid_1's l1: 0.135732\n",
      "[14300]\ttraining's l1: 0.0598771\tvalid_1's l1: 0.135614\n",
      "[14400]\ttraining's l1: 0.059571\tvalid_1's l1: 0.135506\n",
      "[14500]\ttraining's l1: 0.0592597\tvalid_1's l1: 0.135396\n",
      "[14600]\ttraining's l1: 0.0589491\tvalid_1's l1: 0.135286\n",
      "[14700]\ttraining's l1: 0.0586645\tvalid_1's l1: 0.135193\n",
      "[14800]\ttraining's l1: 0.0583577\tvalid_1's l1: 0.135079\n",
      "[14900]\ttraining's l1: 0.0580382\tvalid_1's l1: 0.134951\n",
      "[15000]\ttraining's l1: 0.057747\tvalid_1's l1: 0.134861\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.057747\tvalid_1's l1: 0.134861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3JHH Fold 0, logMAE: -2.003478115268563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.376771\tvalid_1's l1: 0.378257\n",
      "[200]\ttraining's l1: 0.319146\tvalid_1's l1: 0.322653\n",
      "[300]\ttraining's l1: 0.289618\tvalid_1's l1: 0.294949\n",
      "[400]\ttraining's l1: 0.269405\tvalid_1's l1: 0.276518\n",
      "[500]\ttraining's l1: 0.254275\tvalid_1's l1: 0.26288\n",
      "[600]\ttraining's l1: 0.241961\tvalid_1's l1: 0.252195\n",
      "[700]\ttraining's l1: 0.231515\tvalid_1's l1: 0.243106\n",
      "[800]\ttraining's l1: 0.223197\tvalid_1's l1: 0.236122\n",
      "[900]\ttraining's l1: 0.21557\tvalid_1's l1: 0.229846\n",
      "[1000]\ttraining's l1: 0.208915\tvalid_1's l1: 0.224449\n",
      "[1100]\ttraining's l1: 0.202988\tvalid_1's l1: 0.219687\n",
      "[1200]\ttraining's l1: 0.197464\tvalid_1's l1: 0.215354\n",
      "[1300]\ttraining's l1: 0.192419\tvalid_1's l1: 0.211324\n",
      "[1400]\ttraining's l1: 0.188088\tvalid_1's l1: 0.208037\n",
      "[1500]\ttraining's l1: 0.183755\tvalid_1's l1: 0.204664\n",
      "[1600]\ttraining's l1: 0.179601\tvalid_1's l1: 0.201479\n",
      "[1700]\ttraining's l1: 0.175954\tvalid_1's l1: 0.198792\n",
      "[1800]\ttraining's l1: 0.172412\tvalid_1's l1: 0.196212\n",
      "[1900]\ttraining's l1: 0.169427\tvalid_1's l1: 0.194145\n",
      "[2000]\ttraining's l1: 0.166297\tvalid_1's l1: 0.191982\n",
      "[2100]\ttraining's l1: 0.163271\tvalid_1's l1: 0.189871\n",
      "[2200]\ttraining's l1: 0.160429\tvalid_1's l1: 0.187855\n",
      "[2300]\ttraining's l1: 0.157851\tvalid_1's l1: 0.186032\n",
      "[2400]\ttraining's l1: 0.15534\tvalid_1's l1: 0.184381\n",
      "[2500]\ttraining's l1: 0.152889\tvalid_1's l1: 0.182786\n",
      "[2600]\ttraining's l1: 0.150596\tvalid_1's l1: 0.181193\n",
      "[2700]\ttraining's l1: 0.148449\tvalid_1's l1: 0.179742\n",
      "[2800]\ttraining's l1: 0.146268\tvalid_1's l1: 0.178299\n",
      "[2900]\ttraining's l1: 0.144214\tvalid_1's l1: 0.176936\n",
      "[3000]\ttraining's l1: 0.142267\tvalid_1's l1: 0.175707\n",
      "[3100]\ttraining's l1: 0.14036\tvalid_1's l1: 0.174517\n",
      "[3200]\ttraining's l1: 0.138463\tvalid_1's l1: 0.173298\n",
      "[3300]\ttraining's l1: 0.136761\tvalid_1's l1: 0.172259\n",
      "[3400]\ttraining's l1: 0.135135\tvalid_1's l1: 0.171352\n",
      "[3500]\ttraining's l1: 0.133527\tvalid_1's l1: 0.170346\n",
      "[3600]\ttraining's l1: 0.13193\tvalid_1's l1: 0.16935\n",
      "[3700]\ttraining's l1: 0.130419\tvalid_1's l1: 0.168442\n",
      "[3800]\ttraining's l1: 0.128908\tvalid_1's l1: 0.167561\n",
      "[3900]\ttraining's l1: 0.127493\tvalid_1's l1: 0.166751\n",
      "[4000]\ttraining's l1: 0.126051\tvalid_1's l1: 0.165881\n",
      "[4100]\ttraining's l1: 0.124654\tvalid_1's l1: 0.165051\n",
      "[4200]\ttraining's l1: 0.123324\tvalid_1's l1: 0.164267\n",
      "[4300]\ttraining's l1: 0.122051\tvalid_1's l1: 0.163534\n",
      "[4400]\ttraining's l1: 0.120784\tvalid_1's l1: 0.1628\n",
      "[4500]\ttraining's l1: 0.119483\tvalid_1's l1: 0.162096\n",
      "[4600]\ttraining's l1: 0.118296\tvalid_1's l1: 0.161413\n",
      "[4700]\ttraining's l1: 0.117097\tvalid_1's l1: 0.16075\n",
      "[4800]\ttraining's l1: 0.115956\tvalid_1's l1: 0.160104\n",
      "[4900]\ttraining's l1: 0.114815\tvalid_1's l1: 0.159502\n",
      "[5000]\ttraining's l1: 0.113737\tvalid_1's l1: 0.158925\n",
      "[5100]\ttraining's l1: 0.112622\tvalid_1's l1: 0.158292\n",
      "[5200]\ttraining's l1: 0.111516\tvalid_1's l1: 0.157695\n",
      "[5300]\ttraining's l1: 0.110504\tvalid_1's l1: 0.157179\n",
      "[5400]\ttraining's l1: 0.109484\tvalid_1's l1: 0.156611\n",
      "[5500]\ttraining's l1: 0.108444\tvalid_1's l1: 0.156033\n",
      "[5600]\ttraining's l1: 0.107484\tvalid_1's l1: 0.155504\n",
      "[5700]\ttraining's l1: 0.106504\tvalid_1's l1: 0.154987\n",
      "[5800]\ttraining's l1: 0.105585\tvalid_1's l1: 0.154527\n",
      "[5900]\ttraining's l1: 0.104664\tvalid_1's l1: 0.154077\n",
      "[6000]\ttraining's l1: 0.103746\tvalid_1's l1: 0.153611\n",
      "[6100]\ttraining's l1: 0.102855\tvalid_1's l1: 0.153186\n",
      "[6200]\ttraining's l1: 0.101984\tvalid_1's l1: 0.152767\n",
      "[6300]\ttraining's l1: 0.101127\tvalid_1's l1: 0.152325\n",
      "[6400]\ttraining's l1: 0.100315\tvalid_1's l1: 0.15195\n",
      "[6500]\ttraining's l1: 0.099513\tvalid_1's l1: 0.151577\n",
      "[6600]\ttraining's l1: 0.0987446\tvalid_1's l1: 0.151216\n",
      "[6700]\ttraining's l1: 0.0979264\tvalid_1's l1: 0.150792\n",
      "[6800]\ttraining's l1: 0.0971216\tvalid_1's l1: 0.150397\n",
      "[6900]\ttraining's l1: 0.0963779\tvalid_1's l1: 0.150055\n",
      "[7000]\ttraining's l1: 0.0956005\tvalid_1's l1: 0.14969\n",
      "[7100]\ttraining's l1: 0.0948653\tvalid_1's l1: 0.149372\n",
      "[7200]\ttraining's l1: 0.0941263\tvalid_1's l1: 0.149033\n",
      "[7300]\ttraining's l1: 0.0934135\tvalid_1's l1: 0.148709\n",
      "[7400]\ttraining's l1: 0.0927272\tvalid_1's l1: 0.148408\n",
      "[7500]\ttraining's l1: 0.0920072\tvalid_1's l1: 0.148053\n",
      "[7600]\ttraining's l1: 0.0913247\tvalid_1's l1: 0.147715\n",
      "[7700]\ttraining's l1: 0.0906374\tvalid_1's l1: 0.147388\n",
      "[7800]\ttraining's l1: 0.089979\tvalid_1's l1: 0.147058\n",
      "[7900]\ttraining's l1: 0.0893392\tvalid_1's l1: 0.146762\n",
      "[8000]\ttraining's l1: 0.0887253\tvalid_1's l1: 0.146473\n",
      "[8100]\ttraining's l1: 0.0880641\tvalid_1's l1: 0.146176\n",
      "[8200]\ttraining's l1: 0.0874153\tvalid_1's l1: 0.145864\n",
      "[8300]\ttraining's l1: 0.086805\tvalid_1's l1: 0.145577\n",
      "[8400]\ttraining's l1: 0.0861587\tvalid_1's l1: 0.145291\n",
      "[8500]\ttraining's l1: 0.0855751\tvalid_1's l1: 0.145042\n",
      "[8600]\ttraining's l1: 0.0849935\tvalid_1's l1: 0.144777\n",
      "[8700]\ttraining's l1: 0.0844319\tvalid_1's l1: 0.144538\n",
      "[8800]\ttraining's l1: 0.0838451\tvalid_1's l1: 0.144276\n",
      "[8900]\ttraining's l1: 0.0832839\tvalid_1's l1: 0.144031\n",
      "[9000]\ttraining's l1: 0.0827084\tvalid_1's l1: 0.143767\n",
      "[9100]\ttraining's l1: 0.0821325\tvalid_1's l1: 0.143523\n",
      "[9200]\ttraining's l1: 0.0816208\tvalid_1's l1: 0.143313\n",
      "[9300]\ttraining's l1: 0.0810779\tvalid_1's l1: 0.143077\n",
      "[9400]\ttraining's l1: 0.0805449\tvalid_1's l1: 0.142857\n",
      "[9500]\ttraining's l1: 0.0800293\tvalid_1's l1: 0.142645\n",
      "[9600]\ttraining's l1: 0.0795031\tvalid_1's l1: 0.142436\n",
      "[9700]\ttraining's l1: 0.078987\tvalid_1's l1: 0.142203\n",
      "[9800]\ttraining's l1: 0.0784762\tvalid_1's l1: 0.141996\n",
      "[9900]\ttraining's l1: 0.077978\tvalid_1's l1: 0.14179\n",
      "[10000]\ttraining's l1: 0.0774727\tvalid_1's l1: 0.14157\n",
      "[10100]\ttraining's l1: 0.07698\tvalid_1's l1: 0.141357\n",
      "[10200]\ttraining's l1: 0.0764843\tvalid_1's l1: 0.141145\n",
      "[10300]\ttraining's l1: 0.0760009\tvalid_1's l1: 0.140954\n",
      "[10400]\ttraining's l1: 0.0755144\tvalid_1's l1: 0.140754\n",
      "[10500]\ttraining's l1: 0.0750398\tvalid_1's l1: 0.140578\n",
      "[10600]\ttraining's l1: 0.0745759\tvalid_1's l1: 0.140394\n",
      "[10700]\ttraining's l1: 0.0741004\tvalid_1's l1: 0.14019\n",
      "[10800]\ttraining's l1: 0.0736373\tvalid_1's l1: 0.139996\n",
      "[10900]\ttraining's l1: 0.0731862\tvalid_1's l1: 0.139806\n",
      "[11000]\ttraining's l1: 0.0727344\tvalid_1's l1: 0.139633\n",
      "[11100]\ttraining's l1: 0.072296\tvalid_1's l1: 0.139476\n",
      "[11200]\ttraining's l1: 0.0718694\tvalid_1's l1: 0.139303\n",
      "[11300]\ttraining's l1: 0.071425\tvalid_1's l1: 0.139135\n",
      "[11400]\ttraining's l1: 0.0710159\tvalid_1's l1: 0.138978\n",
      "[11500]\ttraining's l1: 0.0705796\tvalid_1's l1: 0.138802\n",
      "[11600]\ttraining's l1: 0.0701526\tvalid_1's l1: 0.138641\n",
      "[11700]\ttraining's l1: 0.0697316\tvalid_1's l1: 0.138481\n",
      "[11800]\ttraining's l1: 0.0693108\tvalid_1's l1: 0.138315\n",
      "[11900]\ttraining's l1: 0.0689162\tvalid_1's l1: 0.138167\n",
      "[12000]\ttraining's l1: 0.068511\tvalid_1's l1: 0.138011\n",
      "[12100]\ttraining's l1: 0.0681061\tvalid_1's l1: 0.137843\n",
      "[12200]\ttraining's l1: 0.0677021\tvalid_1's l1: 0.137687\n",
      "[12300]\ttraining's l1: 0.0673023\tvalid_1's l1: 0.137532\n",
      "[12400]\ttraining's l1: 0.0669194\tvalid_1's l1: 0.137388\n",
      "[12500]\ttraining's l1: 0.0665567\tvalid_1's l1: 0.137257\n",
      "[12600]\ttraining's l1: 0.0661882\tvalid_1's l1: 0.13713\n",
      "[12700]\ttraining's l1: 0.0658036\tvalid_1's l1: 0.13697\n",
      "[12800]\ttraining's l1: 0.0654347\tvalid_1's l1: 0.136824\n",
      "[12900]\ttraining's l1: 0.0650721\tvalid_1's l1: 0.136684\n",
      "[13000]\ttraining's l1: 0.0646938\tvalid_1's l1: 0.136556\n",
      "[13100]\ttraining's l1: 0.0643285\tvalid_1's l1: 0.13642\n",
      "[13200]\ttraining's l1: 0.0639751\tvalid_1's l1: 0.136288\n",
      "[13300]\ttraining's l1: 0.0636206\tvalid_1's l1: 0.136162\n",
      "[13400]\ttraining's l1: 0.0632632\tvalid_1's l1: 0.136038\n",
      "[13500]\ttraining's l1: 0.0629132\tvalid_1's l1: 0.135918\n",
      "[13600]\ttraining's l1: 0.0625631\tvalid_1's l1: 0.135772\n",
      "[13700]\ttraining's l1: 0.0622212\tvalid_1's l1: 0.135635\n",
      "[13800]\ttraining's l1: 0.0618939\tvalid_1's l1: 0.135498\n",
      "[13900]\ttraining's l1: 0.0615433\tvalid_1's l1: 0.135373\n",
      "[14000]\ttraining's l1: 0.0612044\tvalid_1's l1: 0.13524\n",
      "[14100]\ttraining's l1: 0.0608673\tvalid_1's l1: 0.135126\n",
      "[14200]\ttraining's l1: 0.0605345\tvalid_1's l1: 0.135001\n",
      "[14300]\ttraining's l1: 0.0602007\tvalid_1's l1: 0.134861\n",
      "[14400]\ttraining's l1: 0.0598801\tvalid_1's l1: 0.134746\n",
      "[14500]\ttraining's l1: 0.0595534\tvalid_1's l1: 0.134617\n",
      "[14600]\ttraining's l1: 0.059241\tvalid_1's l1: 0.134515\n",
      "[14700]\ttraining's l1: 0.0589218\tvalid_1's l1: 0.134408\n",
      "[14800]\ttraining's l1: 0.0586061\tvalid_1's l1: 0.134304\n",
      "[14900]\ttraining's l1: 0.0583074\tvalid_1's l1: 0.134189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.058013\tvalid_1's l1: 0.134089\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.058013\tvalid_1's l1: 0.134089\n",
      "3JHH Fold 1, logMAE: -2.0092484053706787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.378315\tvalid_1's l1: 0.382553\n",
      "[200]\ttraining's l1: 0.318312\tvalid_1's l1: 0.324329\n",
      "[300]\ttraining's l1: 0.288129\tvalid_1's l1: 0.296012\n",
      "[400]\ttraining's l1: 0.267997\tvalid_1's l1: 0.277468\n",
      "[500]\ttraining's l1: 0.253387\tvalid_1's l1: 0.264297\n",
      "[600]\ttraining's l1: 0.240874\tvalid_1's l1: 0.253042\n",
      "[700]\ttraining's l1: 0.230788\tvalid_1's l1: 0.244325\n",
      "[800]\ttraining's l1: 0.221975\tvalid_1's l1: 0.236827\n",
      "[900]\ttraining's l1: 0.214459\tvalid_1's l1: 0.230602\n",
      "[1000]\ttraining's l1: 0.207571\tvalid_1's l1: 0.2249\n",
      "[1100]\ttraining's l1: 0.20137\tvalid_1's l1: 0.219791\n",
      "[1200]\ttraining's l1: 0.195938\tvalid_1's l1: 0.215394\n",
      "[1300]\ttraining's l1: 0.190994\tvalid_1's l1: 0.211487\n",
      "[1400]\ttraining's l1: 0.186342\tvalid_1's l1: 0.207845\n",
      "[1500]\ttraining's l1: 0.18232\tvalid_1's l1: 0.204807\n",
      "[1600]\ttraining's l1: 0.178433\tvalid_1's l1: 0.201908\n",
      "[1700]\ttraining's l1: 0.174741\tvalid_1's l1: 0.199124\n",
      "[1800]\ttraining's l1: 0.171397\tvalid_1's l1: 0.196729\n",
      "[1900]\ttraining's l1: 0.168226\tvalid_1's l1: 0.194422\n",
      "[2000]\ttraining's l1: 0.165223\tvalid_1's l1: 0.192309\n",
      "[2100]\ttraining's l1: 0.162278\tvalid_1's l1: 0.190135\n",
      "[2200]\ttraining's l1: 0.159542\tvalid_1's l1: 0.188298\n",
      "[2300]\ttraining's l1: 0.156979\tvalid_1's l1: 0.186478\n",
      "[2400]\ttraining's l1: 0.154482\tvalid_1's l1: 0.184793\n",
      "[2500]\ttraining's l1: 0.152116\tvalid_1's l1: 0.183239\n",
      "[2600]\ttraining's l1: 0.149811\tvalid_1's l1: 0.181688\n",
      "[2700]\ttraining's l1: 0.147648\tvalid_1's l1: 0.180263\n",
      "[2800]\ttraining's l1: 0.145553\tvalid_1's l1: 0.178888\n",
      "[2900]\ttraining's l1: 0.143511\tvalid_1's l1: 0.177533\n",
      "[3000]\ttraining's l1: 0.141634\tvalid_1's l1: 0.176334\n",
      "[3100]\ttraining's l1: 0.139826\tvalid_1's l1: 0.175162\n",
      "[3200]\ttraining's l1: 0.137958\tvalid_1's l1: 0.173966\n",
      "[3300]\ttraining's l1: 0.136262\tvalid_1's l1: 0.172904\n",
      "[3400]\ttraining's l1: 0.134591\tvalid_1's l1: 0.171892\n",
      "[3500]\ttraining's l1: 0.132933\tvalid_1's l1: 0.170849\n",
      "[3600]\ttraining's l1: 0.131428\tvalid_1's l1: 0.169962\n",
      "[3700]\ttraining's l1: 0.129847\tvalid_1's l1: 0.168991\n",
      "[3800]\ttraining's l1: 0.128329\tvalid_1's l1: 0.168085\n",
      "[3900]\ttraining's l1: 0.126818\tvalid_1's l1: 0.167148\n",
      "[4000]\ttraining's l1: 0.125374\tvalid_1's l1: 0.166276\n",
      "[4100]\ttraining's l1: 0.124054\tvalid_1's l1: 0.165515\n",
      "[4200]\ttraining's l1: 0.122755\tvalid_1's l1: 0.164786\n",
      "[4300]\ttraining's l1: 0.121545\tvalid_1's l1: 0.164084\n",
      "[4400]\ttraining's l1: 0.120339\tvalid_1's l1: 0.163413\n",
      "[4500]\ttraining's l1: 0.119117\tvalid_1's l1: 0.162702\n",
      "[4600]\ttraining's l1: 0.117937\tvalid_1's l1: 0.162056\n",
      "[4700]\ttraining's l1: 0.116765\tvalid_1's l1: 0.161389\n",
      "[4800]\ttraining's l1: 0.115663\tvalid_1's l1: 0.160783\n",
      "[4900]\ttraining's l1: 0.114502\tvalid_1's l1: 0.160122\n",
      "[5000]\ttraining's l1: 0.113349\tvalid_1's l1: 0.159432\n",
      "[5100]\ttraining's l1: 0.112284\tvalid_1's l1: 0.158861\n",
      "[5200]\ttraining's l1: 0.111193\tvalid_1's l1: 0.15826\n",
      "[5300]\ttraining's l1: 0.110133\tvalid_1's l1: 0.157696\n",
      "[5400]\ttraining's l1: 0.109183\tvalid_1's l1: 0.157201\n",
      "[5500]\ttraining's l1: 0.108218\tvalid_1's l1: 0.156688\n",
      "[5600]\ttraining's l1: 0.107186\tvalid_1's l1: 0.156146\n",
      "[5700]\ttraining's l1: 0.106214\tvalid_1's l1: 0.155668\n",
      "[5800]\ttraining's l1: 0.105303\tvalid_1's l1: 0.155203\n",
      "[5900]\ttraining's l1: 0.104381\tvalid_1's l1: 0.154713\n",
      "[6000]\ttraining's l1: 0.103487\tvalid_1's l1: 0.154267\n",
      "[6100]\ttraining's l1: 0.102598\tvalid_1's l1: 0.153823\n",
      "[6200]\ttraining's l1: 0.101756\tvalid_1's l1: 0.15342\n",
      "[6300]\ttraining's l1: 0.100911\tvalid_1's l1: 0.153004\n",
      "[6400]\ttraining's l1: 0.100077\tvalid_1's l1: 0.152597\n",
      "[6500]\ttraining's l1: 0.0992506\tvalid_1's l1: 0.152193\n",
      "[6600]\ttraining's l1: 0.0984241\tvalid_1's l1: 0.151762\n",
      "[6700]\ttraining's l1: 0.0976237\tvalid_1's l1: 0.15138\n",
      "[6800]\ttraining's l1: 0.0968443\tvalid_1's l1: 0.151005\n",
      "[6900]\ttraining's l1: 0.0960605\tvalid_1's l1: 0.150608\n",
      "[7000]\ttraining's l1: 0.0953122\tvalid_1's l1: 0.150277\n",
      "[7100]\ttraining's l1: 0.0945446\tvalid_1's l1: 0.149906\n",
      "[7200]\ttraining's l1: 0.0937857\tvalid_1's l1: 0.149535\n",
      "[7300]\ttraining's l1: 0.093063\tvalid_1's l1: 0.149179\n",
      "[7400]\ttraining's l1: 0.0923589\tvalid_1's l1: 0.148865\n",
      "[7500]\ttraining's l1: 0.0916518\tvalid_1's l1: 0.148537\n",
      "[7600]\ttraining's l1: 0.0909844\tvalid_1's l1: 0.148236\n",
      "[7700]\ttraining's l1: 0.090296\tvalid_1's l1: 0.147909\n",
      "[7800]\ttraining's l1: 0.0896465\tvalid_1's l1: 0.147633\n",
      "[7900]\ttraining's l1: 0.089019\tvalid_1's l1: 0.147358\n",
      "[8000]\ttraining's l1: 0.088402\tvalid_1's l1: 0.147073\n",
      "[8100]\ttraining's l1: 0.0877833\tvalid_1's l1: 0.146812\n",
      "[8200]\ttraining's l1: 0.0871311\tvalid_1's l1: 0.146537\n",
      "[8300]\ttraining's l1: 0.0865112\tvalid_1's l1: 0.146265\n",
      "[8400]\ttraining's l1: 0.0858945\tvalid_1's l1: 0.145996\n",
      "[8500]\ttraining's l1: 0.0852776\tvalid_1's l1: 0.145685\n",
      "[8600]\ttraining's l1: 0.0847127\tvalid_1's l1: 0.145426\n",
      "[8700]\ttraining's l1: 0.084118\tvalid_1's l1: 0.145165\n",
      "[8800]\ttraining's l1: 0.0835469\tvalid_1's l1: 0.144942\n",
      "[8900]\ttraining's l1: 0.0829619\tvalid_1's l1: 0.144671\n",
      "[9000]\ttraining's l1: 0.0823863\tvalid_1's l1: 0.14442\n",
      "[9100]\ttraining's l1: 0.0818537\tvalid_1's l1: 0.144182\n",
      "[9200]\ttraining's l1: 0.0812948\tvalid_1's l1: 0.143932\n",
      "[9300]\ttraining's l1: 0.0807431\tvalid_1's l1: 0.14371\n",
      "[9400]\ttraining's l1: 0.0801964\tvalid_1's l1: 0.143481\n",
      "[9500]\ttraining's l1: 0.0796648\tvalid_1's l1: 0.143272\n",
      "[9600]\ttraining's l1: 0.079135\tvalid_1's l1: 0.143063\n",
      "[9700]\ttraining's l1: 0.0786302\tvalid_1's l1: 0.142848\n",
      "[9800]\ttraining's l1: 0.0781292\tvalid_1's l1: 0.142645\n",
      "[9900]\ttraining's l1: 0.0776332\tvalid_1's l1: 0.142456\n",
      "[10000]\ttraining's l1: 0.0771452\tvalid_1's l1: 0.14227\n",
      "[10100]\ttraining's l1: 0.0766533\tvalid_1's l1: 0.14206\n",
      "[10200]\ttraining's l1: 0.0761612\tvalid_1's l1: 0.141842\n",
      "[10300]\ttraining's l1: 0.0756835\tvalid_1's l1: 0.14164\n",
      "[10400]\ttraining's l1: 0.075217\tvalid_1's l1: 0.141451\n",
      "[10500]\ttraining's l1: 0.0747298\tvalid_1's l1: 0.141233\n",
      "[10600]\ttraining's l1: 0.0742692\tvalid_1's l1: 0.141034\n",
      "[10700]\ttraining's l1: 0.0738019\tvalid_1's l1: 0.140856\n",
      "[10800]\ttraining's l1: 0.0733564\tvalid_1's l1: 0.14069\n",
      "[10900]\ttraining's l1: 0.0729163\tvalid_1's l1: 0.140516\n",
      "[11000]\ttraining's l1: 0.072479\tvalid_1's l1: 0.140339\n",
      "[11100]\ttraining's l1: 0.0720285\tvalid_1's l1: 0.140139\n",
      "[11200]\ttraining's l1: 0.0715977\tvalid_1's l1: 0.13996\n",
      "[11300]\ttraining's l1: 0.0711474\tvalid_1's l1: 0.139773\n",
      "[11400]\ttraining's l1: 0.0707039\tvalid_1's l1: 0.139591\n",
      "[11500]\ttraining's l1: 0.0702679\tvalid_1's l1: 0.139428\n",
      "[11600]\ttraining's l1: 0.0698589\tvalid_1's l1: 0.139259\n",
      "[11700]\ttraining's l1: 0.0694453\tvalid_1's l1: 0.13909\n",
      "[11800]\ttraining's l1: 0.0690287\tvalid_1's l1: 0.138931\n",
      "[11900]\ttraining's l1: 0.0686235\tvalid_1's l1: 0.138775\n",
      "[12000]\ttraining's l1: 0.0682387\tvalid_1's l1: 0.138628\n",
      "[12100]\ttraining's l1: 0.0678423\tvalid_1's l1: 0.138469\n",
      "[12200]\ttraining's l1: 0.0674586\tvalid_1's l1: 0.138328\n",
      "[12300]\ttraining's l1: 0.0670653\tvalid_1's l1: 0.138175\n",
      "[12400]\ttraining's l1: 0.0666789\tvalid_1's l1: 0.138034\n",
      "[12500]\ttraining's l1: 0.0663012\tvalid_1's l1: 0.137898\n",
      "[12600]\ttraining's l1: 0.0659335\tvalid_1's l1: 0.137762\n",
      "[12700]\ttraining's l1: 0.065577\tvalid_1's l1: 0.137632\n",
      "[12800]\ttraining's l1: 0.0652093\tvalid_1's l1: 0.137483\n",
      "[12900]\ttraining's l1: 0.0648398\tvalid_1's l1: 0.137329\n",
      "[13000]\ttraining's l1: 0.0644791\tvalid_1's l1: 0.137196\n",
      "[13100]\ttraining's l1: 0.0641154\tvalid_1's l1: 0.137049\n",
      "[13200]\ttraining's l1: 0.0637709\tvalid_1's l1: 0.13692\n",
      "[13300]\ttraining's l1: 0.0634099\tvalid_1's l1: 0.136796\n",
      "[13400]\ttraining's l1: 0.0630628\tvalid_1's l1: 0.136684\n",
      "[13500]\ttraining's l1: 0.0627077\tvalid_1's l1: 0.136552\n",
      "[13600]\ttraining's l1: 0.0623618\tvalid_1's l1: 0.136421\n",
      "[13700]\ttraining's l1: 0.0620275\tvalid_1's l1: 0.136301\n",
      "[13800]\ttraining's l1: 0.0616824\tvalid_1's l1: 0.136191\n",
      "[13900]\ttraining's l1: 0.0613563\tvalid_1's l1: 0.136066\n",
      "[14000]\ttraining's l1: 0.0610228\tvalid_1's l1: 0.135947\n",
      "[14100]\ttraining's l1: 0.0606808\tvalid_1's l1: 0.135825\n",
      "[14200]\ttraining's l1: 0.0603616\tvalid_1's l1: 0.135713\n",
      "[14300]\ttraining's l1: 0.060038\tvalid_1's l1: 0.1356\n",
      "[14400]\ttraining's l1: 0.0597197\tvalid_1's l1: 0.135477\n",
      "[14500]\ttraining's l1: 0.0593992\tvalid_1's l1: 0.135362\n",
      "[14600]\ttraining's l1: 0.0590903\tvalid_1's l1: 0.13527\n",
      "[14700]\ttraining's l1: 0.0587733\tvalid_1's l1: 0.135163\n",
      "[14800]\ttraining's l1: 0.0584724\tvalid_1's l1: 0.135065\n",
      "[14900]\ttraining's l1: 0.0581577\tvalid_1's l1: 0.134957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0578551\tvalid_1's l1: 0.134859\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0578551\tvalid_1's l1: 0.134859\n",
      "3JHH Fold 2, logMAE: -2.0035580596821254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.380703\tvalid_1's l1: 0.383405\n",
      "[200]\ttraining's l1: 0.319611\tvalid_1's l1: 0.324562\n",
      "[300]\ttraining's l1: 0.288431\tvalid_1's l1: 0.295507\n",
      "[400]\ttraining's l1: 0.26894\tvalid_1's l1: 0.27771\n",
      "[500]\ttraining's l1: 0.254069\tvalid_1's l1: 0.26412\n",
      "[600]\ttraining's l1: 0.242293\tvalid_1's l1: 0.253918\n",
      "[700]\ttraining's l1: 0.231842\tvalid_1's l1: 0.244972\n",
      "[800]\ttraining's l1: 0.223571\tvalid_1's l1: 0.237973\n",
      "[900]\ttraining's l1: 0.215717\tvalid_1's l1: 0.231348\n",
      "[1000]\ttraining's l1: 0.208983\tvalid_1's l1: 0.225824\n",
      "[1100]\ttraining's l1: 0.202687\tvalid_1's l1: 0.220685\n",
      "[1200]\ttraining's l1: 0.19728\tvalid_1's l1: 0.216326\n",
      "[1300]\ttraining's l1: 0.19234\tvalid_1's l1: 0.212498\n",
      "[1400]\ttraining's l1: 0.187594\tvalid_1's l1: 0.208852\n",
      "[1500]\ttraining's l1: 0.183193\tvalid_1's l1: 0.205478\n",
      "[1600]\ttraining's l1: 0.179172\tvalid_1's l1: 0.202441\n",
      "[1700]\ttraining's l1: 0.175501\tvalid_1's l1: 0.199696\n",
      "[1800]\ttraining's l1: 0.172103\tvalid_1's l1: 0.197196\n",
      "[1900]\ttraining's l1: 0.168842\tvalid_1's l1: 0.194804\n",
      "[2000]\ttraining's l1: 0.165681\tvalid_1's l1: 0.192551\n",
      "[2100]\ttraining's l1: 0.162878\tvalid_1's l1: 0.190548\n",
      "[2200]\ttraining's l1: 0.160254\tvalid_1's l1: 0.188779\n",
      "[2300]\ttraining's l1: 0.157522\tvalid_1's l1: 0.186832\n",
      "[2400]\ttraining's l1: 0.155138\tvalid_1's l1: 0.185241\n",
      "[2500]\ttraining's l1: 0.152909\tvalid_1's l1: 0.183765\n",
      "[2600]\ttraining's l1: 0.150585\tvalid_1's l1: 0.182231\n",
      "[2700]\ttraining's l1: 0.148322\tvalid_1's l1: 0.180667\n",
      "[2800]\ttraining's l1: 0.14621\tvalid_1's l1: 0.179297\n",
      "[2900]\ttraining's l1: 0.144172\tvalid_1's l1: 0.177955\n",
      "[3000]\ttraining's l1: 0.142212\tvalid_1's l1: 0.176702\n",
      "[3100]\ttraining's l1: 0.140406\tvalid_1's l1: 0.175593\n",
      "[3200]\ttraining's l1: 0.138647\tvalid_1's l1: 0.174499\n",
      "[3300]\ttraining's l1: 0.136901\tvalid_1's l1: 0.173421\n",
      "[3400]\ttraining's l1: 0.135139\tvalid_1's l1: 0.172289\n",
      "[3500]\ttraining's l1: 0.13351\tvalid_1's l1: 0.171271\n",
      "[3600]\ttraining's l1: 0.13188\tvalid_1's l1: 0.170226\n",
      "[3700]\ttraining's l1: 0.13036\tvalid_1's l1: 0.16931\n",
      "[3800]\ttraining's l1: 0.12885\tvalid_1's l1: 0.168395\n",
      "[3900]\ttraining's l1: 0.127329\tvalid_1's l1: 0.167463\n",
      "[4000]\ttraining's l1: 0.125912\tvalid_1's l1: 0.166633\n",
      "[4100]\ttraining's l1: 0.124471\tvalid_1's l1: 0.165794\n",
      "[4200]\ttraining's l1: 0.123055\tvalid_1's l1: 0.164939\n",
      "[4300]\ttraining's l1: 0.121676\tvalid_1's l1: 0.164154\n",
      "[4400]\ttraining's l1: 0.120376\tvalid_1's l1: 0.163418\n",
      "[4500]\ttraining's l1: 0.119164\tvalid_1's l1: 0.162744\n",
      "[4600]\ttraining's l1: 0.117973\tvalid_1's l1: 0.162054\n",
      "[4700]\ttraining's l1: 0.116823\tvalid_1's l1: 0.161429\n",
      "[4800]\ttraining's l1: 0.115647\tvalid_1's l1: 0.160757\n",
      "[4900]\ttraining's l1: 0.114528\tvalid_1's l1: 0.160118\n",
      "[5000]\ttraining's l1: 0.113434\tvalid_1's l1: 0.15952\n",
      "[5100]\ttraining's l1: 0.112365\tvalid_1's l1: 0.158935\n",
      "[5200]\ttraining's l1: 0.111324\tvalid_1's l1: 0.158355\n",
      "[5300]\ttraining's l1: 0.110275\tvalid_1's l1: 0.157805\n",
      "[5400]\ttraining's l1: 0.109193\tvalid_1's l1: 0.157193\n",
      "[5500]\ttraining's l1: 0.108185\tvalid_1's l1: 0.156649\n",
      "[5600]\ttraining's l1: 0.107177\tvalid_1's l1: 0.156131\n",
      "[5700]\ttraining's l1: 0.106248\tvalid_1's l1: 0.155655\n",
      "[5800]\ttraining's l1: 0.105343\tvalid_1's l1: 0.155189\n",
      "[5900]\ttraining's l1: 0.104445\tvalid_1's l1: 0.154747\n",
      "[6000]\ttraining's l1: 0.103537\tvalid_1's l1: 0.154289\n",
      "[6100]\ttraining's l1: 0.102698\tvalid_1's l1: 0.153869\n",
      "[6200]\ttraining's l1: 0.101852\tvalid_1's l1: 0.153453\n",
      "[6300]\ttraining's l1: 0.101002\tvalid_1's l1: 0.153032\n",
      "[6400]\ttraining's l1: 0.100126\tvalid_1's l1: 0.152598\n",
      "[6500]\ttraining's l1: 0.0993247\tvalid_1's l1: 0.152207\n",
      "[6600]\ttraining's l1: 0.0984992\tvalid_1's l1: 0.151776\n",
      "[6700]\ttraining's l1: 0.0977235\tvalid_1's l1: 0.1514\n",
      "[6800]\ttraining's l1: 0.096974\tvalid_1's l1: 0.151042\n",
      "[6900]\ttraining's l1: 0.0962367\tvalid_1's l1: 0.150665\n",
      "[7000]\ttraining's l1: 0.0954898\tvalid_1's l1: 0.150276\n",
      "[7100]\ttraining's l1: 0.0947483\tvalid_1's l1: 0.149925\n",
      "[7200]\ttraining's l1: 0.0940253\tvalid_1's l1: 0.149557\n",
      "[7300]\ttraining's l1: 0.0933345\tvalid_1's l1: 0.149235\n",
      "[7400]\ttraining's l1: 0.0926427\tvalid_1's l1: 0.148891\n",
      "[7500]\ttraining's l1: 0.0919525\tvalid_1's l1: 0.148558\n",
      "[7600]\ttraining's l1: 0.0912691\tvalid_1's l1: 0.148236\n",
      "[7700]\ttraining's l1: 0.0905699\tvalid_1's l1: 0.147911\n",
      "[7800]\ttraining's l1: 0.0899083\tvalid_1's l1: 0.147592\n",
      "[7900]\ttraining's l1: 0.0892431\tvalid_1's l1: 0.147266\n",
      "[8000]\ttraining's l1: 0.0885727\tvalid_1's l1: 0.14694\n",
      "[8100]\ttraining's l1: 0.0879553\tvalid_1's l1: 0.146682\n",
      "[8200]\ttraining's l1: 0.0873241\tvalid_1's l1: 0.146402\n",
      "[8300]\ttraining's l1: 0.0867155\tvalid_1's l1: 0.146146\n",
      "[8400]\ttraining's l1: 0.0860589\tvalid_1's l1: 0.145846\n",
      "[8500]\ttraining's l1: 0.0854692\tvalid_1's l1: 0.145599\n",
      "[8600]\ttraining's l1: 0.0848845\tvalid_1's l1: 0.145339\n",
      "[8700]\ttraining's l1: 0.0843118\tvalid_1's l1: 0.145073\n",
      "[8800]\ttraining's l1: 0.0837494\tvalid_1's l1: 0.144836\n",
      "[8900]\ttraining's l1: 0.083198\tvalid_1's l1: 0.144609\n",
      "[9000]\ttraining's l1: 0.0826432\tvalid_1's l1: 0.144366\n",
      "[9100]\ttraining's l1: 0.0820692\tvalid_1's l1: 0.14411\n",
      "[9200]\ttraining's l1: 0.0815246\tvalid_1's l1: 0.143875\n",
      "[9300]\ttraining's l1: 0.0809927\tvalid_1's l1: 0.143659\n",
      "[9400]\ttraining's l1: 0.080477\tvalid_1's l1: 0.143432\n",
      "[9500]\ttraining's l1: 0.0799369\tvalid_1's l1: 0.143197\n",
      "[9600]\ttraining's l1: 0.079394\tvalid_1's l1: 0.142961\n",
      "[9700]\ttraining's l1: 0.0788846\tvalid_1's l1: 0.142727\n",
      "[9800]\ttraining's l1: 0.0783593\tvalid_1's l1: 0.142505\n",
      "[9900]\ttraining's l1: 0.0778635\tvalid_1's l1: 0.142299\n",
      "[10000]\ttraining's l1: 0.0773724\tvalid_1's l1: 0.142089\n",
      "[10100]\ttraining's l1: 0.0768738\tvalid_1's l1: 0.141874\n",
      "[10200]\ttraining's l1: 0.0764001\tvalid_1's l1: 0.141694\n",
      "[10300]\ttraining's l1: 0.0759239\tvalid_1's l1: 0.141487\n",
      "[10400]\ttraining's l1: 0.0754464\tvalid_1's l1: 0.141286\n",
      "[10500]\ttraining's l1: 0.0749723\tvalid_1's l1: 0.141087\n",
      "[10600]\ttraining's l1: 0.074518\tvalid_1's l1: 0.140894\n",
      "[10700]\ttraining's l1: 0.0740545\tvalid_1's l1: 0.140692\n",
      "[10800]\ttraining's l1: 0.073612\tvalid_1's l1: 0.140513\n",
      "[10900]\ttraining's l1: 0.0731642\tvalid_1's l1: 0.140329\n",
      "[11000]\ttraining's l1: 0.0727039\tvalid_1's l1: 0.140129\n",
      "[11100]\ttraining's l1: 0.072253\tvalid_1's l1: 0.139958\n",
      "[11200]\ttraining's l1: 0.07182\tvalid_1's l1: 0.139794\n",
      "[11300]\ttraining's l1: 0.0713766\tvalid_1's l1: 0.139613\n",
      "[11400]\ttraining's l1: 0.0709759\tvalid_1's l1: 0.139456\n",
      "[11500]\ttraining's l1: 0.0705488\tvalid_1's l1: 0.139278\n",
      "[11600]\ttraining's l1: 0.0701402\tvalid_1's l1: 0.139126\n",
      "[11700]\ttraining's l1: 0.069715\tvalid_1's l1: 0.138938\n",
      "[11800]\ttraining's l1: 0.0693136\tvalid_1's l1: 0.138788\n",
      "[11900]\ttraining's l1: 0.0689069\tvalid_1's l1: 0.138629\n",
      "[12000]\ttraining's l1: 0.0684914\tvalid_1's l1: 0.138459\n",
      "[12100]\ttraining's l1: 0.0680948\tvalid_1's l1: 0.138318\n",
      "[12200]\ttraining's l1: 0.0677096\tvalid_1's l1: 0.138179\n",
      "[12300]\ttraining's l1: 0.0673167\tvalid_1's l1: 0.138029\n",
      "[12400]\ttraining's l1: 0.0669237\tvalid_1's l1: 0.137879\n",
      "[12500]\ttraining's l1: 0.0665402\tvalid_1's l1: 0.137734\n",
      "[12600]\ttraining's l1: 0.0661654\tvalid_1's l1: 0.137583\n",
      "[12700]\ttraining's l1: 0.0657951\tvalid_1's l1: 0.137456\n",
      "[12800]\ttraining's l1: 0.0654219\tvalid_1's l1: 0.137308\n",
      "[12900]\ttraining's l1: 0.0650679\tvalid_1's l1: 0.137181\n",
      "[13000]\ttraining's l1: 0.0647214\tvalid_1's l1: 0.137058\n",
      "[13100]\ttraining's l1: 0.0643489\tvalid_1's l1: 0.136914\n",
      "[13200]\ttraining's l1: 0.0639899\tvalid_1's l1: 0.136775\n",
      "[13300]\ttraining's l1: 0.0636328\tvalid_1's l1: 0.136652\n",
      "[13400]\ttraining's l1: 0.0632781\tvalid_1's l1: 0.136515\n",
      "[13500]\ttraining's l1: 0.0629316\tvalid_1's l1: 0.136375\n",
      "[13600]\ttraining's l1: 0.0625944\tvalid_1's l1: 0.136252\n",
      "[13700]\ttraining's l1: 0.062268\tvalid_1's l1: 0.13614\n",
      "[13800]\ttraining's l1: 0.0619312\tvalid_1's l1: 0.136021\n",
      "[13900]\ttraining's l1: 0.061583\tvalid_1's l1: 0.13589\n",
      "[14000]\ttraining's l1: 0.0612476\tvalid_1's l1: 0.135769\n",
      "[14100]\ttraining's l1: 0.0609172\tvalid_1's l1: 0.135652\n",
      "[14200]\ttraining's l1: 0.0605723\tvalid_1's l1: 0.135522\n",
      "[14300]\ttraining's l1: 0.0602496\tvalid_1's l1: 0.135416\n",
      "[14400]\ttraining's l1: 0.0599344\tvalid_1's l1: 0.135312\n",
      "[14500]\ttraining's l1: 0.0596105\tvalid_1's l1: 0.135184\n",
      "[14600]\ttraining's l1: 0.0592912\tvalid_1's l1: 0.135072\n",
      "[14700]\ttraining's l1: 0.058982\tvalid_1's l1: 0.134962\n",
      "[14800]\ttraining's l1: 0.0586872\tvalid_1's l1: 0.134859\n",
      "[14900]\ttraining's l1: 0.05839\tvalid_1's l1: 0.134758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0580877\tvalid_1's l1: 0.134645\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0580877\tvalid_1's l1: 0.134645\n",
      "3JHH Fold 3, logMAE: -2.0050935430519545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.379574\tvalid_1's l1: 0.382308\n",
      "[200]\ttraining's l1: 0.318597\tvalid_1's l1: 0.32286\n",
      "[300]\ttraining's l1: 0.288961\tvalid_1's l1: 0.294859\n",
      "[400]\ttraining's l1: 0.26903\tvalid_1's l1: 0.276513\n",
      "[500]\ttraining's l1: 0.254667\tvalid_1's l1: 0.263507\n",
      "[600]\ttraining's l1: 0.24226\tvalid_1's l1: 0.252411\n",
      "[700]\ttraining's l1: 0.232258\tvalid_1's l1: 0.243843\n",
      "[800]\ttraining's l1: 0.223712\tvalid_1's l1: 0.236615\n",
      "[900]\ttraining's l1: 0.215865\tvalid_1's l1: 0.229971\n",
      "[1000]\ttraining's l1: 0.209052\tvalid_1's l1: 0.224304\n",
      "[1100]\ttraining's l1: 0.203313\tvalid_1's l1: 0.219728\n",
      "[1200]\ttraining's l1: 0.197736\tvalid_1's l1: 0.215256\n",
      "[1300]\ttraining's l1: 0.192686\tvalid_1's l1: 0.211327\n",
      "[1400]\ttraining's l1: 0.187984\tvalid_1's l1: 0.207582\n",
      "[1500]\ttraining's l1: 0.183822\tvalid_1's l1: 0.204447\n",
      "[1600]\ttraining's l1: 0.180033\tvalid_1's l1: 0.201561\n",
      "[1700]\ttraining's l1: 0.176212\tvalid_1's l1: 0.198708\n",
      "[1800]\ttraining's l1: 0.1728\tvalid_1's l1: 0.196245\n",
      "[1900]\ttraining's l1: 0.169639\tvalid_1's l1: 0.193949\n",
      "[2000]\ttraining's l1: 0.166479\tvalid_1's l1: 0.191659\n",
      "[2100]\ttraining's l1: 0.163586\tvalid_1's l1: 0.189629\n",
      "[2200]\ttraining's l1: 0.160854\tvalid_1's l1: 0.187703\n",
      "[2300]\ttraining's l1: 0.158313\tvalid_1's l1: 0.186052\n",
      "[2400]\ttraining's l1: 0.155976\tvalid_1's l1: 0.184563\n",
      "[2500]\ttraining's l1: 0.153571\tvalid_1's l1: 0.182889\n",
      "[2600]\ttraining's l1: 0.151234\tvalid_1's l1: 0.181318\n",
      "[2700]\ttraining's l1: 0.149074\tvalid_1's l1: 0.179901\n",
      "[2800]\ttraining's l1: 0.146975\tvalid_1's l1: 0.178478\n",
      "[2900]\ttraining's l1: 0.144901\tvalid_1's l1: 0.177128\n",
      "[3000]\ttraining's l1: 0.143015\tvalid_1's l1: 0.175965\n",
      "[3100]\ttraining's l1: 0.141135\tvalid_1's l1: 0.174763\n",
      "[3200]\ttraining's l1: 0.1393\tvalid_1's l1: 0.173644\n",
      "[3300]\ttraining's l1: 0.13749\tvalid_1's l1: 0.172507\n",
      "[3400]\ttraining's l1: 0.135821\tvalid_1's l1: 0.171473\n",
      "[3500]\ttraining's l1: 0.134174\tvalid_1's l1: 0.170462\n",
      "[3600]\ttraining's l1: 0.132514\tvalid_1's l1: 0.169446\n",
      "[3700]\ttraining's l1: 0.13095\tvalid_1's l1: 0.168486\n",
      "[3800]\ttraining's l1: 0.129385\tvalid_1's l1: 0.167543\n",
      "[3900]\ttraining's l1: 0.127924\tvalid_1's l1: 0.166741\n",
      "[4000]\ttraining's l1: 0.126548\tvalid_1's l1: 0.165976\n",
      "[4100]\ttraining's l1: 0.125143\tvalid_1's l1: 0.165162\n",
      "[4200]\ttraining's l1: 0.123828\tvalid_1's l1: 0.164419\n",
      "[4300]\ttraining's l1: 0.122522\tvalid_1's l1: 0.163657\n",
      "[4400]\ttraining's l1: 0.121232\tvalid_1's l1: 0.162949\n",
      "[4500]\ttraining's l1: 0.120027\tvalid_1's l1: 0.162273\n",
      "[4600]\ttraining's l1: 0.11879\tvalid_1's l1: 0.161606\n",
      "[4700]\ttraining's l1: 0.117617\tvalid_1's l1: 0.160953\n",
      "[4800]\ttraining's l1: 0.11646\tvalid_1's l1: 0.160316\n",
      "[4900]\ttraining's l1: 0.115295\tvalid_1's l1: 0.159658\n",
      "[5000]\ttraining's l1: 0.114168\tvalid_1's l1: 0.159057\n",
      "[5100]\ttraining's l1: 0.113102\tvalid_1's l1: 0.158498\n",
      "[5200]\ttraining's l1: 0.112094\tvalid_1's l1: 0.157961\n",
      "[5300]\ttraining's l1: 0.111011\tvalid_1's l1: 0.157313\n",
      "[5400]\ttraining's l1: 0.110025\tvalid_1's l1: 0.15678\n",
      "[5500]\ttraining's l1: 0.10906\tvalid_1's l1: 0.156237\n",
      "[5600]\ttraining's l1: 0.108101\tvalid_1's l1: 0.155755\n",
      "[5700]\ttraining's l1: 0.107136\tvalid_1's l1: 0.15527\n",
      "[5800]\ttraining's l1: 0.106236\tvalid_1's l1: 0.154793\n",
      "[5900]\ttraining's l1: 0.105254\tvalid_1's l1: 0.154284\n",
      "[6000]\ttraining's l1: 0.104349\tvalid_1's l1: 0.153817\n",
      "[6100]\ttraining's l1: 0.103483\tvalid_1's l1: 0.153379\n",
      "[6200]\ttraining's l1: 0.102601\tvalid_1's l1: 0.152902\n",
      "[6300]\ttraining's l1: 0.101736\tvalid_1's l1: 0.152476\n",
      "[6400]\ttraining's l1: 0.100888\tvalid_1's l1: 0.152038\n",
      "[6500]\ttraining's l1: 0.100066\tvalid_1's l1: 0.15164\n",
      "[6600]\ttraining's l1: 0.0992778\tvalid_1's l1: 0.151242\n",
      "[6700]\ttraining's l1: 0.0984865\tvalid_1's l1: 0.150864\n",
      "[6800]\ttraining's l1: 0.097679\tvalid_1's l1: 0.150481\n",
      "[6900]\ttraining's l1: 0.0968829\tvalid_1's l1: 0.150101\n",
      "[7000]\ttraining's l1: 0.0961103\tvalid_1's l1: 0.149725\n",
      "[7100]\ttraining's l1: 0.0953506\tvalid_1's l1: 0.149387\n",
      "[7200]\ttraining's l1: 0.0946123\tvalid_1's l1: 0.149044\n",
      "[7300]\ttraining's l1: 0.0938557\tvalid_1's l1: 0.148665\n",
      "[7400]\ttraining's l1: 0.0931443\tvalid_1's l1: 0.148334\n",
      "[7500]\ttraining's l1: 0.0924349\tvalid_1's l1: 0.147979\n",
      "[7600]\ttraining's l1: 0.0917283\tvalid_1's l1: 0.14765\n",
      "[7700]\ttraining's l1: 0.0910121\tvalid_1's l1: 0.147295\n",
      "[7800]\ttraining's l1: 0.0903171\tvalid_1's l1: 0.146981\n",
      "[7900]\ttraining's l1: 0.0896505\tvalid_1's l1: 0.146666\n",
      "[8000]\ttraining's l1: 0.0889995\tvalid_1's l1: 0.146371\n",
      "[8100]\ttraining's l1: 0.0883461\tvalid_1's l1: 0.14606\n",
      "[8200]\ttraining's l1: 0.0877255\tvalid_1's l1: 0.145773\n",
      "[8300]\ttraining's l1: 0.0871291\tvalid_1's l1: 0.145497\n",
      "[8400]\ttraining's l1: 0.0865129\tvalid_1's l1: 0.145221\n",
      "[8500]\ttraining's l1: 0.0859074\tvalid_1's l1: 0.144962\n",
      "[8600]\ttraining's l1: 0.0853149\tvalid_1's l1: 0.144707\n",
      "[8700]\ttraining's l1: 0.0847176\tvalid_1's l1: 0.144448\n",
      "[8800]\ttraining's l1: 0.0841349\tvalid_1's l1: 0.144194\n",
      "[8900]\ttraining's l1: 0.0835876\tvalid_1's l1: 0.143966\n",
      "[9000]\ttraining's l1: 0.0830308\tvalid_1's l1: 0.143725\n",
      "[9100]\ttraining's l1: 0.082462\tvalid_1's l1: 0.143463\n",
      "[9200]\ttraining's l1: 0.0819072\tvalid_1's l1: 0.143211\n",
      "[9300]\ttraining's l1: 0.0813486\tvalid_1's l1: 0.14299\n",
      "[9400]\ttraining's l1: 0.0808246\tvalid_1's l1: 0.142765\n",
      "[9500]\ttraining's l1: 0.0802987\tvalid_1's l1: 0.142559\n",
      "[9600]\ttraining's l1: 0.0797772\tvalid_1's l1: 0.142326\n",
      "[9700]\ttraining's l1: 0.079282\tvalid_1's l1: 0.142118\n",
      "[9800]\ttraining's l1: 0.0787687\tvalid_1's l1: 0.141914\n",
      "[9900]\ttraining's l1: 0.0782713\tvalid_1's l1: 0.141694\n",
      "[10000]\ttraining's l1: 0.0777824\tvalid_1's l1: 0.141489\n",
      "[10100]\ttraining's l1: 0.077258\tvalid_1's l1: 0.141261\n",
      "[10200]\ttraining's l1: 0.0767844\tvalid_1's l1: 0.141059\n",
      "[10300]\ttraining's l1: 0.0763186\tvalid_1's l1: 0.140876\n",
      "[10400]\ttraining's l1: 0.0758409\tvalid_1's l1: 0.140678\n",
      "[10500]\ttraining's l1: 0.0753804\tvalid_1's l1: 0.140486\n",
      "[10600]\ttraining's l1: 0.0749103\tvalid_1's l1: 0.140292\n",
      "[10700]\ttraining's l1: 0.0744208\tvalid_1's l1: 0.140084\n",
      "[10800]\ttraining's l1: 0.0739572\tvalid_1's l1: 0.139892\n",
      "[10900]\ttraining's l1: 0.0734845\tvalid_1's l1: 0.139691\n",
      "[11000]\ttraining's l1: 0.0730457\tvalid_1's l1: 0.139528\n",
      "[11100]\ttraining's l1: 0.0725905\tvalid_1's l1: 0.139354\n",
      "[11200]\ttraining's l1: 0.0721592\tvalid_1's l1: 0.139201\n",
      "[11300]\ttraining's l1: 0.0717191\tvalid_1's l1: 0.139028\n",
      "[11400]\ttraining's l1: 0.0712723\tvalid_1's l1: 0.138857\n",
      "[11500]\ttraining's l1: 0.0708479\tvalid_1's l1: 0.138692\n",
      "[11600]\ttraining's l1: 0.0704202\tvalid_1's l1: 0.138517\n",
      "[11700]\ttraining's l1: 0.0700151\tvalid_1's l1: 0.13835\n",
      "[11800]\ttraining's l1: 0.0696098\tvalid_1's l1: 0.138212\n",
      "[11900]\ttraining's l1: 0.0692085\tvalid_1's l1: 0.138049\n",
      "[12000]\ttraining's l1: 0.0688092\tvalid_1's l1: 0.137897\n",
      "[12100]\ttraining's l1: 0.0684116\tvalid_1's l1: 0.13773\n",
      "[12200]\ttraining's l1: 0.0679818\tvalid_1's l1: 0.137558\n",
      "[12300]\ttraining's l1: 0.0675884\tvalid_1's l1: 0.137416\n",
      "[12400]\ttraining's l1: 0.0672201\tvalid_1's l1: 0.137277\n",
      "[12500]\ttraining's l1: 0.0668585\tvalid_1's l1: 0.137122\n",
      "[12600]\ttraining's l1: 0.0664863\tvalid_1's l1: 0.13698\n",
      "[12700]\ttraining's l1: 0.0661268\tvalid_1's l1: 0.136848\n",
      "[12800]\ttraining's l1: 0.0657548\tvalid_1's l1: 0.136693\n",
      "[12900]\ttraining's l1: 0.0653922\tvalid_1's l1: 0.136566\n",
      "[13000]\ttraining's l1: 0.0650298\tvalid_1's l1: 0.136433\n",
      "[13100]\ttraining's l1: 0.0646839\tvalid_1's l1: 0.136312\n",
      "[13200]\ttraining's l1: 0.0643165\tvalid_1's l1: 0.136167\n",
      "[13300]\ttraining's l1: 0.0639643\tvalid_1's l1: 0.136036\n",
      "[13400]\ttraining's l1: 0.0636132\tvalid_1's l1: 0.135909\n",
      "[13500]\ttraining's l1: 0.0632621\tvalid_1's l1: 0.135784\n",
      "[13600]\ttraining's l1: 0.0629121\tvalid_1's l1: 0.135658\n",
      "[13700]\ttraining's l1: 0.0625367\tvalid_1's l1: 0.135517\n",
      "[13800]\ttraining's l1: 0.0621946\tvalid_1's l1: 0.135386\n",
      "[13900]\ttraining's l1: 0.0618507\tvalid_1's l1: 0.135262\n",
      "[14000]\ttraining's l1: 0.0615194\tvalid_1's l1: 0.135143\n",
      "[14100]\ttraining's l1: 0.0611809\tvalid_1's l1: 0.135013\n",
      "[14200]\ttraining's l1: 0.0608541\tvalid_1's l1: 0.13489\n",
      "[14300]\ttraining's l1: 0.0605163\tvalid_1's l1: 0.13476\n",
      "[14400]\ttraining's l1: 0.0601714\tvalid_1's l1: 0.134631\n",
      "[14500]\ttraining's l1: 0.0598627\tvalid_1's l1: 0.134525\n",
      "[14600]\ttraining's l1: 0.0595321\tvalid_1's l1: 0.134422\n",
      "[14700]\ttraining's l1: 0.059224\tvalid_1's l1: 0.134325\n",
      "[14800]\ttraining's l1: 0.0589048\tvalid_1's l1: 0.134226\n",
      "[14900]\ttraining's l1: 0.0585982\tvalid_1's l1: 0.134117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0582905\tvalid_1's l1: 0.134014\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0582905\tvalid_1's l1: 0.134014\n",
      "3JHH Fold 4, logMAE: -2.0098080705694343\n",
      "*** Training Model for 3JHC ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.75675\tvalid_1's l1: 0.7586\n",
      "[200]\ttraining's l1: 0.656663\tvalid_1's l1: 0.661371\n",
      "[300]\ttraining's l1: 0.604582\tvalid_1's l1: 0.611244\n",
      "[400]\ttraining's l1: 0.569169\tvalid_1's l1: 0.577726\n",
      "[500]\ttraining's l1: 0.541757\tvalid_1's l1: 0.551924\n",
      "[600]\ttraining's l1: 0.518778\tvalid_1's l1: 0.530392\n",
      "[700]\ttraining's l1: 0.500728\tvalid_1's l1: 0.513931\n",
      "[800]\ttraining's l1: 0.485576\tvalid_1's l1: 0.499936\n",
      "[900]\ttraining's l1: 0.471816\tvalid_1's l1: 0.487378\n",
      "[1000]\ttraining's l1: 0.460479\tvalid_1's l1: 0.477197\n",
      "[1100]\ttraining's l1: 0.449898\tvalid_1's l1: 0.467738\n",
      "[1200]\ttraining's l1: 0.440026\tvalid_1's l1: 0.458883\n",
      "[1300]\ttraining's l1: 0.431674\tvalid_1's l1: 0.451535\n",
      "[1400]\ttraining's l1: 0.424047\tvalid_1's l1: 0.444918\n",
      "[1500]\ttraining's l1: 0.416809\tvalid_1's l1: 0.438617\n",
      "[1600]\ttraining's l1: 0.410252\tvalid_1's l1: 0.432945\n",
      "[1700]\ttraining's l1: 0.404017\tvalid_1's l1: 0.42759\n",
      "[1800]\ttraining's l1: 0.398401\tvalid_1's l1: 0.422886\n",
      "[1900]\ttraining's l1: 0.392658\tvalid_1's l1: 0.417911\n",
      "[2000]\ttraining's l1: 0.387502\tvalid_1's l1: 0.413604\n",
      "[2100]\ttraining's l1: 0.382584\tvalid_1's l1: 0.409536\n",
      "[2200]\ttraining's l1: 0.377861\tvalid_1's l1: 0.40562\n",
      "[2300]\ttraining's l1: 0.373408\tvalid_1's l1: 0.402003\n",
      "[2400]\ttraining's l1: 0.36906\tvalid_1's l1: 0.398376\n",
      "[2500]\ttraining's l1: 0.365378\tvalid_1's l1: 0.395546\n",
      "[2600]\ttraining's l1: 0.361534\tvalid_1's l1: 0.392503\n",
      "[2700]\ttraining's l1: 0.357714\tvalid_1's l1: 0.389468\n",
      "[2800]\ttraining's l1: 0.354191\tvalid_1's l1: 0.386632\n",
      "[2900]\ttraining's l1: 0.350703\tvalid_1's l1: 0.383904\n",
      "[3000]\ttraining's l1: 0.347572\tvalid_1's l1: 0.381484\n",
      "[3100]\ttraining's l1: 0.344283\tvalid_1's l1: 0.378884\n",
      "[3200]\ttraining's l1: 0.341241\tvalid_1's l1: 0.376532\n",
      "[3300]\ttraining's l1: 0.33836\tvalid_1's l1: 0.374413\n",
      "[3400]\ttraining's l1: 0.335535\tvalid_1's l1: 0.372227\n",
      "[3500]\ttraining's l1: 0.33281\tvalid_1's l1: 0.370218\n",
      "[3600]\ttraining's l1: 0.330157\tvalid_1's l1: 0.368271\n",
      "[3700]\ttraining's l1: 0.327498\tvalid_1's l1: 0.366261\n",
      "[3800]\ttraining's l1: 0.324944\tvalid_1's l1: 0.36437\n",
      "[3900]\ttraining's l1: 0.322461\tvalid_1's l1: 0.362532\n",
      "[4000]\ttraining's l1: 0.31994\tvalid_1's l1: 0.360646\n",
      "[4100]\ttraining's l1: 0.317632\tvalid_1's l1: 0.358965\n",
      "[4200]\ttraining's l1: 0.315444\tvalid_1's l1: 0.357384\n",
      "[4300]\ttraining's l1: 0.313119\tvalid_1's l1: 0.355724\n",
      "[4400]\ttraining's l1: 0.310853\tvalid_1's l1: 0.354115\n",
      "[4500]\ttraining's l1: 0.308553\tvalid_1's l1: 0.352401\n",
      "[4600]\ttraining's l1: 0.306442\tvalid_1's l1: 0.350914\n",
      "[4700]\ttraining's l1: 0.304298\tvalid_1's l1: 0.349362\n",
      "[4800]\ttraining's l1: 0.302351\tvalid_1's l1: 0.347992\n",
      "[4900]\ttraining's l1: 0.300358\tvalid_1's l1: 0.346605\n",
      "[5000]\ttraining's l1: 0.298476\tvalid_1's l1: 0.345315\n",
      "[5100]\ttraining's l1: 0.296589\tvalid_1's l1: 0.343987\n",
      "[5200]\ttraining's l1: 0.294659\tvalid_1's l1: 0.342622\n",
      "[5300]\ttraining's l1: 0.292767\tvalid_1's l1: 0.341282\n",
      "[5400]\ttraining's l1: 0.290909\tvalid_1's l1: 0.339999\n",
      "[5500]\ttraining's l1: 0.289146\tvalid_1's l1: 0.338784\n",
      "[5600]\ttraining's l1: 0.287279\tvalid_1's l1: 0.337442\n",
      "[5700]\ttraining's l1: 0.285675\tvalid_1's l1: 0.336409\n",
      "[5800]\ttraining's l1: 0.284014\tvalid_1's l1: 0.335249\n",
      "[5900]\ttraining's l1: 0.282428\tvalid_1's l1: 0.334208\n",
      "[6000]\ttraining's l1: 0.280756\tvalid_1's l1: 0.33311\n",
      "[6100]\ttraining's l1: 0.279169\tvalid_1's l1: 0.332039\n",
      "[6200]\ttraining's l1: 0.277677\tvalid_1's l1: 0.331073\n",
      "[6300]\ttraining's l1: 0.276159\tvalid_1's l1: 0.330105\n",
      "[6400]\ttraining's l1: 0.27461\tvalid_1's l1: 0.329088\n",
      "[6500]\ttraining's l1: 0.273206\tvalid_1's l1: 0.328209\n",
      "[6600]\ttraining's l1: 0.271713\tvalid_1's l1: 0.327252\n",
      "[6700]\ttraining's l1: 0.270341\tvalid_1's l1: 0.326387\n",
      "[6800]\ttraining's l1: 0.268911\tvalid_1's l1: 0.325513\n",
      "[6900]\ttraining's l1: 0.267525\tvalid_1's l1: 0.324653\n",
      "[7000]\ttraining's l1: 0.266139\tvalid_1's l1: 0.323764\n",
      "[7100]\ttraining's l1: 0.264812\tvalid_1's l1: 0.322922\n",
      "[7200]\ttraining's l1: 0.263482\tvalid_1's l1: 0.322069\n",
      "[7300]\ttraining's l1: 0.262103\tvalid_1's l1: 0.321164\n",
      "[7400]\ttraining's l1: 0.2608\tvalid_1's l1: 0.320364\n",
      "[7500]\ttraining's l1: 0.259568\tvalid_1's l1: 0.319627\n",
      "[7600]\ttraining's l1: 0.258365\tvalid_1's l1: 0.318895\n",
      "[7700]\ttraining's l1: 0.257098\tvalid_1's l1: 0.318081\n",
      "[7800]\ttraining's l1: 0.255894\tvalid_1's l1: 0.317313\n",
      "[7900]\ttraining's l1: 0.254715\tvalid_1's l1: 0.316603\n",
      "[8000]\ttraining's l1: 0.253512\tvalid_1's l1: 0.315868\n",
      "[8100]\ttraining's l1: 0.252348\tvalid_1's l1: 0.315171\n",
      "[8200]\ttraining's l1: 0.251141\tvalid_1's l1: 0.314431\n",
      "[8300]\ttraining's l1: 0.249941\tvalid_1's l1: 0.313699\n",
      "[8400]\ttraining's l1: 0.248857\tvalid_1's l1: 0.31311\n",
      "[8500]\ttraining's l1: 0.247792\tvalid_1's l1: 0.312492\n",
      "[8600]\ttraining's l1: 0.246693\tvalid_1's l1: 0.311856\n",
      "[8700]\ttraining's l1: 0.2456\tvalid_1's l1: 0.311224\n",
      "[8800]\ttraining's l1: 0.244528\tvalid_1's l1: 0.310634\n",
      "[8900]\ttraining's l1: 0.243461\tvalid_1's l1: 0.310007\n",
      "[9000]\ttraining's l1: 0.242388\tvalid_1's l1: 0.309363\n",
      "[9100]\ttraining's l1: 0.241312\tvalid_1's l1: 0.308739\n",
      "[9200]\ttraining's l1: 0.240254\tvalid_1's l1: 0.308127\n",
      "[9300]\ttraining's l1: 0.239251\tvalid_1's l1: 0.307583\n",
      "[9400]\ttraining's l1: 0.23826\tvalid_1's l1: 0.307015\n",
      "[9500]\ttraining's l1: 0.237267\tvalid_1's l1: 0.306465\n",
      "[9600]\ttraining's l1: 0.236282\tvalid_1's l1: 0.30591\n",
      "[9700]\ttraining's l1: 0.235235\tvalid_1's l1: 0.305271\n",
      "[9800]\ttraining's l1: 0.234304\tvalid_1's l1: 0.304744\n",
      "[9900]\ttraining's l1: 0.233333\tvalid_1's l1: 0.304204\n",
      "[10000]\ttraining's l1: 0.232351\tvalid_1's l1: 0.303612\n",
      "[10100]\ttraining's l1: 0.231432\tvalid_1's l1: 0.303107\n",
      "[10200]\ttraining's l1: 0.230517\tvalid_1's l1: 0.302585\n",
      "[10300]\ttraining's l1: 0.229639\tvalid_1's l1: 0.302087\n",
      "[10400]\ttraining's l1: 0.228774\tvalid_1's l1: 0.301616\n",
      "[10500]\ttraining's l1: 0.227855\tvalid_1's l1: 0.301099\n",
      "[10600]\ttraining's l1: 0.226945\tvalid_1's l1: 0.300577\n",
      "[10700]\ttraining's l1: 0.226057\tvalid_1's l1: 0.300055\n",
      "[10800]\ttraining's l1: 0.225273\tvalid_1's l1: 0.299632\n",
      "[10900]\ttraining's l1: 0.224441\tvalid_1's l1: 0.299221\n",
      "[11000]\ttraining's l1: 0.223599\tvalid_1's l1: 0.2988\n",
      "[11100]\ttraining's l1: 0.222761\tvalid_1's l1: 0.298346\n",
      "[11200]\ttraining's l1: 0.221949\tvalid_1's l1: 0.297904\n",
      "[11300]\ttraining's l1: 0.221093\tvalid_1's l1: 0.297399\n",
      "[11400]\ttraining's l1: 0.220302\tvalid_1's l1: 0.297\n",
      "[11500]\ttraining's l1: 0.219489\tvalid_1's l1: 0.296568\n",
      "[11600]\ttraining's l1: 0.218682\tvalid_1's l1: 0.296151\n",
      "[11700]\ttraining's l1: 0.217875\tvalid_1's l1: 0.29575\n",
      "[11800]\ttraining's l1: 0.217101\tvalid_1's l1: 0.29535\n",
      "[11900]\ttraining's l1: 0.216304\tvalid_1's l1: 0.29493\n",
      "[12000]\ttraining's l1: 0.215561\tvalid_1's l1: 0.29454\n",
      "[12100]\ttraining's l1: 0.214766\tvalid_1's l1: 0.294129\n",
      "[12200]\ttraining's l1: 0.214\tvalid_1's l1: 0.293723\n",
      "[12300]\ttraining's l1: 0.213262\tvalid_1's l1: 0.293343\n",
      "[12400]\ttraining's l1: 0.212479\tvalid_1's l1: 0.292935\n",
      "[12500]\ttraining's l1: 0.211735\tvalid_1's l1: 0.292552\n",
      "[12600]\ttraining's l1: 0.211032\tvalid_1's l1: 0.292196\n",
      "[12700]\ttraining's l1: 0.210288\tvalid_1's l1: 0.291809\n",
      "[12800]\ttraining's l1: 0.209576\tvalid_1's l1: 0.291468\n",
      "[12900]\ttraining's l1: 0.208871\tvalid_1's l1: 0.29111\n",
      "[13000]\ttraining's l1: 0.208144\tvalid_1's l1: 0.290743\n",
      "[13100]\ttraining's l1: 0.207427\tvalid_1's l1: 0.29036\n",
      "[13200]\ttraining's l1: 0.206738\tvalid_1's l1: 0.290015\n",
      "[13300]\ttraining's l1: 0.206043\tvalid_1's l1: 0.289658\n",
      "[13400]\ttraining's l1: 0.205367\tvalid_1's l1: 0.289315\n",
      "[13500]\ttraining's l1: 0.20472\tvalid_1's l1: 0.289005\n",
      "[13600]\ttraining's l1: 0.204025\tvalid_1's l1: 0.288648\n",
      "[13700]\ttraining's l1: 0.203383\tvalid_1's l1: 0.288365\n",
      "[13800]\ttraining's l1: 0.202717\tvalid_1's l1: 0.288059\n",
      "[13900]\ttraining's l1: 0.202024\tvalid_1's l1: 0.287717\n",
      "[14000]\ttraining's l1: 0.201297\tvalid_1's l1: 0.287347\n",
      "[14100]\ttraining's l1: 0.200628\tvalid_1's l1: 0.287051\n",
      "[14200]\ttraining's l1: 0.199991\tvalid_1's l1: 0.286733\n",
      "[14300]\ttraining's l1: 0.199354\tvalid_1's l1: 0.286425\n",
      "[14400]\ttraining's l1: 0.198739\tvalid_1's l1: 0.286142\n",
      "[14500]\ttraining's l1: 0.198087\tvalid_1's l1: 0.285808\n",
      "[14600]\ttraining's l1: 0.197436\tvalid_1's l1: 0.285506\n",
      "[14700]\ttraining's l1: 0.196823\tvalid_1's l1: 0.285215\n",
      "[14800]\ttraining's l1: 0.196184\tvalid_1's l1: 0.284918\n",
      "[14900]\ttraining's l1: 0.195551\tvalid_1's l1: 0.284613\n",
      "[15000]\ttraining's l1: 0.19496\tvalid_1's l1: 0.284344\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.19496\tvalid_1's l1: 0.284344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3JHC Fold 0, logMAE: -1.2575690817773812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.754785\tvalid_1's l1: 0.756464\n",
      "[200]\ttraining's l1: 0.655291\tvalid_1's l1: 0.658219\n",
      "[300]\ttraining's l1: 0.60353\tvalid_1's l1: 0.607626\n",
      "[400]\ttraining's l1: 0.566568\tvalid_1's l1: 0.5719\n",
      "[500]\ttraining's l1: 0.538824\tvalid_1's l1: 0.54565\n",
      "[600]\ttraining's l1: 0.517555\tvalid_1's l1: 0.525734\n",
      "[700]\ttraining's l1: 0.499342\tvalid_1's l1: 0.508785\n",
      "[800]\ttraining's l1: 0.484854\tvalid_1's l1: 0.495434\n",
      "[900]\ttraining's l1: 0.471189\tvalid_1's l1: 0.482889\n",
      "[1000]\ttraining's l1: 0.459986\tvalid_1's l1: 0.47277\n",
      "[1100]\ttraining's l1: 0.449901\tvalid_1's l1: 0.463677\n",
      "[1200]\ttraining's l1: 0.440206\tvalid_1's l1: 0.454958\n",
      "[1300]\ttraining's l1: 0.431588\tvalid_1's l1: 0.447303\n",
      "[1400]\ttraining's l1: 0.423823\tvalid_1's l1: 0.440452\n",
      "[1500]\ttraining's l1: 0.416507\tvalid_1's l1: 0.434079\n",
      "[1600]\ttraining's l1: 0.410027\tvalid_1's l1: 0.428591\n",
      "[1700]\ttraining's l1: 0.403905\tvalid_1's l1: 0.423418\n",
      "[1800]\ttraining's l1: 0.397992\tvalid_1's l1: 0.418463\n",
      "[1900]\ttraining's l1: 0.392553\tvalid_1's l1: 0.413879\n",
      "[2000]\ttraining's l1: 0.38764\tvalid_1's l1: 0.409773\n",
      "[2100]\ttraining's l1: 0.382634\tvalid_1's l1: 0.405613\n",
      "[2200]\ttraining's l1: 0.377912\tvalid_1's l1: 0.401738\n",
      "[2300]\ttraining's l1: 0.373508\tvalid_1's l1: 0.398119\n",
      "[2400]\ttraining's l1: 0.36931\tvalid_1's l1: 0.394773\n",
      "[2500]\ttraining's l1: 0.3651\tvalid_1's l1: 0.391429\n",
      "[2600]\ttraining's l1: 0.361204\tvalid_1's l1: 0.388335\n",
      "[2700]\ttraining's l1: 0.357613\tvalid_1's l1: 0.385484\n",
      "[2800]\ttraining's l1: 0.354112\tvalid_1's l1: 0.382746\n",
      "[2900]\ttraining's l1: 0.350578\tvalid_1's l1: 0.380006\n",
      "[3000]\ttraining's l1: 0.347279\tvalid_1's l1: 0.377545\n",
      "[3100]\ttraining's l1: 0.344144\tvalid_1's l1: 0.375118\n",
      "[3200]\ttraining's l1: 0.341097\tvalid_1's l1: 0.372832\n",
      "[3300]\ttraining's l1: 0.338213\tvalid_1's l1: 0.370675\n",
      "[3400]\ttraining's l1: 0.335218\tvalid_1's l1: 0.3684\n",
      "[3500]\ttraining's l1: 0.332395\tvalid_1's l1: 0.366319\n",
      "[3600]\ttraining's l1: 0.329662\tvalid_1's l1: 0.364276\n",
      "[3700]\ttraining's l1: 0.327012\tvalid_1's l1: 0.362333\n",
      "[3800]\ttraining's l1: 0.32445\tvalid_1's l1: 0.360488\n",
      "[3900]\ttraining's l1: 0.321809\tvalid_1's l1: 0.358532\n",
      "[4000]\ttraining's l1: 0.319505\tvalid_1's l1: 0.35692\n",
      "[4100]\ttraining's l1: 0.317156\tvalid_1's l1: 0.35526\n",
      "[4200]\ttraining's l1: 0.314874\tvalid_1's l1: 0.353682\n",
      "[4300]\ttraining's l1: 0.312562\tvalid_1's l1: 0.352044\n",
      "[4400]\ttraining's l1: 0.310353\tvalid_1's l1: 0.350484\n",
      "[4500]\ttraining's l1: 0.308247\tvalid_1's l1: 0.349035\n",
      "[4600]\ttraining's l1: 0.3062\tvalid_1's l1: 0.347651\n",
      "[4700]\ttraining's l1: 0.304094\tvalid_1's l1: 0.34615\n",
      "[4800]\ttraining's l1: 0.302053\tvalid_1's l1: 0.344728\n",
      "[4900]\ttraining's l1: 0.300063\tvalid_1's l1: 0.343318\n",
      "[5000]\ttraining's l1: 0.298158\tvalid_1's l1: 0.342054\n",
      "[5100]\ttraining's l1: 0.296226\tvalid_1's l1: 0.340687\n",
      "[5200]\ttraining's l1: 0.294366\tvalid_1's l1: 0.339414\n",
      "[5300]\ttraining's l1: 0.292602\tvalid_1's l1: 0.338219\n",
      "[5400]\ttraining's l1: 0.290761\tvalid_1's l1: 0.336991\n",
      "[5500]\ttraining's l1: 0.28909\tvalid_1's l1: 0.335871\n",
      "[5600]\ttraining's l1: 0.287362\tvalid_1's l1: 0.33474\n",
      "[5700]\ttraining's l1: 0.285775\tvalid_1's l1: 0.333707\n",
      "[5800]\ttraining's l1: 0.284091\tvalid_1's l1: 0.332581\n",
      "[5900]\ttraining's l1: 0.282517\tvalid_1's l1: 0.331562\n",
      "[6000]\ttraining's l1: 0.280863\tvalid_1's l1: 0.330503\n",
      "[6100]\ttraining's l1: 0.279263\tvalid_1's l1: 0.329456\n",
      "[6200]\ttraining's l1: 0.277695\tvalid_1's l1: 0.328483\n",
      "[6300]\ttraining's l1: 0.276252\tvalid_1's l1: 0.327563\n",
      "[6400]\ttraining's l1: 0.27473\tvalid_1's l1: 0.326583\n",
      "[6500]\ttraining's l1: 0.273347\tvalid_1's l1: 0.325694\n",
      "[6600]\ttraining's l1: 0.271884\tvalid_1's l1: 0.324783\n",
      "[6700]\ttraining's l1: 0.270523\tvalid_1's l1: 0.323902\n",
      "[6800]\ttraining's l1: 0.269125\tvalid_1's l1: 0.323016\n",
      "[6900]\ttraining's l1: 0.267708\tvalid_1's l1: 0.322168\n",
      "[7000]\ttraining's l1: 0.266314\tvalid_1's l1: 0.32129\n",
      "[7100]\ttraining's l1: 0.2649\tvalid_1's l1: 0.320391\n",
      "[7200]\ttraining's l1: 0.263618\tvalid_1's l1: 0.319618\n",
      "[7300]\ttraining's l1: 0.262357\tvalid_1's l1: 0.318886\n",
      "[7400]\ttraining's l1: 0.261067\tvalid_1's l1: 0.318127\n",
      "[7500]\ttraining's l1: 0.259774\tvalid_1's l1: 0.3173\n",
      "[7600]\ttraining's l1: 0.258554\tvalid_1's l1: 0.316577\n",
      "[7700]\ttraining's l1: 0.257252\tvalid_1's l1: 0.315822\n",
      "[7800]\ttraining's l1: 0.256039\tvalid_1's l1: 0.315068\n",
      "[7900]\ttraining's l1: 0.254929\tvalid_1's l1: 0.314437\n",
      "[8000]\ttraining's l1: 0.253783\tvalid_1's l1: 0.3138\n",
      "[8100]\ttraining's l1: 0.25261\tvalid_1's l1: 0.313122\n",
      "[8200]\ttraining's l1: 0.251508\tvalid_1's l1: 0.312528\n",
      "[8300]\ttraining's l1: 0.250339\tvalid_1's l1: 0.311823\n",
      "[8400]\ttraining's l1: 0.249191\tvalid_1's l1: 0.311154\n",
      "[8500]\ttraining's l1: 0.248072\tvalid_1's l1: 0.310513\n",
      "[8600]\ttraining's l1: 0.246992\tvalid_1's l1: 0.309888\n",
      "[8700]\ttraining's l1: 0.245841\tvalid_1's l1: 0.309231\n",
      "[8800]\ttraining's l1: 0.244734\tvalid_1's l1: 0.308567\n",
      "[8900]\ttraining's l1: 0.243694\tvalid_1's l1: 0.307989\n",
      "[9000]\ttraining's l1: 0.242651\tvalid_1's l1: 0.307362\n",
      "[9100]\ttraining's l1: 0.241586\tvalid_1's l1: 0.306713\n",
      "[9200]\ttraining's l1: 0.240599\tvalid_1's l1: 0.30618\n",
      "[9300]\ttraining's l1: 0.239582\tvalid_1's l1: 0.305642\n",
      "[9400]\ttraining's l1: 0.238606\tvalid_1's l1: 0.305084\n",
      "[9500]\ttraining's l1: 0.237586\tvalid_1's l1: 0.304506\n",
      "[9600]\ttraining's l1: 0.236584\tvalid_1's l1: 0.303949\n",
      "[9700]\ttraining's l1: 0.235596\tvalid_1's l1: 0.303408\n",
      "[9800]\ttraining's l1: 0.234583\tvalid_1's l1: 0.30282\n",
      "[9900]\ttraining's l1: 0.233604\tvalid_1's l1: 0.30229\n",
      "[10000]\ttraining's l1: 0.232692\tvalid_1's l1: 0.301807\n",
      "[10100]\ttraining's l1: 0.231738\tvalid_1's l1: 0.301291\n",
      "[10200]\ttraining's l1: 0.230819\tvalid_1's l1: 0.300833\n",
      "[10300]\ttraining's l1: 0.229897\tvalid_1's l1: 0.30033\n",
      "[10400]\ttraining's l1: 0.229011\tvalid_1's l1: 0.299865\n",
      "[10500]\ttraining's l1: 0.228148\tvalid_1's l1: 0.299392\n",
      "[10600]\ttraining's l1: 0.227234\tvalid_1's l1: 0.298898\n",
      "[10700]\ttraining's l1: 0.226358\tvalid_1's l1: 0.298428\n",
      "[10800]\ttraining's l1: 0.225539\tvalid_1's l1: 0.298011\n",
      "[10900]\ttraining's l1: 0.22467\tvalid_1's l1: 0.297523\n",
      "[11000]\ttraining's l1: 0.223804\tvalid_1's l1: 0.297075\n",
      "[11100]\ttraining's l1: 0.222964\tvalid_1's l1: 0.296629\n",
      "[11200]\ttraining's l1: 0.222132\tvalid_1's l1: 0.296199\n",
      "[11300]\ttraining's l1: 0.22133\tvalid_1's l1: 0.295779\n",
      "[11400]\ttraining's l1: 0.220526\tvalid_1's l1: 0.295407\n",
      "[11500]\ttraining's l1: 0.219719\tvalid_1's l1: 0.294996\n",
      "[11600]\ttraining's l1: 0.218952\tvalid_1's l1: 0.294599\n",
      "[11700]\ttraining's l1: 0.218149\tvalid_1's l1: 0.29419\n",
      "[11800]\ttraining's l1: 0.217348\tvalid_1's l1: 0.29378\n",
      "[11900]\ttraining's l1: 0.216544\tvalid_1's l1: 0.293363\n",
      "[12000]\ttraining's l1: 0.215817\tvalid_1's l1: 0.293025\n",
      "[12100]\ttraining's l1: 0.21506\tvalid_1's l1: 0.292641\n",
      "[12200]\ttraining's l1: 0.214281\tvalid_1's l1: 0.292251\n",
      "[12300]\ttraining's l1: 0.213546\tvalid_1's l1: 0.2919\n",
      "[12400]\ttraining's l1: 0.212816\tvalid_1's l1: 0.291552\n",
      "[12500]\ttraining's l1: 0.212068\tvalid_1's l1: 0.291207\n",
      "[12600]\ttraining's l1: 0.21132\tvalid_1's l1: 0.290806\n",
      "[12700]\ttraining's l1: 0.210548\tvalid_1's l1: 0.290418\n",
      "[12800]\ttraining's l1: 0.209804\tvalid_1's l1: 0.290043\n",
      "[12900]\ttraining's l1: 0.209106\tvalid_1's l1: 0.289703\n",
      "[13000]\ttraining's l1: 0.208394\tvalid_1's l1: 0.289331\n",
      "[13100]\ttraining's l1: 0.207638\tvalid_1's l1: 0.288945\n",
      "[13200]\ttraining's l1: 0.20695\tvalid_1's l1: 0.288588\n",
      "[13300]\ttraining's l1: 0.206245\tvalid_1's l1: 0.288226\n",
      "[13400]\ttraining's l1: 0.205564\tvalid_1's l1: 0.287889\n",
      "[13500]\ttraining's l1: 0.20486\tvalid_1's l1: 0.287546\n",
      "[13600]\ttraining's l1: 0.204145\tvalid_1's l1: 0.287161\n",
      "[13700]\ttraining's l1: 0.203471\tvalid_1's l1: 0.286839\n",
      "[13800]\ttraining's l1: 0.202818\tvalid_1's l1: 0.286513\n",
      "[13900]\ttraining's l1: 0.202157\tvalid_1's l1: 0.286201\n",
      "[14000]\ttraining's l1: 0.20151\tvalid_1's l1: 0.285892\n",
      "[14100]\ttraining's l1: 0.200857\tvalid_1's l1: 0.285567\n",
      "[14200]\ttraining's l1: 0.200213\tvalid_1's l1: 0.285262\n",
      "[14300]\ttraining's l1: 0.199566\tvalid_1's l1: 0.284927\n",
      "[14400]\ttraining's l1: 0.198931\tvalid_1's l1: 0.284613\n",
      "[14500]\ttraining's l1: 0.198315\tvalid_1's l1: 0.284322\n",
      "[14600]\ttraining's l1: 0.197657\tvalid_1's l1: 0.284014\n",
      "[14700]\ttraining's l1: 0.197027\tvalid_1's l1: 0.283721\n",
      "[14800]\ttraining's l1: 0.196418\tvalid_1's l1: 0.283465\n",
      "[14900]\ttraining's l1: 0.195813\tvalid_1's l1: 0.283174\n",
      "[15000]\ttraining's l1: 0.195213\tvalid_1's l1: 0.282894\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.195213\tvalid_1's l1: 0.282894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3JHC Fold 1, logMAE: -1.2626838003179817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.755539\tvalid_1's l1: 0.758923\n",
      "[200]\ttraining's l1: 0.656652\tvalid_1's l1: 0.661301\n",
      "[300]\ttraining's l1: 0.602209\tvalid_1's l1: 0.607898\n",
      "[400]\ttraining's l1: 0.568084\tvalid_1's l1: 0.574864\n",
      "[500]\ttraining's l1: 0.539603\tvalid_1's l1: 0.547599\n",
      "[600]\ttraining's l1: 0.517839\tvalid_1's l1: 0.526887\n",
      "[700]\ttraining's l1: 0.49958\tvalid_1's l1: 0.509896\n",
      "[800]\ttraining's l1: 0.484285\tvalid_1's l1: 0.495761\n",
      "[900]\ttraining's l1: 0.47139\tvalid_1's l1: 0.483911\n",
      "[1000]\ttraining's l1: 0.459859\tvalid_1's l1: 0.473496\n",
      "[1100]\ttraining's l1: 0.449784\tvalid_1's l1: 0.464425\n",
      "[1200]\ttraining's l1: 0.440516\tvalid_1's l1: 0.456104\n",
      "[1300]\ttraining's l1: 0.431809\tvalid_1's l1: 0.448409\n",
      "[1400]\ttraining's l1: 0.42418\tvalid_1's l1: 0.441803\n",
      "[1500]\ttraining's l1: 0.417117\tvalid_1's l1: 0.435708\n",
      "[1600]\ttraining's l1: 0.410595\tvalid_1's l1: 0.430115\n",
      "[1700]\ttraining's l1: 0.404368\tvalid_1's l1: 0.424738\n",
      "[1800]\ttraining's l1: 0.398574\tvalid_1's l1: 0.419852\n",
      "[1900]\ttraining's l1: 0.393417\tvalid_1's l1: 0.415619\n",
      "[2000]\ttraining's l1: 0.388219\tvalid_1's l1: 0.411194\n",
      "[2100]\ttraining's l1: 0.382805\tvalid_1's l1: 0.406597\n",
      "[2200]\ttraining's l1: 0.378145\tvalid_1's l1: 0.402813\n",
      "[2300]\ttraining's l1: 0.373503\tvalid_1's l1: 0.39906\n",
      "[2400]\ttraining's l1: 0.369263\tvalid_1's l1: 0.395642\n",
      "[2500]\ttraining's l1: 0.365547\tvalid_1's l1: 0.392673\n",
      "[2600]\ttraining's l1: 0.361381\tvalid_1's l1: 0.389301\n",
      "[2700]\ttraining's l1: 0.357574\tvalid_1's l1: 0.386306\n",
      "[2800]\ttraining's l1: 0.354018\tvalid_1's l1: 0.383549\n",
      "[2900]\ttraining's l1: 0.350635\tvalid_1's l1: 0.380908\n",
      "[3000]\ttraining's l1: 0.347315\tvalid_1's l1: 0.378297\n",
      "[3100]\ttraining's l1: 0.344068\tvalid_1's l1: 0.375753\n",
      "[3200]\ttraining's l1: 0.340873\tvalid_1's l1: 0.373313\n",
      "[3300]\ttraining's l1: 0.337816\tvalid_1's l1: 0.370955\n",
      "[3400]\ttraining's l1: 0.334993\tvalid_1's l1: 0.368861\n",
      "[3500]\ttraining's l1: 0.332203\tvalid_1's l1: 0.36669\n",
      "[3600]\ttraining's l1: 0.329514\tvalid_1's l1: 0.364696\n",
      "[3700]\ttraining's l1: 0.326937\tvalid_1's l1: 0.362841\n",
      "[3800]\ttraining's l1: 0.324386\tvalid_1's l1: 0.360991\n",
      "[3900]\ttraining's l1: 0.321884\tvalid_1's l1: 0.359187\n",
      "[4000]\ttraining's l1: 0.319459\tvalid_1's l1: 0.357411\n",
      "[4100]\ttraining's l1: 0.317083\tvalid_1's l1: 0.355631\n",
      "[4200]\ttraining's l1: 0.314794\tvalid_1's l1: 0.353975\n",
      "[4300]\ttraining's l1: 0.312475\tvalid_1's l1: 0.352301\n",
      "[4400]\ttraining's l1: 0.310264\tvalid_1's l1: 0.350726\n",
      "[4500]\ttraining's l1: 0.307937\tvalid_1's l1: 0.349086\n",
      "[4600]\ttraining's l1: 0.305948\tvalid_1's l1: 0.347726\n",
      "[4700]\ttraining's l1: 0.303868\tvalid_1's l1: 0.346245\n",
      "[4800]\ttraining's l1: 0.301885\tvalid_1's l1: 0.344889\n",
      "[4900]\ttraining's l1: 0.299887\tvalid_1's l1: 0.343459\n",
      "[5000]\ttraining's l1: 0.297952\tvalid_1's l1: 0.342141\n",
      "[5100]\ttraining's l1: 0.296267\tvalid_1's l1: 0.341021\n",
      "[5200]\ttraining's l1: 0.294395\tvalid_1's l1: 0.339755\n",
      "[5300]\ttraining's l1: 0.292546\tvalid_1's l1: 0.338561\n",
      "[5400]\ttraining's l1: 0.290835\tvalid_1's l1: 0.337494\n",
      "[5500]\ttraining's l1: 0.28914\tvalid_1's l1: 0.336342\n",
      "[5600]\ttraining's l1: 0.287372\tvalid_1's l1: 0.335147\n",
      "[5700]\ttraining's l1: 0.285648\tvalid_1's l1: 0.334017\n",
      "[5800]\ttraining's l1: 0.284099\tvalid_1's l1: 0.332994\n",
      "[5900]\ttraining's l1: 0.282463\tvalid_1's l1: 0.331921\n",
      "[6000]\ttraining's l1: 0.280864\tvalid_1's l1: 0.330888\n",
      "[6100]\ttraining's l1: 0.279313\tvalid_1's l1: 0.329895\n",
      "[6200]\ttraining's l1: 0.277687\tvalid_1's l1: 0.328802\n",
      "[6300]\ttraining's l1: 0.276209\tvalid_1's l1: 0.327887\n",
      "[6400]\ttraining's l1: 0.274796\tvalid_1's l1: 0.327014\n",
      "[6500]\ttraining's l1: 0.273364\tvalid_1's l1: 0.326113\n",
      "[6600]\ttraining's l1: 0.271964\tvalid_1's l1: 0.325233\n",
      "[6700]\ttraining's l1: 0.270512\tvalid_1's l1: 0.324319\n",
      "[6800]\ttraining's l1: 0.269149\tvalid_1's l1: 0.323485\n",
      "[6900]\ttraining's l1: 0.267726\tvalid_1's l1: 0.322581\n",
      "[7000]\ttraining's l1: 0.266378\tvalid_1's l1: 0.321748\n",
      "[7100]\ttraining's l1: 0.265002\tvalid_1's l1: 0.320918\n",
      "[7200]\ttraining's l1: 0.263677\tvalid_1's l1: 0.320113\n",
      "[7300]\ttraining's l1: 0.262388\tvalid_1's l1: 0.319333\n",
      "[7400]\ttraining's l1: 0.261088\tvalid_1's l1: 0.318491\n",
      "[7500]\ttraining's l1: 0.25984\tvalid_1's l1: 0.317749\n",
      "[7600]\ttraining's l1: 0.258634\tvalid_1's l1: 0.31705\n",
      "[7700]\ttraining's l1: 0.257414\tvalid_1's l1: 0.316306\n",
      "[7800]\ttraining's l1: 0.256246\tvalid_1's l1: 0.315612\n",
      "[7900]\ttraining's l1: 0.255074\tvalid_1's l1: 0.314901\n",
      "[8000]\ttraining's l1: 0.253861\tvalid_1's l1: 0.314171\n",
      "[8100]\ttraining's l1: 0.252641\tvalid_1's l1: 0.313457\n",
      "[8200]\ttraining's l1: 0.251508\tvalid_1's l1: 0.312808\n",
      "[8300]\ttraining's l1: 0.250367\tvalid_1's l1: 0.31214\n",
      "[8400]\ttraining's l1: 0.249227\tvalid_1's l1: 0.311459\n",
      "[8500]\ttraining's l1: 0.248123\tvalid_1's l1: 0.310798\n",
      "[8600]\ttraining's l1: 0.24706\tvalid_1's l1: 0.310182\n",
      "[8700]\ttraining's l1: 0.245932\tvalid_1's l1: 0.309533\n",
      "[8800]\ttraining's l1: 0.244889\tvalid_1's l1: 0.30894\n",
      "[8900]\ttraining's l1: 0.243895\tvalid_1's l1: 0.308377\n",
      "[9000]\ttraining's l1: 0.24288\tvalid_1's l1: 0.307809\n",
      "[9100]\ttraining's l1: 0.241837\tvalid_1's l1: 0.307222\n",
      "[9200]\ttraining's l1: 0.240828\tvalid_1's l1: 0.30668\n",
      "[9300]\ttraining's l1: 0.239873\tvalid_1's l1: 0.306181\n",
      "[9400]\ttraining's l1: 0.238871\tvalid_1's l1: 0.305612\n",
      "[9500]\ttraining's l1: 0.237863\tvalid_1's l1: 0.305028\n",
      "[9600]\ttraining's l1: 0.2369\tvalid_1's l1: 0.304512\n",
      "[9700]\ttraining's l1: 0.23588\tvalid_1's l1: 0.303946\n",
      "[9800]\ttraining's l1: 0.234903\tvalid_1's l1: 0.303436\n",
      "[9900]\ttraining's l1: 0.233922\tvalid_1's l1: 0.302874\n",
      "[10000]\ttraining's l1: 0.232933\tvalid_1's l1: 0.302311\n",
      "[10100]\ttraining's l1: 0.231953\tvalid_1's l1: 0.301738\n",
      "[10200]\ttraining's l1: 0.231024\tvalid_1's l1: 0.301223\n",
      "[10300]\ttraining's l1: 0.230097\tvalid_1's l1: 0.300722\n",
      "[10400]\ttraining's l1: 0.229189\tvalid_1's l1: 0.300207\n",
      "[10500]\ttraining's l1: 0.228314\tvalid_1's l1: 0.299738\n",
      "[10600]\ttraining's l1: 0.227458\tvalid_1's l1: 0.299309\n",
      "[10700]\ttraining's l1: 0.22663\tvalid_1's l1: 0.298853\n",
      "[10800]\ttraining's l1: 0.225741\tvalid_1's l1: 0.298363\n",
      "[10900]\ttraining's l1: 0.224872\tvalid_1's l1: 0.297889\n",
      "[11000]\ttraining's l1: 0.224071\tvalid_1's l1: 0.297485\n",
      "[11100]\ttraining's l1: 0.223242\tvalid_1's l1: 0.297054\n",
      "[11200]\ttraining's l1: 0.222409\tvalid_1's l1: 0.296625\n",
      "[11300]\ttraining's l1: 0.221559\tvalid_1's l1: 0.296195\n",
      "[11400]\ttraining's l1: 0.220718\tvalid_1's l1: 0.295746\n",
      "[11500]\ttraining's l1: 0.219924\tvalid_1's l1: 0.29536\n",
      "[11600]\ttraining's l1: 0.219129\tvalid_1's l1: 0.294976\n",
      "[11700]\ttraining's l1: 0.218279\tvalid_1's l1: 0.29452\n",
      "[11800]\ttraining's l1: 0.217476\tvalid_1's l1: 0.294127\n",
      "[11900]\ttraining's l1: 0.216674\tvalid_1's l1: 0.293703\n",
      "[12000]\ttraining's l1: 0.21593\tvalid_1's l1: 0.293324\n",
      "[12100]\ttraining's l1: 0.21516\tvalid_1's l1: 0.29294\n",
      "[12200]\ttraining's l1: 0.214416\tvalid_1's l1: 0.292572\n",
      "[12300]\ttraining's l1: 0.21369\tvalid_1's l1: 0.292187\n",
      "[12400]\ttraining's l1: 0.212928\tvalid_1's l1: 0.291789\n",
      "[12500]\ttraining's l1: 0.212154\tvalid_1's l1: 0.291399\n",
      "[12600]\ttraining's l1: 0.211362\tvalid_1's l1: 0.290988\n",
      "[12700]\ttraining's l1: 0.210633\tvalid_1's l1: 0.29062\n",
      "[12800]\ttraining's l1: 0.209874\tvalid_1's l1: 0.290247\n",
      "[12900]\ttraining's l1: 0.209158\tvalid_1's l1: 0.289875\n",
      "[13000]\ttraining's l1: 0.208417\tvalid_1's l1: 0.289505\n",
      "[13100]\ttraining's l1: 0.207725\tvalid_1's l1: 0.289177\n",
      "[13200]\ttraining's l1: 0.207046\tvalid_1's l1: 0.288877\n",
      "[13300]\ttraining's l1: 0.206321\tvalid_1's l1: 0.288499\n",
      "[13400]\ttraining's l1: 0.205643\tvalid_1's l1: 0.288179\n",
      "[13500]\ttraining's l1: 0.204979\tvalid_1's l1: 0.28786\n",
      "[13600]\ttraining's l1: 0.204303\tvalid_1's l1: 0.287541\n",
      "[13700]\ttraining's l1: 0.203608\tvalid_1's l1: 0.287205\n",
      "[13800]\ttraining's l1: 0.202944\tvalid_1's l1: 0.286892\n",
      "[13900]\ttraining's l1: 0.20227\tvalid_1's l1: 0.286559\n",
      "[14000]\ttraining's l1: 0.201591\tvalid_1's l1: 0.286224\n",
      "[14100]\ttraining's l1: 0.20092\tvalid_1's l1: 0.285889\n",
      "[14200]\ttraining's l1: 0.20028\tvalid_1's l1: 0.285578\n",
      "[14300]\ttraining's l1: 0.199661\tvalid_1's l1: 0.285267\n",
      "[14400]\ttraining's l1: 0.198986\tvalid_1's l1: 0.284929\n",
      "[14500]\ttraining's l1: 0.198352\tvalid_1's l1: 0.28463\n",
      "[14600]\ttraining's l1: 0.197712\tvalid_1's l1: 0.284337\n",
      "[14700]\ttraining's l1: 0.197093\tvalid_1's l1: 0.284055\n",
      "[14800]\ttraining's l1: 0.196473\tvalid_1's l1: 0.283784\n",
      "[14900]\ttraining's l1: 0.195852\tvalid_1's l1: 0.283499\n",
      "[15000]\ttraining's l1: 0.195226\tvalid_1's l1: 0.283201\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.195226\tvalid_1's l1: 0.283201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3JHC Fold 2, logMAE: -1.2615989062104886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.758621\tvalid_1's l1: 0.764276\n",
      "[200]\ttraining's l1: 0.654844\tvalid_1's l1: 0.660901\n",
      "[300]\ttraining's l1: 0.601884\tvalid_1's l1: 0.608921\n",
      "[400]\ttraining's l1: 0.564646\tvalid_1's l1: 0.572715\n",
      "[500]\ttraining's l1: 0.538462\tvalid_1's l1: 0.547456\n",
      "[600]\ttraining's l1: 0.517108\tvalid_1's l1: 0.527234\n",
      "[700]\ttraining's l1: 0.499364\tvalid_1's l1: 0.510387\n",
      "[800]\ttraining's l1: 0.485285\tvalid_1's l1: 0.497353\n",
      "[900]\ttraining's l1: 0.471701\tvalid_1's l1: 0.484798\n",
      "[1000]\ttraining's l1: 0.460229\tvalid_1's l1: 0.474439\n",
      "[1100]\ttraining's l1: 0.449953\tvalid_1's l1: 0.465111\n",
      "[1200]\ttraining's l1: 0.440576\tvalid_1's l1: 0.456807\n",
      "[1300]\ttraining's l1: 0.432603\tvalid_1's l1: 0.449797\n",
      "[1400]\ttraining's l1: 0.42487\tvalid_1's l1: 0.443106\n",
      "[1500]\ttraining's l1: 0.417752\tvalid_1's l1: 0.436948\n",
      "[1600]\ttraining's l1: 0.411262\tvalid_1's l1: 0.431379\n",
      "[1700]\ttraining's l1: 0.405263\tvalid_1's l1: 0.426327\n",
      "[1800]\ttraining's l1: 0.399351\tvalid_1's l1: 0.421295\n",
      "[1900]\ttraining's l1: 0.394003\tvalid_1's l1: 0.416814\n",
      "[2000]\ttraining's l1: 0.388992\tvalid_1's l1: 0.412641\n",
      "[2100]\ttraining's l1: 0.384026\tvalid_1's l1: 0.408597\n",
      "[2200]\ttraining's l1: 0.37911\tvalid_1's l1: 0.40448\n",
      "[2300]\ttraining's l1: 0.374813\tvalid_1's l1: 0.401022\n",
      "[2400]\ttraining's l1: 0.370376\tvalid_1's l1: 0.397365\n",
      "[2500]\ttraining's l1: 0.366137\tvalid_1's l1: 0.393905\n",
      "[2600]\ttraining's l1: 0.362138\tvalid_1's l1: 0.390721\n",
      "[2700]\ttraining's l1: 0.358428\tvalid_1's l1: 0.387811\n",
      "[2800]\ttraining's l1: 0.354682\tvalid_1's l1: 0.384867\n",
      "[2900]\ttraining's l1: 0.351251\tvalid_1's l1: 0.382151\n",
      "[3000]\ttraining's l1: 0.347991\tvalid_1's l1: 0.379675\n",
      "[3100]\ttraining's l1: 0.344767\tvalid_1's l1: 0.377172\n",
      "[3200]\ttraining's l1: 0.341732\tvalid_1's l1: 0.374846\n",
      "[3300]\ttraining's l1: 0.338723\tvalid_1's l1: 0.372617\n",
      "[3400]\ttraining's l1: 0.335691\tvalid_1's l1: 0.37031\n",
      "[3500]\ttraining's l1: 0.332751\tvalid_1's l1: 0.36803\n",
      "[3600]\ttraining's l1: 0.330099\tvalid_1's l1: 0.365982\n",
      "[3700]\ttraining's l1: 0.3274\tvalid_1's l1: 0.363959\n",
      "[3800]\ttraining's l1: 0.32496\tvalid_1's l1: 0.362187\n",
      "[3900]\ttraining's l1: 0.322428\tvalid_1's l1: 0.360363\n",
      "[4000]\ttraining's l1: 0.319858\tvalid_1's l1: 0.35847\n",
      "[4100]\ttraining's l1: 0.317436\tvalid_1's l1: 0.356791\n",
      "[4200]\ttraining's l1: 0.315136\tvalid_1's l1: 0.355106\n",
      "[4300]\ttraining's l1: 0.312883\tvalid_1's l1: 0.353431\n",
      "[4400]\ttraining's l1: 0.310698\tvalid_1's l1: 0.351838\n",
      "[4500]\ttraining's l1: 0.30863\tvalid_1's l1: 0.350411\n",
      "[4600]\ttraining's l1: 0.306619\tvalid_1's l1: 0.349028\n",
      "[4700]\ttraining's l1: 0.30451\tvalid_1's l1: 0.347567\n",
      "[4800]\ttraining's l1: 0.302552\tvalid_1's l1: 0.346282\n",
      "[4900]\ttraining's l1: 0.300719\tvalid_1's l1: 0.345045\n",
      "[5000]\ttraining's l1: 0.298731\tvalid_1's l1: 0.343658\n",
      "[5100]\ttraining's l1: 0.296674\tvalid_1's l1: 0.342246\n",
      "[5200]\ttraining's l1: 0.29476\tvalid_1's l1: 0.340937\n",
      "[5300]\ttraining's l1: 0.292982\tvalid_1's l1: 0.339724\n",
      "[5400]\ttraining's l1: 0.29121\tvalid_1's l1: 0.338529\n",
      "[5500]\ttraining's l1: 0.289391\tvalid_1's l1: 0.337295\n",
      "[5600]\ttraining's l1: 0.287659\tvalid_1's l1: 0.33617\n",
      "[5700]\ttraining's l1: 0.285924\tvalid_1's l1: 0.335002\n",
      "[5800]\ttraining's l1: 0.284303\tvalid_1's l1: 0.333908\n",
      "[5900]\ttraining's l1: 0.28268\tvalid_1's l1: 0.332847\n",
      "[6000]\ttraining's l1: 0.281061\tvalid_1's l1: 0.331771\n",
      "[6100]\ttraining's l1: 0.279512\tvalid_1's l1: 0.330763\n",
      "[6200]\ttraining's l1: 0.277935\tvalid_1's l1: 0.329689\n",
      "[6300]\ttraining's l1: 0.276428\tvalid_1's l1: 0.328726\n",
      "[6400]\ttraining's l1: 0.274986\tvalid_1's l1: 0.32782\n",
      "[6500]\ttraining's l1: 0.273505\tvalid_1's l1: 0.326858\n",
      "[6600]\ttraining's l1: 0.272054\tvalid_1's l1: 0.325929\n",
      "[6700]\ttraining's l1: 0.270638\tvalid_1's l1: 0.325027\n",
      "[6800]\ttraining's l1: 0.269225\tvalid_1's l1: 0.324081\n",
      "[6900]\ttraining's l1: 0.267909\tvalid_1's l1: 0.323259\n",
      "[7000]\ttraining's l1: 0.266567\tvalid_1's l1: 0.322474\n",
      "[7100]\ttraining's l1: 0.265299\tvalid_1's l1: 0.321699\n",
      "[7200]\ttraining's l1: 0.264006\tvalid_1's l1: 0.320948\n",
      "[7300]\ttraining's l1: 0.262649\tvalid_1's l1: 0.320076\n",
      "[7400]\ttraining's l1: 0.261351\tvalid_1's l1: 0.319252\n",
      "[7500]\ttraining's l1: 0.260113\tvalid_1's l1: 0.318494\n",
      "[7600]\ttraining's l1: 0.258801\tvalid_1's l1: 0.317695\n",
      "[7700]\ttraining's l1: 0.257631\tvalid_1's l1: 0.317\n",
      "[7800]\ttraining's l1: 0.256479\tvalid_1's l1: 0.316323\n",
      "[7900]\ttraining's l1: 0.255291\tvalid_1's l1: 0.315609\n",
      "[8000]\ttraining's l1: 0.254141\tvalid_1's l1: 0.314952\n",
      "[8100]\ttraining's l1: 0.252958\tvalid_1's l1: 0.314246\n",
      "[8200]\ttraining's l1: 0.251818\tvalid_1's l1: 0.313589\n",
      "[8300]\ttraining's l1: 0.250682\tvalid_1's l1: 0.312935\n",
      "[8400]\ttraining's l1: 0.249531\tvalid_1's l1: 0.312276\n",
      "[8500]\ttraining's l1: 0.24842\tvalid_1's l1: 0.311654\n",
      "[8600]\ttraining's l1: 0.247338\tvalid_1's l1: 0.311015\n",
      "[8700]\ttraining's l1: 0.24629\tvalid_1's l1: 0.310404\n",
      "[8800]\ttraining's l1: 0.245185\tvalid_1's l1: 0.309781\n",
      "[8900]\ttraining's l1: 0.244112\tvalid_1's l1: 0.309184\n",
      "[9000]\ttraining's l1: 0.243088\tvalid_1's l1: 0.308618\n",
      "[9100]\ttraining's l1: 0.242037\tvalid_1's l1: 0.308036\n",
      "[9200]\ttraining's l1: 0.241014\tvalid_1's l1: 0.307481\n",
      "[9300]\ttraining's l1: 0.24\tvalid_1's l1: 0.306916\n",
      "[9400]\ttraining's l1: 0.239\tvalid_1's l1: 0.306339\n",
      "[9500]\ttraining's l1: 0.238007\tvalid_1's l1: 0.305813\n",
      "[9600]\ttraining's l1: 0.237039\tvalid_1's l1: 0.305286\n",
      "[9700]\ttraining's l1: 0.235991\tvalid_1's l1: 0.304649\n",
      "[9800]\ttraining's l1: 0.235028\tvalid_1's l1: 0.304103\n",
      "[9900]\ttraining's l1: 0.234047\tvalid_1's l1: 0.303556\n",
      "[10000]\ttraining's l1: 0.233086\tvalid_1's l1: 0.303021\n",
      "[10100]\ttraining's l1: 0.232141\tvalid_1's l1: 0.302492\n",
      "[10200]\ttraining's l1: 0.231227\tvalid_1's l1: 0.301997\n",
      "[10300]\ttraining's l1: 0.230332\tvalid_1's l1: 0.301513\n",
      "[10400]\ttraining's l1: 0.229424\tvalid_1's l1: 0.301009\n",
      "[10500]\ttraining's l1: 0.228535\tvalid_1's l1: 0.30056\n",
      "[10600]\ttraining's l1: 0.227694\tvalid_1's l1: 0.300106\n",
      "[10700]\ttraining's l1: 0.226801\tvalid_1's l1: 0.299608\n",
      "[10800]\ttraining's l1: 0.225926\tvalid_1's l1: 0.299129\n",
      "[10900]\ttraining's l1: 0.225076\tvalid_1's l1: 0.298659\n",
      "[11000]\ttraining's l1: 0.224224\tvalid_1's l1: 0.298183\n",
      "[11100]\ttraining's l1: 0.223352\tvalid_1's l1: 0.297742\n",
      "[11200]\ttraining's l1: 0.222485\tvalid_1's l1: 0.297302\n",
      "[11300]\ttraining's l1: 0.221641\tvalid_1's l1: 0.296836\n",
      "[11400]\ttraining's l1: 0.220824\tvalid_1's l1: 0.296403\n",
      "[11500]\ttraining's l1: 0.220006\tvalid_1's l1: 0.295981\n",
      "[11600]\ttraining's l1: 0.219219\tvalid_1's l1: 0.295613\n",
      "[11700]\ttraining's l1: 0.218446\tvalid_1's l1: 0.295199\n",
      "[11800]\ttraining's l1: 0.217636\tvalid_1's l1: 0.294774\n",
      "[11900]\ttraining's l1: 0.216898\tvalid_1's l1: 0.294396\n",
      "[12000]\ttraining's l1: 0.216098\tvalid_1's l1: 0.293989\n",
      "[12100]\ttraining's l1: 0.215309\tvalid_1's l1: 0.293574\n",
      "[12200]\ttraining's l1: 0.21453\tvalid_1's l1: 0.293169\n",
      "[12300]\ttraining's l1: 0.213767\tvalid_1's l1: 0.29278\n",
      "[12400]\ttraining's l1: 0.213021\tvalid_1's l1: 0.292418\n",
      "[12500]\ttraining's l1: 0.212242\tvalid_1's l1: 0.292025\n",
      "[12600]\ttraining's l1: 0.211512\tvalid_1's l1: 0.291689\n",
      "[12700]\ttraining's l1: 0.210765\tvalid_1's l1: 0.291301\n",
      "[12800]\ttraining's l1: 0.210051\tvalid_1's l1: 0.290945\n",
      "[12900]\ttraining's l1: 0.209315\tvalid_1's l1: 0.290598\n",
      "[13000]\ttraining's l1: 0.208585\tvalid_1's l1: 0.290252\n",
      "[13100]\ttraining's l1: 0.207803\tvalid_1's l1: 0.289842\n",
      "[13200]\ttraining's l1: 0.207096\tvalid_1's l1: 0.289484\n",
      "[13300]\ttraining's l1: 0.206386\tvalid_1's l1: 0.289138\n",
      "[13400]\ttraining's l1: 0.205656\tvalid_1's l1: 0.288798\n",
      "[13500]\ttraining's l1: 0.204962\tvalid_1's l1: 0.288438\n",
      "[13600]\ttraining's l1: 0.204249\tvalid_1's l1: 0.288081\n",
      "[13700]\ttraining's l1: 0.203592\tvalid_1's l1: 0.287754\n",
      "[13800]\ttraining's l1: 0.202919\tvalid_1's l1: 0.287435\n",
      "[13900]\ttraining's l1: 0.202232\tvalid_1's l1: 0.287107\n",
      "[14000]\ttraining's l1: 0.201578\tvalid_1's l1: 0.286815\n",
      "[14100]\ttraining's l1: 0.200901\tvalid_1's l1: 0.286501\n",
      "[14200]\ttraining's l1: 0.200241\tvalid_1's l1: 0.286172\n",
      "[14300]\ttraining's l1: 0.199555\tvalid_1's l1: 0.285864\n",
      "[14400]\ttraining's l1: 0.198898\tvalid_1's l1: 0.285559\n",
      "[14500]\ttraining's l1: 0.198256\tvalid_1's l1: 0.285234\n",
      "[14600]\ttraining's l1: 0.197603\tvalid_1's l1: 0.284923\n",
      "[14700]\ttraining's l1: 0.196994\tvalid_1's l1: 0.284633\n",
      "[14800]\ttraining's l1: 0.196383\tvalid_1's l1: 0.284352\n",
      "[14900]\ttraining's l1: 0.195766\tvalid_1's l1: 0.284037\n",
      "[15000]\ttraining's l1: 0.195146\tvalid_1's l1: 0.283719\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.195146\tvalid_1's l1: 0.283719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3JHC Fold 3, logMAE: -1.2597712817649032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.754271\tvalid_1's l1: 0.755367\n",
      "[200]\ttraining's l1: 0.655242\tvalid_1's l1: 0.658393\n",
      "[300]\ttraining's l1: 0.601707\tvalid_1's l1: 0.606727\n",
      "[400]\ttraining's l1: 0.564993\tvalid_1's l1: 0.571581\n",
      "[500]\ttraining's l1: 0.538493\tvalid_1's l1: 0.546621\n",
      "[600]\ttraining's l1: 0.51747\tvalid_1's l1: 0.526935\n",
      "[700]\ttraining's l1: 0.498777\tvalid_1's l1: 0.509522\n",
      "[800]\ttraining's l1: 0.484622\tvalid_1's l1: 0.496487\n",
      "[900]\ttraining's l1: 0.470838\tvalid_1's l1: 0.483757\n",
      "[1000]\ttraining's l1: 0.458916\tvalid_1's l1: 0.473076\n",
      "[1100]\ttraining's l1: 0.448728\tvalid_1's l1: 0.464011\n",
      "[1200]\ttraining's l1: 0.439439\tvalid_1's l1: 0.455729\n",
      "[1300]\ttraining's l1: 0.430884\tvalid_1's l1: 0.448082\n",
      "[1400]\ttraining's l1: 0.423516\tvalid_1's l1: 0.441657\n",
      "[1500]\ttraining's l1: 0.416487\tvalid_1's l1: 0.435549\n",
      "[1600]\ttraining's l1: 0.410065\tvalid_1's l1: 0.430051\n",
      "[1700]\ttraining's l1: 0.403837\tvalid_1's l1: 0.424647\n",
      "[1800]\ttraining's l1: 0.397955\tvalid_1's l1: 0.41956\n",
      "[1900]\ttraining's l1: 0.392667\tvalid_1's l1: 0.415223\n",
      "[2000]\ttraining's l1: 0.38738\tvalid_1's l1: 0.410707\n",
      "[2100]\ttraining's l1: 0.382283\tvalid_1's l1: 0.406393\n",
      "[2200]\ttraining's l1: 0.37759\tvalid_1's l1: 0.402585\n",
      "[2300]\ttraining's l1: 0.373165\tvalid_1's l1: 0.398996\n",
      "[2400]\ttraining's l1: 0.368806\tvalid_1's l1: 0.395535\n",
      "[2500]\ttraining's l1: 0.364772\tvalid_1's l1: 0.392228\n",
      "[2600]\ttraining's l1: 0.360949\tvalid_1's l1: 0.389222\n",
      "[2700]\ttraining's l1: 0.357148\tvalid_1's l1: 0.38622\n",
      "[2800]\ttraining's l1: 0.353474\tvalid_1's l1: 0.383286\n",
      "[2900]\ttraining's l1: 0.349929\tvalid_1's l1: 0.380506\n",
      "[3000]\ttraining's l1: 0.346653\tvalid_1's l1: 0.377983\n",
      "[3100]\ttraining's l1: 0.343556\tvalid_1's l1: 0.375588\n",
      "[3200]\ttraining's l1: 0.34028\tvalid_1's l1: 0.37308\n",
      "[3300]\ttraining's l1: 0.337056\tvalid_1's l1: 0.370588\n",
      "[3400]\ttraining's l1: 0.33402\tvalid_1's l1: 0.368267\n",
      "[3500]\ttraining's l1: 0.331265\tvalid_1's l1: 0.366151\n",
      "[3600]\ttraining's l1: 0.328494\tvalid_1's l1: 0.364123\n",
      "[3700]\ttraining's l1: 0.325938\tvalid_1's l1: 0.362267\n",
      "[3800]\ttraining's l1: 0.323497\tvalid_1's l1: 0.360488\n",
      "[3900]\ttraining's l1: 0.320911\tvalid_1's l1: 0.358617\n",
      "[4000]\ttraining's l1: 0.318584\tvalid_1's l1: 0.356921\n",
      "[4100]\ttraining's l1: 0.316173\tvalid_1's l1: 0.355134\n",
      "[4200]\ttraining's l1: 0.313791\tvalid_1's l1: 0.353416\n",
      "[4300]\ttraining's l1: 0.311542\tvalid_1's l1: 0.351814\n",
      "[4400]\ttraining's l1: 0.309247\tvalid_1's l1: 0.350137\n",
      "[4500]\ttraining's l1: 0.307144\tvalid_1's l1: 0.348665\n",
      "[4600]\ttraining's l1: 0.305043\tvalid_1's l1: 0.347154\n",
      "[4700]\ttraining's l1: 0.303025\tvalid_1's l1: 0.345748\n",
      "[4800]\ttraining's l1: 0.301029\tvalid_1's l1: 0.344381\n",
      "[4900]\ttraining's l1: 0.299112\tvalid_1's l1: 0.343086\n",
      "[5000]\ttraining's l1: 0.297181\tvalid_1's l1: 0.341751\n",
      "[5100]\ttraining's l1: 0.295347\tvalid_1's l1: 0.340533\n",
      "[5200]\ttraining's l1: 0.293536\tvalid_1's l1: 0.339314\n",
      "[5300]\ttraining's l1: 0.291787\tvalid_1's l1: 0.338129\n",
      "[5400]\ttraining's l1: 0.290031\tvalid_1's l1: 0.336931\n",
      "[5500]\ttraining's l1: 0.288328\tvalid_1's l1: 0.335808\n",
      "[5600]\ttraining's l1: 0.286561\tvalid_1's l1: 0.334582\n",
      "[5700]\ttraining's l1: 0.28496\tvalid_1's l1: 0.333549\n",
      "[5800]\ttraining's l1: 0.283312\tvalid_1's l1: 0.332436\n",
      "[5900]\ttraining's l1: 0.281718\tvalid_1's l1: 0.331356\n",
      "[6000]\ttraining's l1: 0.280103\tvalid_1's l1: 0.330274\n",
      "[6100]\ttraining's l1: 0.278574\tvalid_1's l1: 0.329283\n",
      "[6200]\ttraining's l1: 0.277023\tvalid_1's l1: 0.328306\n",
      "[6300]\ttraining's l1: 0.275569\tvalid_1's l1: 0.32739\n",
      "[6400]\ttraining's l1: 0.274097\tvalid_1's l1: 0.32646\n",
      "[6500]\ttraining's l1: 0.272624\tvalid_1's l1: 0.325487\n",
      "[6600]\ttraining's l1: 0.271156\tvalid_1's l1: 0.324588\n",
      "[6700]\ttraining's l1: 0.269791\tvalid_1's l1: 0.323777\n",
      "[6800]\ttraining's l1: 0.268414\tvalid_1's l1: 0.322892\n",
      "[6900]\ttraining's l1: 0.267012\tvalid_1's l1: 0.322021\n",
      "[7000]\ttraining's l1: 0.265653\tvalid_1's l1: 0.32117\n",
      "[7100]\ttraining's l1: 0.264331\tvalid_1's l1: 0.320335\n",
      "[7200]\ttraining's l1: 0.263056\tvalid_1's l1: 0.319557\n",
      "[7300]\ttraining's l1: 0.261753\tvalid_1's l1: 0.318771\n",
      "[7400]\ttraining's l1: 0.260404\tvalid_1's l1: 0.317917\n",
      "[7500]\ttraining's l1: 0.259135\tvalid_1's l1: 0.317123\n",
      "[7600]\ttraining's l1: 0.257923\tvalid_1's l1: 0.316398\n",
      "[7700]\ttraining's l1: 0.256677\tvalid_1's l1: 0.315657\n",
      "[7800]\ttraining's l1: 0.255447\tvalid_1's l1: 0.314925\n",
      "[7900]\ttraining's l1: 0.254324\tvalid_1's l1: 0.314295\n",
      "[8000]\ttraining's l1: 0.253081\tvalid_1's l1: 0.313565\n",
      "[8100]\ttraining's l1: 0.251909\tvalid_1's l1: 0.312907\n",
      "[8200]\ttraining's l1: 0.250771\tvalid_1's l1: 0.312225\n",
      "[8300]\ttraining's l1: 0.249623\tvalid_1's l1: 0.311569\n",
      "[8400]\ttraining's l1: 0.248513\tvalid_1's l1: 0.310932\n",
      "[8500]\ttraining's l1: 0.2474\tvalid_1's l1: 0.310267\n",
      "[8600]\ttraining's l1: 0.246354\tvalid_1's l1: 0.309647\n",
      "[8700]\ttraining's l1: 0.245275\tvalid_1's l1: 0.309023\n",
      "[8800]\ttraining's l1: 0.244234\tvalid_1's l1: 0.308476\n",
      "[8900]\ttraining's l1: 0.243162\tvalid_1's l1: 0.307879\n",
      "[9000]\ttraining's l1: 0.242137\tvalid_1's l1: 0.307335\n",
      "[9100]\ttraining's l1: 0.241111\tvalid_1's l1: 0.306743\n",
      "[9200]\ttraining's l1: 0.240108\tvalid_1's l1: 0.306185\n",
      "[9300]\ttraining's l1: 0.239113\tvalid_1's l1: 0.305614\n",
      "[9400]\ttraining's l1: 0.238133\tvalid_1's l1: 0.305084\n",
      "[9500]\ttraining's l1: 0.23713\tvalid_1's l1: 0.304512\n",
      "[9600]\ttraining's l1: 0.236125\tvalid_1's l1: 0.303947\n",
      "[9700]\ttraining's l1: 0.23515\tvalid_1's l1: 0.303439\n",
      "[9800]\ttraining's l1: 0.234214\tvalid_1's l1: 0.302921\n",
      "[9900]\ttraining's l1: 0.23321\tvalid_1's l1: 0.302355\n",
      "[10000]\ttraining's l1: 0.232293\tvalid_1's l1: 0.301855\n",
      "[10100]\ttraining's l1: 0.231409\tvalid_1's l1: 0.301417\n",
      "[10200]\ttraining's l1: 0.230477\tvalid_1's l1: 0.300913\n",
      "[10300]\ttraining's l1: 0.229558\tvalid_1's l1: 0.300413\n",
      "[10400]\ttraining's l1: 0.228616\tvalid_1's l1: 0.299888\n",
      "[10500]\ttraining's l1: 0.227741\tvalid_1's l1: 0.299421\n",
      "[10600]\ttraining's l1: 0.226854\tvalid_1's l1: 0.298939\n",
      "[10700]\ttraining's l1: 0.226003\tvalid_1's l1: 0.298533\n",
      "[10800]\ttraining's l1: 0.225131\tvalid_1's l1: 0.298079\n",
      "[10900]\ttraining's l1: 0.224262\tvalid_1's l1: 0.297588\n",
      "[11000]\ttraining's l1: 0.223421\tvalid_1's l1: 0.297143\n",
      "[11100]\ttraining's l1: 0.22257\tvalid_1's l1: 0.296677\n",
      "[11200]\ttraining's l1: 0.221729\tvalid_1's l1: 0.296205\n",
      "[11300]\ttraining's l1: 0.220929\tvalid_1's l1: 0.295772\n",
      "[11400]\ttraining's l1: 0.2201\tvalid_1's l1: 0.295334\n",
      "[11500]\ttraining's l1: 0.219299\tvalid_1's l1: 0.294915\n",
      "[11600]\ttraining's l1: 0.218512\tvalid_1's l1: 0.294497\n",
      "[11700]\ttraining's l1: 0.217689\tvalid_1's l1: 0.294067\n",
      "[11800]\ttraining's l1: 0.216928\tvalid_1's l1: 0.293698\n",
      "[11900]\ttraining's l1: 0.216123\tvalid_1's l1: 0.293265\n",
      "[12000]\ttraining's l1: 0.215372\tvalid_1's l1: 0.292869\n",
      "[12100]\ttraining's l1: 0.214581\tvalid_1's l1: 0.292468\n",
      "[12200]\ttraining's l1: 0.213859\tvalid_1's l1: 0.292115\n",
      "[12300]\ttraining's l1: 0.213084\tvalid_1's l1: 0.29171\n",
      "[12400]\ttraining's l1: 0.212334\tvalid_1's l1: 0.291315\n",
      "[12500]\ttraining's l1: 0.211595\tvalid_1's l1: 0.290945\n",
      "[12600]\ttraining's l1: 0.210888\tvalid_1's l1: 0.290615\n",
      "[12700]\ttraining's l1: 0.210134\tvalid_1's l1: 0.290229\n",
      "[12800]\ttraining's l1: 0.209419\tvalid_1's l1: 0.289875\n",
      "[12900]\ttraining's l1: 0.208659\tvalid_1's l1: 0.289481\n",
      "[13000]\ttraining's l1: 0.207975\tvalid_1's l1: 0.289151\n",
      "[13100]\ttraining's l1: 0.207266\tvalid_1's l1: 0.288798\n",
      "[13200]\ttraining's l1: 0.206554\tvalid_1's l1: 0.288443\n",
      "[13300]\ttraining's l1: 0.20587\tvalid_1's l1: 0.288097\n",
      "[13400]\ttraining's l1: 0.205156\tvalid_1's l1: 0.287746\n",
      "[13500]\ttraining's l1: 0.204488\tvalid_1's l1: 0.287384\n",
      "[13600]\ttraining's l1: 0.203811\tvalid_1's l1: 0.287068\n",
      "[13700]\ttraining's l1: 0.203116\tvalid_1's l1: 0.286714\n",
      "[13800]\ttraining's l1: 0.202445\tvalid_1's l1: 0.286363\n",
      "[13900]\ttraining's l1: 0.201792\tvalid_1's l1: 0.28604\n",
      "[14000]\ttraining's l1: 0.201124\tvalid_1's l1: 0.285742\n",
      "[14100]\ttraining's l1: 0.200464\tvalid_1's l1: 0.285441\n",
      "[14200]\ttraining's l1: 0.199818\tvalid_1's l1: 0.285141\n",
      "[14300]\ttraining's l1: 0.199179\tvalid_1's l1: 0.284848\n",
      "[14400]\ttraining's l1: 0.198543\tvalid_1's l1: 0.284549\n",
      "[14500]\ttraining's l1: 0.197917\tvalid_1's l1: 0.284257\n",
      "[14600]\ttraining's l1: 0.19726\tvalid_1's l1: 0.283954\n",
      "[14700]\ttraining's l1: 0.196644\tvalid_1's l1: 0.283673\n",
      "[14800]\ttraining's l1: 0.196003\tvalid_1's l1: 0.283374\n",
      "[14900]\ttraining's l1: 0.195372\tvalid_1's l1: 0.283073\n",
      "[15000]\ttraining's l1: 0.194736\tvalid_1's l1: 0.282757\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.194736\tvalid_1's l1: 0.282757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3JHC Fold 4, logMAE: -1.263154924423659\n",
      "*** Training Model for 3JHN ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.245052\tvalid_1's l1: 0.254022\n",
      "[200]\ttraining's l1: 0.201821\tvalid_1's l1: 0.214953\n",
      "[300]\ttraining's l1: 0.177824\tvalid_1's l1: 0.194551\n",
      "[400]\ttraining's l1: 0.161824\tvalid_1's l1: 0.181455\n",
      "[500]\ttraining's l1: 0.14943\tvalid_1's l1: 0.171749\n",
      "[600]\ttraining's l1: 0.139618\tvalid_1's l1: 0.16435\n",
      "[700]\ttraining's l1: 0.131363\tvalid_1's l1: 0.158483\n",
      "[800]\ttraining's l1: 0.124386\tvalid_1's l1: 0.153519\n",
      "[900]\ttraining's l1: 0.118434\tvalid_1's l1: 0.149642\n",
      "[1000]\ttraining's l1: 0.112999\tvalid_1's l1: 0.14605\n",
      "[1100]\ttraining's l1: 0.108169\tvalid_1's l1: 0.142964\n",
      "[1200]\ttraining's l1: 0.103875\tvalid_1's l1: 0.140286\n",
      "[1300]\ttraining's l1: 0.0999727\tvalid_1's l1: 0.137877\n",
      "[1400]\ttraining's l1: 0.0964001\tvalid_1's l1: 0.135805\n",
      "[1500]\ttraining's l1: 0.0932088\tvalid_1's l1: 0.134064\n",
      "[1600]\ttraining's l1: 0.0901698\tvalid_1's l1: 0.132458\n",
      "[1700]\ttraining's l1: 0.0873086\tvalid_1's l1: 0.130838\n",
      "[1800]\ttraining's l1: 0.0846122\tvalid_1's l1: 0.129407\n",
      "[1900]\ttraining's l1: 0.0821426\tvalid_1's l1: 0.128183\n",
      "[2000]\ttraining's l1: 0.0799336\tvalid_1's l1: 0.127115\n",
      "[2100]\ttraining's l1: 0.077772\tvalid_1's l1: 0.126102\n",
      "[2200]\ttraining's l1: 0.0757216\tvalid_1's l1: 0.125046\n",
      "[2300]\ttraining's l1: 0.0737114\tvalid_1's l1: 0.123977\n",
      "[2400]\ttraining's l1: 0.0719813\tvalid_1's l1: 0.123198\n",
      "[2500]\ttraining's l1: 0.0702026\tvalid_1's l1: 0.122399\n",
      "[2600]\ttraining's l1: 0.0685355\tvalid_1's l1: 0.121652\n",
      "[2700]\ttraining's l1: 0.0669744\tvalid_1's l1: 0.120932\n",
      "[2800]\ttraining's l1: 0.0654309\tvalid_1's l1: 0.120277\n",
      "[2900]\ttraining's l1: 0.0639949\tvalid_1's l1: 0.11967\n",
      "[3000]\ttraining's l1: 0.0626066\tvalid_1's l1: 0.119101\n",
      "[3100]\ttraining's l1: 0.061263\tvalid_1's l1: 0.118563\n",
      "[3200]\ttraining's l1: 0.0600014\tvalid_1's l1: 0.118028\n",
      "[3300]\ttraining's l1: 0.0587846\tvalid_1's l1: 0.117546\n",
      "[3400]\ttraining's l1: 0.0575836\tvalid_1's l1: 0.117074\n",
      "[3500]\ttraining's l1: 0.0564257\tvalid_1's l1: 0.116627\n",
      "[3600]\ttraining's l1: 0.0553045\tvalid_1's l1: 0.116186\n",
      "[3700]\ttraining's l1: 0.0542418\tvalid_1's l1: 0.11578\n",
      "[3800]\ttraining's l1: 0.0532343\tvalid_1's l1: 0.115414\n",
      "[3900]\ttraining's l1: 0.052219\tvalid_1's l1: 0.115015\n",
      "[4000]\ttraining's l1: 0.0512162\tvalid_1's l1: 0.114616\n",
      "[4100]\ttraining's l1: 0.0502905\tvalid_1's l1: 0.114256\n",
      "[4200]\ttraining's l1: 0.0494023\tvalid_1's l1: 0.113946\n",
      "[4300]\ttraining's l1: 0.048505\tvalid_1's l1: 0.113601\n",
      "[4400]\ttraining's l1: 0.0476465\tvalid_1's l1: 0.113293\n",
      "[4500]\ttraining's l1: 0.0468127\tvalid_1's l1: 0.112971\n",
      "[4600]\ttraining's l1: 0.0459906\tvalid_1's l1: 0.112704\n",
      "[4700]\ttraining's l1: 0.0452052\tvalid_1's l1: 0.11242\n",
      "[4800]\ttraining's l1: 0.044423\tvalid_1's l1: 0.112155\n",
      "[4900]\ttraining's l1: 0.0436787\tvalid_1's l1: 0.111924\n",
      "[5000]\ttraining's l1: 0.0429542\tvalid_1's l1: 0.111726\n",
      "[5100]\ttraining's l1: 0.0422469\tvalid_1's l1: 0.1115\n",
      "[5200]\ttraining's l1: 0.0415625\tvalid_1's l1: 0.111281\n",
      "[5300]\ttraining's l1: 0.0409051\tvalid_1's l1: 0.111088\n",
      "[5400]\ttraining's l1: 0.0402655\tvalid_1's l1: 0.110895\n",
      "[5500]\ttraining's l1: 0.0396322\tvalid_1's l1: 0.110697\n",
      "[5600]\ttraining's l1: 0.0389994\tvalid_1's l1: 0.11049\n",
      "[5700]\ttraining's l1: 0.0383867\tvalid_1's l1: 0.110315\n",
      "[5800]\ttraining's l1: 0.0377788\tvalid_1's l1: 0.110138\n",
      "[5900]\ttraining's l1: 0.0372141\tvalid_1's l1: 0.109993\n",
      "[6000]\ttraining's l1: 0.036644\tvalid_1's l1: 0.109842\n",
      "[6100]\ttraining's l1: 0.0360945\tvalid_1's l1: 0.109707\n",
      "[6200]\ttraining's l1: 0.0355674\tvalid_1's l1: 0.109586\n",
      "[6300]\ttraining's l1: 0.0350403\tvalid_1's l1: 0.109433\n",
      "[6400]\ttraining's l1: 0.03453\tvalid_1's l1: 0.109299\n",
      "[6500]\ttraining's l1: 0.0340307\tvalid_1's l1: 0.109158\n",
      "[6600]\ttraining's l1: 0.0335255\tvalid_1's l1: 0.109016\n",
      "[6700]\ttraining's l1: 0.0330386\tvalid_1's l1: 0.108893\n",
      "[6800]\ttraining's l1: 0.0325653\tvalid_1's l1: 0.108757\n",
      "[6900]\ttraining's l1: 0.0321085\tvalid_1's l1: 0.108632\n",
      "[7000]\ttraining's l1: 0.0316451\tvalid_1's l1: 0.1085\n",
      "[7100]\ttraining's l1: 0.0311988\tvalid_1's l1: 0.108372\n",
      "[7200]\ttraining's l1: 0.0307755\tvalid_1's l1: 0.108283\n",
      "[7300]\ttraining's l1: 0.0303512\tvalid_1's l1: 0.108179\n",
      "[7400]\ttraining's l1: 0.0299336\tvalid_1's l1: 0.108066\n",
      "[7500]\ttraining's l1: 0.029531\tvalid_1's l1: 0.107957\n",
      "[7600]\ttraining's l1: 0.0291194\tvalid_1's l1: 0.107844\n",
      "[7700]\ttraining's l1: 0.0287266\tvalid_1's l1: 0.107761\n",
      "[7800]\ttraining's l1: 0.0283517\tvalid_1's l1: 0.107677\n",
      "[7900]\ttraining's l1: 0.0279883\tvalid_1's l1: 0.107609\n",
      "[8000]\ttraining's l1: 0.0276155\tvalid_1's l1: 0.107499\n",
      "[8100]\ttraining's l1: 0.0272483\tvalid_1's l1: 0.107413\n",
      "[8200]\ttraining's l1: 0.0268941\tvalid_1's l1: 0.107332\n",
      "[8300]\ttraining's l1: 0.0265531\tvalid_1's l1: 0.107241\n",
      "[8400]\ttraining's l1: 0.0262014\tvalid_1's l1: 0.107149\n",
      "[8500]\ttraining's l1: 0.0258722\tvalid_1's l1: 0.107064\n",
      "[8600]\ttraining's l1: 0.0255408\tvalid_1's l1: 0.106978\n",
      "[8700]\ttraining's l1: 0.0252108\tvalid_1's l1: 0.106907\n",
      "[8800]\ttraining's l1: 0.0249013\tvalid_1's l1: 0.106834\n",
      "[8900]\ttraining's l1: 0.0245928\tvalid_1's l1: 0.106777\n",
      "[9000]\ttraining's l1: 0.0242831\tvalid_1's l1: 0.106711\n",
      "[9100]\ttraining's l1: 0.0239829\tvalid_1's l1: 0.106649\n",
      "[9200]\ttraining's l1: 0.0236766\tvalid_1's l1: 0.106579\n",
      "[9300]\ttraining's l1: 0.0233841\tvalid_1's l1: 0.106516\n",
      "[9400]\ttraining's l1: 0.0231059\tvalid_1's l1: 0.106461\n",
      "[9500]\ttraining's l1: 0.0228154\tvalid_1's l1: 0.106392\n",
      "[9600]\ttraining's l1: 0.0225384\tvalid_1's l1: 0.106322\n",
      "[9700]\ttraining's l1: 0.0222569\tvalid_1's l1: 0.106258\n",
      "[9800]\ttraining's l1: 0.0219895\tvalid_1's l1: 0.106202\n",
      "[9900]\ttraining's l1: 0.0217328\tvalid_1's l1: 0.106156\n",
      "[10000]\ttraining's l1: 0.0214746\tvalid_1's l1: 0.106096\n",
      "[10100]\ttraining's l1: 0.0212273\tvalid_1's l1: 0.106042\n",
      "[10200]\ttraining's l1: 0.0209866\tvalid_1's l1: 0.105984\n",
      "[10300]\ttraining's l1: 0.0207375\tvalid_1's l1: 0.105938\n",
      "[10400]\ttraining's l1: 0.0204919\tvalid_1's l1: 0.10589\n",
      "[10500]\ttraining's l1: 0.0202505\tvalid_1's l1: 0.10584\n",
      "[10600]\ttraining's l1: 0.0200178\tvalid_1's l1: 0.10579\n",
      "[10700]\ttraining's l1: 0.0197904\tvalid_1's l1: 0.105746\n",
      "[10800]\ttraining's l1: 0.0195656\tvalid_1's l1: 0.105712\n",
      "[10900]\ttraining's l1: 0.0193426\tvalid_1's l1: 0.105659\n",
      "[11000]\ttraining's l1: 0.0191168\tvalid_1's l1: 0.105624\n",
      "[11100]\ttraining's l1: 0.0188917\tvalid_1's l1: 0.105585\n",
      "[11200]\ttraining's l1: 0.0186783\tvalid_1's l1: 0.105548\n",
      "[11300]\ttraining's l1: 0.0184676\tvalid_1's l1: 0.105505\n",
      "[11400]\ttraining's l1: 0.0182532\tvalid_1's l1: 0.105466\n",
      "[11500]\ttraining's l1: 0.018044\tvalid_1's l1: 0.105425\n",
      "[11600]\ttraining's l1: 0.0178295\tvalid_1's l1: 0.105387\n",
      "[11700]\ttraining's l1: 0.017632\tvalid_1's l1: 0.105345\n",
      "[11800]\ttraining's l1: 0.0174371\tvalid_1's l1: 0.105305\n",
      "[11900]\ttraining's l1: 0.0172524\tvalid_1's l1: 0.105265\n",
      "[12000]\ttraining's l1: 0.0170711\tvalid_1's l1: 0.10523\n",
      "[12100]\ttraining's l1: 0.0168841\tvalid_1's l1: 0.105202\n",
      "[12200]\ttraining's l1: 0.0167002\tvalid_1's l1: 0.105165\n",
      "[12300]\ttraining's l1: 0.0165161\tvalid_1's l1: 0.105133\n",
      "[12400]\ttraining's l1: 0.0163274\tvalid_1's l1: 0.105098\n",
      "[12500]\ttraining's l1: 0.0161421\tvalid_1's l1: 0.105062\n",
      "[12600]\ttraining's l1: 0.0159684\tvalid_1's l1: 0.105025\n",
      "[12700]\ttraining's l1: 0.0157969\tvalid_1's l1: 0.104992\n",
      "[12800]\ttraining's l1: 0.0156231\tvalid_1's l1: 0.104955\n",
      "[12900]\ttraining's l1: 0.0154572\tvalid_1's l1: 0.104925\n",
      "[13000]\ttraining's l1: 0.0152986\tvalid_1's l1: 0.104888\n",
      "[13100]\ttraining's l1: 0.0151393\tvalid_1's l1: 0.104863\n",
      "[13200]\ttraining's l1: 0.0149748\tvalid_1's l1: 0.104835\n",
      "[13300]\ttraining's l1: 0.0148182\tvalid_1's l1: 0.104804\n",
      "[13400]\ttraining's l1: 0.0146585\tvalid_1's l1: 0.104777\n",
      "[13500]\ttraining's l1: 0.0145035\tvalid_1's l1: 0.104752\n",
      "[13600]\ttraining's l1: 0.0143511\tvalid_1's l1: 0.104723\n",
      "[13700]\ttraining's l1: 0.0141966\tvalid_1's l1: 0.104697\n",
      "[13800]\ttraining's l1: 0.0140534\tvalid_1's l1: 0.104676\n",
      "[13900]\ttraining's l1: 0.0139003\tvalid_1's l1: 0.104647\n",
      "[14000]\ttraining's l1: 0.0137549\tvalid_1's l1: 0.104617\n",
      "[14100]\ttraining's l1: 0.0136121\tvalid_1's l1: 0.104593\n",
      "[14200]\ttraining's l1: 0.013473\tvalid_1's l1: 0.104567\n",
      "[14300]\ttraining's l1: 0.0133348\tvalid_1's l1: 0.104546\n",
      "[14400]\ttraining's l1: 0.0131949\tvalid_1's l1: 0.104526\n",
      "[14500]\ttraining's l1: 0.013051\tvalid_1's l1: 0.104512\n",
      "[14600]\ttraining's l1: 0.012917\tvalid_1's l1: 0.104486\n",
      "[14700]\ttraining's l1: 0.0127845\tvalid_1's l1: 0.104467\n",
      "[14800]\ttraining's l1: 0.0126588\tvalid_1's l1: 0.104445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14900]\ttraining's l1: 0.012536\tvalid_1's l1: 0.10442\n",
      "[15000]\ttraining's l1: 0.0124147\tvalid_1's l1: 0.104392\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0124147\tvalid_1's l1: 0.104392\n",
      "3JHN Fold 0, logMAE: -2.2596033872504275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.246936\tvalid_1's l1: 0.25374\n",
      "[200]\ttraining's l1: 0.203471\tvalid_1's l1: 0.214059\n",
      "[300]\ttraining's l1: 0.178559\tvalid_1's l1: 0.192565\n",
      "[400]\ttraining's l1: 0.162628\tvalid_1's l1: 0.179594\n",
      "[500]\ttraining's l1: 0.150121\tvalid_1's l1: 0.16995\n",
      "[600]\ttraining's l1: 0.140294\tvalid_1's l1: 0.162772\n",
      "[700]\ttraining's l1: 0.132112\tvalid_1's l1: 0.156889\n",
      "[800]\ttraining's l1: 0.124993\tvalid_1's l1: 0.151843\n",
      "[900]\ttraining's l1: 0.119094\tvalid_1's l1: 0.147913\n",
      "[1000]\ttraining's l1: 0.113663\tvalid_1's l1: 0.144381\n",
      "[1100]\ttraining's l1: 0.10902\tvalid_1's l1: 0.141566\n",
      "[1200]\ttraining's l1: 0.104602\tvalid_1's l1: 0.138897\n",
      "[1300]\ttraining's l1: 0.100567\tvalid_1's l1: 0.136376\n",
      "[1400]\ttraining's l1: 0.0970966\tvalid_1's l1: 0.134377\n",
      "[1500]\ttraining's l1: 0.0938106\tvalid_1's l1: 0.132566\n",
      "[1600]\ttraining's l1: 0.090806\tvalid_1's l1: 0.130859\n",
      "[1700]\ttraining's l1: 0.0880181\tvalid_1's l1: 0.129436\n",
      "[1800]\ttraining's l1: 0.0853207\tvalid_1's l1: 0.127973\n",
      "[1900]\ttraining's l1: 0.0828015\tvalid_1's l1: 0.126654\n",
      "[2000]\ttraining's l1: 0.0804142\tvalid_1's l1: 0.125376\n",
      "[2100]\ttraining's l1: 0.0782639\tvalid_1's l1: 0.124354\n",
      "[2200]\ttraining's l1: 0.0761117\tvalid_1's l1: 0.123246\n",
      "[2300]\ttraining's l1: 0.0740629\tvalid_1's l1: 0.122292\n",
      "[2400]\ttraining's l1: 0.0722514\tvalid_1's l1: 0.121443\n",
      "[2500]\ttraining's l1: 0.0704514\tvalid_1's l1: 0.120603\n",
      "[2600]\ttraining's l1: 0.0687734\tvalid_1's l1: 0.119784\n",
      "[2700]\ttraining's l1: 0.0671246\tvalid_1's l1: 0.11907\n",
      "[2800]\ttraining's l1: 0.0655996\tvalid_1's l1: 0.118431\n",
      "[2900]\ttraining's l1: 0.0641228\tvalid_1's l1: 0.117777\n",
      "[3000]\ttraining's l1: 0.0626448\tvalid_1's l1: 0.117136\n",
      "[3100]\ttraining's l1: 0.0612721\tvalid_1's l1: 0.116604\n",
      "[3200]\ttraining's l1: 0.0599832\tvalid_1's l1: 0.116052\n",
      "[3300]\ttraining's l1: 0.0587111\tvalid_1's l1: 0.115528\n",
      "[3400]\ttraining's l1: 0.0575287\tvalid_1's l1: 0.115068\n",
      "[3500]\ttraining's l1: 0.0563713\tvalid_1's l1: 0.11462\n",
      "[3600]\ttraining's l1: 0.0552722\tvalid_1's l1: 0.114174\n",
      "[3700]\ttraining's l1: 0.0542301\tvalid_1's l1: 0.113813\n",
      "[3800]\ttraining's l1: 0.0532256\tvalid_1's l1: 0.113394\n",
      "[3900]\ttraining's l1: 0.0522193\tvalid_1's l1: 0.112984\n",
      "[4000]\ttraining's l1: 0.0512274\tvalid_1's l1: 0.112662\n",
      "[4100]\ttraining's l1: 0.0502929\tvalid_1's l1: 0.112345\n",
      "[4200]\ttraining's l1: 0.0494226\tvalid_1's l1: 0.112062\n",
      "[4300]\ttraining's l1: 0.0485314\tvalid_1's l1: 0.111728\n",
      "[4400]\ttraining's l1: 0.0476808\tvalid_1's l1: 0.111397\n",
      "[4500]\ttraining's l1: 0.0468457\tvalid_1's l1: 0.111137\n",
      "[4600]\ttraining's l1: 0.0460445\tvalid_1's l1: 0.11085\n",
      "[4700]\ttraining's l1: 0.0452754\tvalid_1's l1: 0.110619\n",
      "[4800]\ttraining's l1: 0.0444659\tvalid_1's l1: 0.110334\n",
      "[4900]\ttraining's l1: 0.0437138\tvalid_1's l1: 0.110123\n",
      "[5000]\ttraining's l1: 0.0429906\tvalid_1's l1: 0.109916\n",
      "[5100]\ttraining's l1: 0.0422831\tvalid_1's l1: 0.109694\n",
      "[5200]\ttraining's l1: 0.0415663\tvalid_1's l1: 0.109466\n",
      "[5300]\ttraining's l1: 0.0408867\tvalid_1's l1: 0.109246\n",
      "[5400]\ttraining's l1: 0.0402078\tvalid_1's l1: 0.109028\n",
      "[5500]\ttraining's l1: 0.0395401\tvalid_1's l1: 0.108832\n",
      "[5600]\ttraining's l1: 0.0389084\tvalid_1's l1: 0.108634\n",
      "[5700]\ttraining's l1: 0.0383066\tvalid_1's l1: 0.108464\n",
      "[5800]\ttraining's l1: 0.0377193\tvalid_1's l1: 0.10827\n",
      "[5900]\ttraining's l1: 0.0371572\tvalid_1's l1: 0.108102\n",
      "[6000]\ttraining's l1: 0.0366139\tvalid_1's l1: 0.107945\n",
      "[6100]\ttraining's l1: 0.0360523\tvalid_1's l1: 0.107781\n",
      "[6200]\ttraining's l1: 0.0355167\tvalid_1's l1: 0.107609\n",
      "[6300]\ttraining's l1: 0.0349781\tvalid_1's l1: 0.107444\n",
      "[6400]\ttraining's l1: 0.0344559\tvalid_1's l1: 0.107305\n",
      "[6500]\ttraining's l1: 0.033969\tvalid_1's l1: 0.107179\n",
      "[6600]\ttraining's l1: 0.0334699\tvalid_1's l1: 0.107047\n",
      "[6700]\ttraining's l1: 0.0330174\tvalid_1's l1: 0.106906\n",
      "[6800]\ttraining's l1: 0.0325274\tvalid_1's l1: 0.106759\n",
      "[6900]\ttraining's l1: 0.0320609\tvalid_1's l1: 0.106623\n",
      "[7000]\ttraining's l1: 0.0316026\tvalid_1's l1: 0.106491\n",
      "[7100]\ttraining's l1: 0.031149\tvalid_1's l1: 0.106356\n",
      "[7200]\ttraining's l1: 0.0307045\tvalid_1's l1: 0.106245\n",
      "[7300]\ttraining's l1: 0.030278\tvalid_1's l1: 0.106143\n",
      "[7400]\ttraining's l1: 0.0298736\tvalid_1's l1: 0.106032\n",
      "[7500]\ttraining's l1: 0.0294769\tvalid_1's l1: 0.105914\n",
      "[7600]\ttraining's l1: 0.0290712\tvalid_1's l1: 0.105818\n",
      "[7700]\ttraining's l1: 0.0286819\tvalid_1's l1: 0.105716\n",
      "[7800]\ttraining's l1: 0.0282911\tvalid_1's l1: 0.105619\n",
      "[7900]\ttraining's l1: 0.0279062\tvalid_1's l1: 0.105523\n",
      "[8000]\ttraining's l1: 0.0275249\tvalid_1's l1: 0.105424\n",
      "[8100]\ttraining's l1: 0.0271647\tvalid_1's l1: 0.105318\n",
      "[8200]\ttraining's l1: 0.0268039\tvalid_1's l1: 0.10523\n",
      "[8300]\ttraining's l1: 0.026449\tvalid_1's l1: 0.105158\n",
      "[8400]\ttraining's l1: 0.0260988\tvalid_1's l1: 0.105065\n",
      "[8500]\ttraining's l1: 0.0257683\tvalid_1's l1: 0.104985\n",
      "[8600]\ttraining's l1: 0.0254451\tvalid_1's l1: 0.104906\n",
      "[8700]\ttraining's l1: 0.0251151\tvalid_1's l1: 0.104823\n",
      "[8800]\ttraining's l1: 0.024795\tvalid_1's l1: 0.104746\n",
      "[8900]\ttraining's l1: 0.0244851\tvalid_1's l1: 0.104672\n",
      "[9000]\ttraining's l1: 0.0241713\tvalid_1's l1: 0.104586\n",
      "[9100]\ttraining's l1: 0.0238698\tvalid_1's l1: 0.104517\n",
      "[9200]\ttraining's l1: 0.0235728\tvalid_1's l1: 0.104437\n",
      "[9300]\ttraining's l1: 0.0232754\tvalid_1's l1: 0.104384\n",
      "[9400]\ttraining's l1: 0.0229964\tvalid_1's l1: 0.104325\n",
      "[9500]\ttraining's l1: 0.0227038\tvalid_1's l1: 0.104253\n",
      "[9600]\ttraining's l1: 0.0224327\tvalid_1's l1: 0.104194\n",
      "[9700]\ttraining's l1: 0.0221647\tvalid_1's l1: 0.104147\n",
      "[9800]\ttraining's l1: 0.0218985\tvalid_1's l1: 0.104089\n",
      "[9900]\ttraining's l1: 0.021631\tvalid_1's l1: 0.104023\n",
      "[10000]\ttraining's l1: 0.0213746\tvalid_1's l1: 0.103957\n",
      "[10100]\ttraining's l1: 0.0211144\tvalid_1's l1: 0.103906\n",
      "[10200]\ttraining's l1: 0.0208719\tvalid_1's l1: 0.103849\n",
      "[10300]\ttraining's l1: 0.0206221\tvalid_1's l1: 0.103791\n",
      "[10400]\ttraining's l1: 0.0203749\tvalid_1's l1: 0.103739\n",
      "[10500]\ttraining's l1: 0.0201409\tvalid_1's l1: 0.10368\n",
      "[10600]\ttraining's l1: 0.0199076\tvalid_1's l1: 0.103628\n",
      "[10700]\ttraining's l1: 0.0196819\tvalid_1's l1: 0.103578\n",
      "[10800]\ttraining's l1: 0.0194504\tvalid_1's l1: 0.103534\n",
      "[10900]\ttraining's l1: 0.0192246\tvalid_1's l1: 0.103487\n",
      "[11000]\ttraining's l1: 0.0190012\tvalid_1's l1: 0.103438\n",
      "[11100]\ttraining's l1: 0.018785\tvalid_1's l1: 0.10339\n",
      "[11200]\ttraining's l1: 0.018575\tvalid_1's l1: 0.103353\n",
      "[11300]\ttraining's l1: 0.0183643\tvalid_1's l1: 0.103319\n",
      "[11400]\ttraining's l1: 0.018152\tvalid_1's l1: 0.103279\n",
      "[11500]\ttraining's l1: 0.0179626\tvalid_1's l1: 0.103237\n",
      "[11600]\ttraining's l1: 0.0177655\tvalid_1's l1: 0.103193\n",
      "[11700]\ttraining's l1: 0.0175742\tvalid_1's l1: 0.103153\n",
      "[11800]\ttraining's l1: 0.0173819\tvalid_1's l1: 0.103113\n",
      "[11900]\ttraining's l1: 0.0171831\tvalid_1's l1: 0.103077\n",
      "[12000]\ttraining's l1: 0.0169948\tvalid_1's l1: 0.103047\n",
      "[12100]\ttraining's l1: 0.0168079\tvalid_1's l1: 0.103013\n",
      "[12200]\ttraining's l1: 0.0166229\tvalid_1's l1: 0.102984\n",
      "[12300]\ttraining's l1: 0.016437\tvalid_1's l1: 0.102945\n",
      "[12400]\ttraining's l1: 0.0162601\tvalid_1's l1: 0.102904\n",
      "[12500]\ttraining's l1: 0.0160786\tvalid_1's l1: 0.10286\n",
      "[12600]\ttraining's l1: 0.0159017\tvalid_1's l1: 0.102825\n",
      "[12700]\ttraining's l1: 0.0157321\tvalid_1's l1: 0.10279\n",
      "[12800]\ttraining's l1: 0.0155683\tvalid_1's l1: 0.10276\n",
      "[12900]\ttraining's l1: 0.0154027\tvalid_1's l1: 0.102733\n",
      "[13000]\ttraining's l1: 0.0152315\tvalid_1's l1: 0.102706\n",
      "[13100]\ttraining's l1: 0.0150644\tvalid_1's l1: 0.102688\n",
      "[13200]\ttraining's l1: 0.0149089\tvalid_1's l1: 0.102664\n",
      "[13300]\ttraining's l1: 0.0147566\tvalid_1's l1: 0.102637\n",
      "[13400]\ttraining's l1: 0.0145958\tvalid_1's l1: 0.102608\n",
      "[13500]\ttraining's l1: 0.0144442\tvalid_1's l1: 0.102587\n",
      "[13600]\ttraining's l1: 0.0142914\tvalid_1's l1: 0.102561\n",
      "[13700]\ttraining's l1: 0.0141425\tvalid_1's l1: 0.102533\n",
      "[13800]\ttraining's l1: 0.0140045\tvalid_1's l1: 0.102509\n",
      "[13900]\ttraining's l1: 0.0138702\tvalid_1's l1: 0.102486\n",
      "[14000]\ttraining's l1: 0.0137284\tvalid_1's l1: 0.102458\n",
      "[14100]\ttraining's l1: 0.0135888\tvalid_1's l1: 0.102434\n",
      "[14200]\ttraining's l1: 0.0134459\tvalid_1's l1: 0.102411\n",
      "[14300]\ttraining's l1: 0.013316\tvalid_1's l1: 0.102383\n",
      "[14400]\ttraining's l1: 0.0131804\tvalid_1's l1: 0.102353\n",
      "[14500]\ttraining's l1: 0.0130463\tvalid_1's l1: 0.102331\n",
      "[14600]\ttraining's l1: 0.0129119\tvalid_1's l1: 0.102304\n",
      "[14700]\ttraining's l1: 0.0127801\tvalid_1's l1: 0.102286\n",
      "[14800]\ttraining's l1: 0.0126518\tvalid_1's l1: 0.102263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14900]\ttraining's l1: 0.0125251\tvalid_1's l1: 0.102238\n",
      "[15000]\ttraining's l1: 0.0123983\tvalid_1's l1: 0.102219\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0123983\tvalid_1's l1: 0.102219\n",
      "3JHN Fold 1, logMAE: -2.28063630117181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.245869\tvalid_1's l1: 0.24989\n",
      "[200]\ttraining's l1: 0.201671\tvalid_1's l1: 0.210456\n",
      "[300]\ttraining's l1: 0.177836\tvalid_1's l1: 0.191114\n",
      "[400]\ttraining's l1: 0.161364\tvalid_1's l1: 0.178552\n",
      "[500]\ttraining's l1: 0.14883\tvalid_1's l1: 0.169076\n",
      "[600]\ttraining's l1: 0.139242\tvalid_1's l1: 0.162176\n",
      "[700]\ttraining's l1: 0.131075\tvalid_1's l1: 0.15645\n",
      "[800]\ttraining's l1: 0.124211\tvalid_1's l1: 0.151726\n",
      "[900]\ttraining's l1: 0.118295\tvalid_1's l1: 0.147956\n",
      "[1000]\ttraining's l1: 0.113119\tvalid_1's l1: 0.144715\n",
      "[1100]\ttraining's l1: 0.108529\tvalid_1's l1: 0.141831\n",
      "[1200]\ttraining's l1: 0.10426\tvalid_1's l1: 0.139324\n",
      "[1300]\ttraining's l1: 0.100306\tvalid_1's l1: 0.137067\n",
      "[1400]\ttraining's l1: 0.0966595\tvalid_1's l1: 0.135043\n",
      "[1500]\ttraining's l1: 0.0934315\tvalid_1's l1: 0.133258\n",
      "[1600]\ttraining's l1: 0.0903962\tvalid_1's l1: 0.131695\n",
      "[1700]\ttraining's l1: 0.0875826\tvalid_1's l1: 0.130133\n",
      "[1800]\ttraining's l1: 0.084915\tvalid_1's l1: 0.128743\n",
      "[1900]\ttraining's l1: 0.0824628\tvalid_1's l1: 0.127572\n",
      "[2000]\ttraining's l1: 0.0801445\tvalid_1's l1: 0.1264\n",
      "[2100]\ttraining's l1: 0.0779827\tvalid_1's l1: 0.12538\n",
      "[2200]\ttraining's l1: 0.0759101\tvalid_1's l1: 0.12442\n",
      "[2300]\ttraining's l1: 0.0738926\tvalid_1's l1: 0.123413\n",
      "[2400]\ttraining's l1: 0.0720235\tvalid_1's l1: 0.12254\n",
      "[2500]\ttraining's l1: 0.07025\tvalid_1's l1: 0.121702\n",
      "[2600]\ttraining's l1: 0.0686323\tvalid_1's l1: 0.121022\n",
      "[2700]\ttraining's l1: 0.0670591\tvalid_1's l1: 0.12033\n",
      "[2800]\ttraining's l1: 0.0654797\tvalid_1's l1: 0.119674\n",
      "[2900]\ttraining's l1: 0.0640311\tvalid_1's l1: 0.119136\n",
      "[3000]\ttraining's l1: 0.0626039\tvalid_1's l1: 0.118535\n",
      "[3100]\ttraining's l1: 0.0612179\tvalid_1's l1: 0.117935\n",
      "[3200]\ttraining's l1: 0.0599229\tvalid_1's l1: 0.117421\n",
      "[3300]\ttraining's l1: 0.0586864\tvalid_1's l1: 0.116954\n",
      "[3400]\ttraining's l1: 0.0574603\tvalid_1's l1: 0.116448\n",
      "[3500]\ttraining's l1: 0.056307\tvalid_1's l1: 0.115997\n",
      "[3600]\ttraining's l1: 0.0552127\tvalid_1's l1: 0.115588\n",
      "[3700]\ttraining's l1: 0.0541454\tvalid_1's l1: 0.115192\n",
      "[3800]\ttraining's l1: 0.0531018\tvalid_1's l1: 0.11479\n",
      "[3900]\ttraining's l1: 0.0520988\tvalid_1's l1: 0.114392\n",
      "[4000]\ttraining's l1: 0.0511187\tvalid_1's l1: 0.114018\n",
      "[4100]\ttraining's l1: 0.0501408\tvalid_1's l1: 0.113638\n",
      "[4200]\ttraining's l1: 0.0492356\tvalid_1's l1: 0.113329\n",
      "[4300]\ttraining's l1: 0.0483352\tvalid_1's l1: 0.112992\n",
      "[4400]\ttraining's l1: 0.0474654\tvalid_1's l1: 0.11266\n",
      "[4500]\ttraining's l1: 0.0466404\tvalid_1's l1: 0.112377\n",
      "[4600]\ttraining's l1: 0.0457987\tvalid_1's l1: 0.112104\n",
      "[4700]\ttraining's l1: 0.0450366\tvalid_1's l1: 0.11185\n",
      "[4800]\ttraining's l1: 0.0442287\tvalid_1's l1: 0.111544\n",
      "[4900]\ttraining's l1: 0.0435024\tvalid_1's l1: 0.111287\n",
      "[5000]\ttraining's l1: 0.0427787\tvalid_1's l1: 0.111029\n",
      "[5100]\ttraining's l1: 0.0420834\tvalid_1's l1: 0.110795\n",
      "[5200]\ttraining's l1: 0.0413802\tvalid_1's l1: 0.110561\n",
      "[5300]\ttraining's l1: 0.0407334\tvalid_1's l1: 0.110375\n",
      "[5400]\ttraining's l1: 0.0400823\tvalid_1's l1: 0.110181\n",
      "[5500]\ttraining's l1: 0.0394526\tvalid_1's l1: 0.109989\n",
      "[5600]\ttraining's l1: 0.03884\tvalid_1's l1: 0.109799\n",
      "[5700]\ttraining's l1: 0.0382467\tvalid_1's l1: 0.109636\n",
      "[5800]\ttraining's l1: 0.0376637\tvalid_1's l1: 0.109442\n",
      "[5900]\ttraining's l1: 0.0371018\tvalid_1's l1: 0.109275\n",
      "[6000]\ttraining's l1: 0.0365275\tvalid_1's l1: 0.109122\n",
      "[6100]\ttraining's l1: 0.0359862\tvalid_1's l1: 0.108958\n",
      "[6200]\ttraining's l1: 0.0354289\tvalid_1's l1: 0.108779\n",
      "[6300]\ttraining's l1: 0.0348921\tvalid_1's l1: 0.108642\n",
      "[6400]\ttraining's l1: 0.0343739\tvalid_1's l1: 0.108497\n",
      "[6500]\ttraining's l1: 0.0338642\tvalid_1's l1: 0.10835\n",
      "[6600]\ttraining's l1: 0.0333757\tvalid_1's l1: 0.108213\n",
      "[6700]\ttraining's l1: 0.0329074\tvalid_1's l1: 0.108106\n",
      "[6800]\ttraining's l1: 0.0324319\tvalid_1's l1: 0.107981\n",
      "[6900]\ttraining's l1: 0.0319652\tvalid_1's l1: 0.107858\n",
      "[7000]\ttraining's l1: 0.0315153\tvalid_1's l1: 0.107751\n",
      "[7100]\ttraining's l1: 0.0310524\tvalid_1's l1: 0.107648\n",
      "[7200]\ttraining's l1: 0.0306273\tvalid_1's l1: 0.107538\n",
      "[7300]\ttraining's l1: 0.0302107\tvalid_1's l1: 0.107419\n",
      "[7400]\ttraining's l1: 0.0297999\tvalid_1's l1: 0.107331\n",
      "[7500]\ttraining's l1: 0.0293972\tvalid_1's l1: 0.107227\n",
      "[7600]\ttraining's l1: 0.0290006\tvalid_1's l1: 0.107117\n",
      "[7700]\ttraining's l1: 0.0286143\tvalid_1's l1: 0.107034\n",
      "[7800]\ttraining's l1: 0.0282257\tvalid_1's l1: 0.106933\n",
      "[7900]\ttraining's l1: 0.0278562\tvalid_1's l1: 0.106817\n",
      "[8000]\ttraining's l1: 0.0274917\tvalid_1's l1: 0.106719\n",
      "[8100]\ttraining's l1: 0.0271487\tvalid_1's l1: 0.106643\n",
      "[8200]\ttraining's l1: 0.0267902\tvalid_1's l1: 0.106558\n",
      "[8300]\ttraining's l1: 0.0264627\tvalid_1's l1: 0.106473\n",
      "[8400]\ttraining's l1: 0.0261312\tvalid_1's l1: 0.106393\n",
      "[8500]\ttraining's l1: 0.025795\tvalid_1's l1: 0.106321\n",
      "[8600]\ttraining's l1: 0.0254702\tvalid_1's l1: 0.106243\n",
      "[8700]\ttraining's l1: 0.0251401\tvalid_1's l1: 0.106159\n",
      "[8800]\ttraining's l1: 0.0248145\tvalid_1's l1: 0.106089\n",
      "[8900]\ttraining's l1: 0.0245024\tvalid_1's l1: 0.106016\n",
      "[9000]\ttraining's l1: 0.0241966\tvalid_1's l1: 0.105962\n",
      "[9100]\ttraining's l1: 0.0238907\tvalid_1's l1: 0.105894\n",
      "[9200]\ttraining's l1: 0.0235835\tvalid_1's l1: 0.105832\n",
      "[9300]\ttraining's l1: 0.0232849\tvalid_1's l1: 0.105763\n",
      "[9400]\ttraining's l1: 0.0229976\tvalid_1's l1: 0.105703\n",
      "[9500]\ttraining's l1: 0.0227168\tvalid_1's l1: 0.105643\n",
      "[9600]\ttraining's l1: 0.0224482\tvalid_1's l1: 0.105595\n",
      "[9700]\ttraining's l1: 0.0221578\tvalid_1's l1: 0.105544\n",
      "[9800]\ttraining's l1: 0.0218894\tvalid_1's l1: 0.105486\n",
      "[9900]\ttraining's l1: 0.0216208\tvalid_1's l1: 0.105441\n",
      "[10000]\ttraining's l1: 0.0213567\tvalid_1's l1: 0.1054\n",
      "[10100]\ttraining's l1: 0.0211055\tvalid_1's l1: 0.105363\n",
      "[10200]\ttraining's l1: 0.0208432\tvalid_1's l1: 0.105321\n",
      "[10300]\ttraining's l1: 0.02059\tvalid_1's l1: 0.105273\n",
      "[10400]\ttraining's l1: 0.0203415\tvalid_1's l1: 0.105219\n",
      "[10500]\ttraining's l1: 0.020111\tvalid_1's l1: 0.105168\n",
      "[10600]\ttraining's l1: 0.0198902\tvalid_1's l1: 0.105134\n",
      "[10700]\ttraining's l1: 0.0196574\tvalid_1's l1: 0.105088\n",
      "[10800]\ttraining's l1: 0.019436\tvalid_1's l1: 0.105042\n",
      "[10900]\ttraining's l1: 0.0192146\tvalid_1's l1: 0.105006\n",
      "[11000]\ttraining's l1: 0.0190026\tvalid_1's l1: 0.10496\n",
      "[11100]\ttraining's l1: 0.0187907\tvalid_1's l1: 0.104911\n",
      "[11200]\ttraining's l1: 0.0185844\tvalid_1's l1: 0.104876\n",
      "[11300]\ttraining's l1: 0.0183647\tvalid_1's l1: 0.104844\n",
      "[11400]\ttraining's l1: 0.0181636\tvalid_1's l1: 0.104804\n",
      "[11500]\ttraining's l1: 0.017962\tvalid_1's l1: 0.104758\n",
      "[11600]\ttraining's l1: 0.0177598\tvalid_1's l1: 0.10472\n",
      "[11700]\ttraining's l1: 0.0175618\tvalid_1's l1: 0.104693\n",
      "[11800]\ttraining's l1: 0.0173644\tvalid_1's l1: 0.104648\n",
      "[11900]\ttraining's l1: 0.0171755\tvalid_1's l1: 0.10461\n",
      "[12000]\ttraining's l1: 0.0169886\tvalid_1's l1: 0.10457\n",
      "[12100]\ttraining's l1: 0.0167989\tvalid_1's l1: 0.104541\n",
      "[12200]\ttraining's l1: 0.0166125\tvalid_1's l1: 0.104505\n",
      "[12300]\ttraining's l1: 0.0164271\tvalid_1's l1: 0.104467\n",
      "[12400]\ttraining's l1: 0.0162352\tvalid_1's l1: 0.104428\n",
      "[12500]\ttraining's l1: 0.0160506\tvalid_1's l1: 0.104388\n",
      "[12600]\ttraining's l1: 0.0158743\tvalid_1's l1: 0.104354\n",
      "[12700]\ttraining's l1: 0.0157042\tvalid_1's l1: 0.104322\n",
      "[12800]\ttraining's l1: 0.0155292\tvalid_1's l1: 0.10429\n",
      "[12900]\ttraining's l1: 0.0153676\tvalid_1's l1: 0.104259\n",
      "[13000]\ttraining's l1: 0.0152099\tvalid_1's l1: 0.104233\n",
      "[13100]\ttraining's l1: 0.0150463\tvalid_1's l1: 0.104201\n",
      "[13200]\ttraining's l1: 0.0148806\tvalid_1's l1: 0.104173\n",
      "[13300]\ttraining's l1: 0.0147287\tvalid_1's l1: 0.10415\n",
      "[13400]\ttraining's l1: 0.0145697\tvalid_1's l1: 0.104125\n",
      "[13500]\ttraining's l1: 0.0144127\tvalid_1's l1: 0.104102\n",
      "[13600]\ttraining's l1: 0.014262\tvalid_1's l1: 0.104079\n",
      "[13700]\ttraining's l1: 0.0141106\tvalid_1's l1: 0.10405\n",
      "[13800]\ttraining's l1: 0.0139615\tvalid_1's l1: 0.104019\n",
      "[13900]\ttraining's l1: 0.0138204\tvalid_1's l1: 0.103993\n",
      "[14000]\ttraining's l1: 0.0136749\tvalid_1's l1: 0.10397\n",
      "[14100]\ttraining's l1: 0.0135295\tvalid_1's l1: 0.103945\n",
      "[14200]\ttraining's l1: 0.0133898\tvalid_1's l1: 0.103928\n",
      "[14300]\ttraining's l1: 0.013252\tvalid_1's l1: 0.1039\n",
      "[14400]\ttraining's l1: 0.0131146\tvalid_1's l1: 0.103872\n",
      "[14500]\ttraining's l1: 0.0129826\tvalid_1's l1: 0.10385\n",
      "[14600]\ttraining's l1: 0.0128516\tvalid_1's l1: 0.103823\n",
      "[14700]\ttraining's l1: 0.0127208\tvalid_1's l1: 0.103795\n",
      "[14800]\ttraining's l1: 0.0125941\tvalid_1's l1: 0.103775\n",
      "[14900]\ttraining's l1: 0.0124669\tvalid_1's l1: 0.103758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0123444\tvalid_1's l1: 0.103739\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0123444\tvalid_1's l1: 0.103739\n",
      "3JHN Fold 2, logMAE: -2.2658725275257514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.247143\tvalid_1's l1: 0.252515\n",
      "[200]\ttraining's l1: 0.203008\tvalid_1's l1: 0.213152\n",
      "[300]\ttraining's l1: 0.179158\tvalid_1's l1: 0.193106\n",
      "[400]\ttraining's l1: 0.163232\tvalid_1's l1: 0.180301\n",
      "[500]\ttraining's l1: 0.150657\tvalid_1's l1: 0.17038\n",
      "[600]\ttraining's l1: 0.140869\tvalid_1's l1: 0.163114\n",
      "[700]\ttraining's l1: 0.132597\tvalid_1's l1: 0.156996\n",
      "[800]\ttraining's l1: 0.125393\tvalid_1's l1: 0.151856\n",
      "[900]\ttraining's l1: 0.119193\tvalid_1's l1: 0.147607\n",
      "[1000]\ttraining's l1: 0.113813\tvalid_1's l1: 0.14402\n",
      "[1100]\ttraining's l1: 0.10911\tvalid_1's l1: 0.141019\n",
      "[1200]\ttraining's l1: 0.104801\tvalid_1's l1: 0.138334\n",
      "[1300]\ttraining's l1: 0.100936\tvalid_1's l1: 0.135918\n",
      "[1400]\ttraining's l1: 0.0974138\tvalid_1's l1: 0.133801\n",
      "[1500]\ttraining's l1: 0.0941216\tvalid_1's l1: 0.131868\n",
      "[1600]\ttraining's l1: 0.0910172\tvalid_1's l1: 0.130242\n",
      "[1700]\ttraining's l1: 0.0880965\tvalid_1's l1: 0.128612\n",
      "[1800]\ttraining's l1: 0.0854807\tvalid_1's l1: 0.127152\n",
      "[1900]\ttraining's l1: 0.0830228\tvalid_1's l1: 0.125939\n",
      "[2000]\ttraining's l1: 0.0807137\tvalid_1's l1: 0.124782\n",
      "[2100]\ttraining's l1: 0.0784883\tvalid_1's l1: 0.123611\n",
      "[2200]\ttraining's l1: 0.0764217\tvalid_1's l1: 0.122637\n",
      "[2300]\ttraining's l1: 0.0744446\tvalid_1's l1: 0.121692\n",
      "[2400]\ttraining's l1: 0.0725757\tvalid_1's l1: 0.120823\n",
      "[2500]\ttraining's l1: 0.0707854\tvalid_1's l1: 0.119976\n",
      "[2600]\ttraining's l1: 0.0690972\tvalid_1's l1: 0.119243\n",
      "[2700]\ttraining's l1: 0.0674794\tvalid_1's l1: 0.118524\n",
      "[2800]\ttraining's l1: 0.0659256\tvalid_1's l1: 0.117834\n",
      "[2900]\ttraining's l1: 0.0644827\tvalid_1's l1: 0.117275\n",
      "[3000]\ttraining's l1: 0.0630331\tvalid_1's l1: 0.116669\n",
      "[3100]\ttraining's l1: 0.0616197\tvalid_1's l1: 0.116072\n",
      "[3200]\ttraining's l1: 0.0603505\tvalid_1's l1: 0.115535\n",
      "[3300]\ttraining's l1: 0.0591289\tvalid_1's l1: 0.115021\n",
      "[3400]\ttraining's l1: 0.0579397\tvalid_1's l1: 0.114575\n",
      "[3500]\ttraining's l1: 0.0567456\tvalid_1's l1: 0.114068\n",
      "[3600]\ttraining's l1: 0.0556512\tvalid_1's l1: 0.113671\n",
      "[3700]\ttraining's l1: 0.0545757\tvalid_1's l1: 0.113287\n",
      "[3800]\ttraining's l1: 0.0535321\tvalid_1's l1: 0.112909\n",
      "[3900]\ttraining's l1: 0.0524847\tvalid_1's l1: 0.112521\n",
      "[4000]\ttraining's l1: 0.0514916\tvalid_1's l1: 0.112148\n",
      "[4100]\ttraining's l1: 0.0505332\tvalid_1's l1: 0.111822\n",
      "[4200]\ttraining's l1: 0.049635\tvalid_1's l1: 0.111497\n",
      "[4300]\ttraining's l1: 0.0487527\tvalid_1's l1: 0.111199\n",
      "[4400]\ttraining's l1: 0.047891\tvalid_1's l1: 0.110881\n",
      "[4500]\ttraining's l1: 0.0470418\tvalid_1's l1: 0.110578\n",
      "[4600]\ttraining's l1: 0.0462109\tvalid_1's l1: 0.110312\n",
      "[4700]\ttraining's l1: 0.0454169\tvalid_1's l1: 0.11004\n",
      "[4800]\ttraining's l1: 0.0446614\tvalid_1's l1: 0.109811\n",
      "[4900]\ttraining's l1: 0.0439175\tvalid_1's l1: 0.109563\n",
      "[5000]\ttraining's l1: 0.0431804\tvalid_1's l1: 0.109325\n",
      "[5100]\ttraining's l1: 0.0424591\tvalid_1's l1: 0.109099\n",
      "[5200]\ttraining's l1: 0.0417748\tvalid_1's l1: 0.108858\n",
      "[5300]\ttraining's l1: 0.0411036\tvalid_1's l1: 0.108629\n",
      "[5400]\ttraining's l1: 0.0404434\tvalid_1's l1: 0.108413\n",
      "[5500]\ttraining's l1: 0.0398041\tvalid_1's l1: 0.108225\n",
      "[5600]\ttraining's l1: 0.0391694\tvalid_1's l1: 0.108049\n",
      "[5700]\ttraining's l1: 0.0385689\tvalid_1's l1: 0.107884\n",
      "[5800]\ttraining's l1: 0.037956\tvalid_1's l1: 0.10771\n",
      "[5900]\ttraining's l1: 0.0373903\tvalid_1's l1: 0.107562\n",
      "[6000]\ttraining's l1: 0.0368213\tvalid_1's l1: 0.10739\n",
      "[6100]\ttraining's l1: 0.0362638\tvalid_1's l1: 0.107206\n",
      "[6200]\ttraining's l1: 0.0357146\tvalid_1's l1: 0.107039\n",
      "[6300]\ttraining's l1: 0.0351929\tvalid_1's l1: 0.106889\n",
      "[6400]\ttraining's l1: 0.0346458\tvalid_1's l1: 0.106749\n",
      "[6500]\ttraining's l1: 0.0341269\tvalid_1's l1: 0.106586\n",
      "[6600]\ttraining's l1: 0.0336381\tvalid_1's l1: 0.106442\n",
      "[6700]\ttraining's l1: 0.0331416\tvalid_1's l1: 0.106312\n",
      "[6800]\ttraining's l1: 0.0326617\tvalid_1's l1: 0.106174\n",
      "[6900]\ttraining's l1: 0.0322187\tvalid_1's l1: 0.106051\n",
      "[7000]\ttraining's l1: 0.0317542\tvalid_1's l1: 0.105927\n",
      "[7100]\ttraining's l1: 0.0312979\tvalid_1's l1: 0.105819\n",
      "[7200]\ttraining's l1: 0.0308575\tvalid_1's l1: 0.105717\n",
      "[7300]\ttraining's l1: 0.0304242\tvalid_1's l1: 0.105608\n",
      "[7400]\ttraining's l1: 0.0300038\tvalid_1's l1: 0.105491\n",
      "[7500]\ttraining's l1: 0.0295647\tvalid_1's l1: 0.105387\n",
      "[7600]\ttraining's l1: 0.0291733\tvalid_1's l1: 0.105281\n",
      "[7700]\ttraining's l1: 0.0287685\tvalid_1's l1: 0.10518\n",
      "[7800]\ttraining's l1: 0.0283775\tvalid_1's l1: 0.105092\n",
      "[7900]\ttraining's l1: 0.0279974\tvalid_1's l1: 0.105003\n",
      "[8000]\ttraining's l1: 0.0276114\tvalid_1's l1: 0.104919\n",
      "[8100]\ttraining's l1: 0.0272474\tvalid_1's l1: 0.104842\n",
      "[8200]\ttraining's l1: 0.026879\tvalid_1's l1: 0.104762\n",
      "[8300]\ttraining's l1: 0.0265087\tvalid_1's l1: 0.104681\n",
      "[8400]\ttraining's l1: 0.0261586\tvalid_1's l1: 0.104602\n",
      "[8500]\ttraining's l1: 0.0258346\tvalid_1's l1: 0.104526\n",
      "[8600]\ttraining's l1: 0.0254876\tvalid_1's l1: 0.104441\n",
      "[8700]\ttraining's l1: 0.0251805\tvalid_1's l1: 0.104373\n",
      "[8800]\ttraining's l1: 0.0248715\tvalid_1's l1: 0.104293\n",
      "[8900]\ttraining's l1: 0.0245518\tvalid_1's l1: 0.104231\n",
      "[9000]\ttraining's l1: 0.0242399\tvalid_1's l1: 0.104164\n",
      "[9100]\ttraining's l1: 0.0239419\tvalid_1's l1: 0.104091\n",
      "[9200]\ttraining's l1: 0.0236433\tvalid_1's l1: 0.104025\n",
      "[9300]\ttraining's l1: 0.023348\tvalid_1's l1: 0.103967\n",
      "[9400]\ttraining's l1: 0.0230548\tvalid_1's l1: 0.103904\n",
      "[9500]\ttraining's l1: 0.0227769\tvalid_1's l1: 0.103842\n",
      "[9600]\ttraining's l1: 0.0225018\tvalid_1's l1: 0.103787\n",
      "[9700]\ttraining's l1: 0.0222258\tvalid_1's l1: 0.103729\n",
      "[9800]\ttraining's l1: 0.0219571\tvalid_1's l1: 0.10368\n",
      "[9900]\ttraining's l1: 0.0216924\tvalid_1's l1: 0.103626\n",
      "[10000]\ttraining's l1: 0.0214269\tvalid_1's l1: 0.103571\n",
      "[10100]\ttraining's l1: 0.021174\tvalid_1's l1: 0.103513\n",
      "[10200]\ttraining's l1: 0.0209224\tvalid_1's l1: 0.103454\n",
      "[10300]\ttraining's l1: 0.0206782\tvalid_1's l1: 0.1034\n",
      "[10400]\ttraining's l1: 0.020423\tvalid_1's l1: 0.103363\n",
      "[10500]\ttraining's l1: 0.0201789\tvalid_1's l1: 0.103309\n",
      "[10600]\ttraining's l1: 0.0199434\tvalid_1's l1: 0.103261\n",
      "[10700]\ttraining's l1: 0.0197149\tvalid_1's l1: 0.103216\n",
      "[10800]\ttraining's l1: 0.0194906\tvalid_1's l1: 0.103173\n",
      "[10900]\ttraining's l1: 0.0192616\tvalid_1's l1: 0.103132\n",
      "[11000]\ttraining's l1: 0.0190405\tvalid_1's l1: 0.103084\n",
      "[11100]\ttraining's l1: 0.0188165\tvalid_1's l1: 0.103038\n",
      "[11200]\ttraining's l1: 0.0186001\tvalid_1's l1: 0.103\n",
      "[11300]\ttraining's l1: 0.0183899\tvalid_1's l1: 0.102961\n",
      "[11400]\ttraining's l1: 0.0181749\tvalid_1's l1: 0.102933\n",
      "[11500]\ttraining's l1: 0.0179721\tvalid_1's l1: 0.102896\n",
      "[11600]\ttraining's l1: 0.0177635\tvalid_1's l1: 0.102852\n",
      "[11700]\ttraining's l1: 0.0175611\tvalid_1's l1: 0.102809\n",
      "[11800]\ttraining's l1: 0.017366\tvalid_1's l1: 0.102782\n",
      "[11900]\ttraining's l1: 0.017171\tvalid_1's l1: 0.10275\n",
      "[12000]\ttraining's l1: 0.0169946\tvalid_1's l1: 0.102716\n",
      "[12100]\ttraining's l1: 0.0168055\tvalid_1's l1: 0.102691\n",
      "[12200]\ttraining's l1: 0.0166153\tvalid_1's l1: 0.102659\n",
      "[12300]\ttraining's l1: 0.0164336\tvalid_1's l1: 0.10263\n",
      "[12400]\ttraining's l1: 0.0162472\tvalid_1's l1: 0.102593\n",
      "[12500]\ttraining's l1: 0.0160729\tvalid_1's l1: 0.102561\n",
      "[12600]\ttraining's l1: 0.0158998\tvalid_1's l1: 0.102531\n",
      "[12700]\ttraining's l1: 0.0157346\tvalid_1's l1: 0.102499\n",
      "[12800]\ttraining's l1: 0.0155655\tvalid_1's l1: 0.102472\n",
      "[12900]\ttraining's l1: 0.0153941\tvalid_1's l1: 0.102441\n",
      "[13000]\ttraining's l1: 0.015231\tvalid_1's l1: 0.102412\n",
      "[13100]\ttraining's l1: 0.0150666\tvalid_1's l1: 0.10238\n",
      "[13200]\ttraining's l1: 0.014902\tvalid_1's l1: 0.102351\n",
      "[13300]\ttraining's l1: 0.0147448\tvalid_1's l1: 0.102322\n",
      "[13400]\ttraining's l1: 0.0145869\tvalid_1's l1: 0.102286\n",
      "[13500]\ttraining's l1: 0.0144284\tvalid_1's l1: 0.102266\n",
      "[13600]\ttraining's l1: 0.0142715\tvalid_1's l1: 0.102242\n",
      "[13700]\ttraining's l1: 0.0141311\tvalid_1's l1: 0.102219\n",
      "[13800]\ttraining's l1: 0.0139857\tvalid_1's l1: 0.102196\n",
      "[13900]\ttraining's l1: 0.0138461\tvalid_1's l1: 0.102173\n",
      "[14000]\ttraining's l1: 0.0137031\tvalid_1's l1: 0.102148\n",
      "[14100]\ttraining's l1: 0.013562\tvalid_1's l1: 0.102122\n",
      "[14200]\ttraining's l1: 0.0134216\tvalid_1's l1: 0.102103\n",
      "[14300]\ttraining's l1: 0.013291\tvalid_1's l1: 0.102078\n",
      "[14400]\ttraining's l1: 0.0131612\tvalid_1's l1: 0.102052\n",
      "[14500]\ttraining's l1: 0.0130255\tvalid_1's l1: 0.102021\n",
      "[14600]\ttraining's l1: 0.0128908\tvalid_1's l1: 0.102005\n",
      "[14700]\ttraining's l1: 0.0127594\tvalid_1's l1: 0.101991\n",
      "[14800]\ttraining's l1: 0.0126337\tvalid_1's l1: 0.101969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14900]\ttraining's l1: 0.0125055\tvalid_1's l1: 0.101944\n",
      "[15000]\ttraining's l1: 0.0123849\tvalid_1's l1: 0.101921\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0123849\tvalid_1's l1: 0.101921\n",
      "3JHN Fold 3, logMAE: -2.28355338493762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['atom_10', 'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8', 'atom_9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[100]\ttraining's l1: 0.246235\tvalid_1's l1: 0.257501\n",
      "[200]\ttraining's l1: 0.201345\tvalid_1's l1: 0.215582\n",
      "[300]\ttraining's l1: 0.177415\tvalid_1's l1: 0.194626\n",
      "[400]\ttraining's l1: 0.160859\tvalid_1's l1: 0.180704\n",
      "[500]\ttraining's l1: 0.148345\tvalid_1's l1: 0.170813\n",
      "[600]\ttraining's l1: 0.138207\tvalid_1's l1: 0.162923\n",
      "[700]\ttraining's l1: 0.130189\tvalid_1's l1: 0.157048\n",
      "[800]\ttraining's l1: 0.12356\tvalid_1's l1: 0.15228\n",
      "[900]\ttraining's l1: 0.117629\tvalid_1's l1: 0.148091\n",
      "[1000]\ttraining's l1: 0.112371\tvalid_1's l1: 0.144632\n",
      "[1100]\ttraining's l1: 0.107777\tvalid_1's l1: 0.141745\n",
      "[1200]\ttraining's l1: 0.103663\tvalid_1's l1: 0.139335\n",
      "[1300]\ttraining's l1: 0.100042\tvalid_1's l1: 0.137185\n",
      "[1400]\ttraining's l1: 0.0964206\tvalid_1's l1: 0.135067\n",
      "[1500]\ttraining's l1: 0.0931597\tvalid_1's l1: 0.133077\n",
      "[1600]\ttraining's l1: 0.0901067\tvalid_1's l1: 0.131298\n",
      "[1700]\ttraining's l1: 0.0873327\tvalid_1's l1: 0.129776\n",
      "[1800]\ttraining's l1: 0.0847541\tvalid_1's l1: 0.128359\n",
      "[1900]\ttraining's l1: 0.0823306\tvalid_1's l1: 0.127131\n",
      "[2000]\ttraining's l1: 0.0800262\tvalid_1's l1: 0.125956\n",
      "[2100]\ttraining's l1: 0.0777794\tvalid_1's l1: 0.124809\n",
      "[2200]\ttraining's l1: 0.0757378\tvalid_1's l1: 0.123807\n",
      "[2300]\ttraining's l1: 0.0738285\tvalid_1's l1: 0.122857\n",
      "[2400]\ttraining's l1: 0.0719777\tvalid_1's l1: 0.12191\n",
      "[2500]\ttraining's l1: 0.0702287\tvalid_1's l1: 0.121072\n",
      "[2600]\ttraining's l1: 0.0685564\tvalid_1's l1: 0.120361\n",
      "[2700]\ttraining's l1: 0.0669228\tvalid_1's l1: 0.11965\n",
      "[2800]\ttraining's l1: 0.065369\tvalid_1's l1: 0.118944\n",
      "[2900]\ttraining's l1: 0.0639056\tvalid_1's l1: 0.118327\n",
      "[3000]\ttraining's l1: 0.0625315\tvalid_1's l1: 0.117743\n",
      "[3100]\ttraining's l1: 0.0612082\tvalid_1's l1: 0.117222\n",
      "[3200]\ttraining's l1: 0.0598954\tvalid_1's l1: 0.11664\n",
      "[3300]\ttraining's l1: 0.0586476\tvalid_1's l1: 0.116106\n",
      "[3400]\ttraining's l1: 0.057482\tvalid_1's l1: 0.115669\n",
      "[3500]\ttraining's l1: 0.056298\tvalid_1's l1: 0.115184\n",
      "[3600]\ttraining's l1: 0.0552048\tvalid_1's l1: 0.114779\n",
      "[3700]\ttraining's l1: 0.054127\tvalid_1's l1: 0.114401\n",
      "[3800]\ttraining's l1: 0.053097\tvalid_1's l1: 0.114012\n",
      "[3900]\ttraining's l1: 0.0521159\tvalid_1's l1: 0.113674\n",
      "[4000]\ttraining's l1: 0.051185\tvalid_1's l1: 0.113334\n",
      "[4100]\ttraining's l1: 0.050219\tvalid_1's l1: 0.113015\n",
      "[4200]\ttraining's l1: 0.0492598\tvalid_1's l1: 0.112645\n",
      "[4300]\ttraining's l1: 0.048385\tvalid_1's l1: 0.112356\n",
      "[4400]\ttraining's l1: 0.047527\tvalid_1's l1: 0.112074\n",
      "[4500]\ttraining's l1: 0.046719\tvalid_1's l1: 0.111778\n",
      "[4600]\ttraining's l1: 0.0458976\tvalid_1's l1: 0.111515\n",
      "[4700]\ttraining's l1: 0.0451155\tvalid_1's l1: 0.11127\n",
      "[4800]\ttraining's l1: 0.0443242\tvalid_1's l1: 0.111008\n",
      "[4900]\ttraining's l1: 0.0435385\tvalid_1's l1: 0.110771\n",
      "[5000]\ttraining's l1: 0.0428324\tvalid_1's l1: 0.110527\n",
      "[5100]\ttraining's l1: 0.0421437\tvalid_1's l1: 0.110306\n",
      "[5200]\ttraining's l1: 0.0414309\tvalid_1's l1: 0.110094\n",
      "[5300]\ttraining's l1: 0.0407436\tvalid_1's l1: 0.10988\n",
      "[5400]\ttraining's l1: 0.040088\tvalid_1's l1: 0.109691\n",
      "[5500]\ttraining's l1: 0.0394661\tvalid_1's l1: 0.109511\n",
      "[5600]\ttraining's l1: 0.0388416\tvalid_1's l1: 0.10934\n",
      "[5700]\ttraining's l1: 0.038217\tvalid_1's l1: 0.109161\n",
      "[5800]\ttraining's l1: 0.0376233\tvalid_1's l1: 0.108988\n",
      "[5900]\ttraining's l1: 0.0370421\tvalid_1's l1: 0.108798\n",
      "[6000]\ttraining's l1: 0.0364767\tvalid_1's l1: 0.108634\n",
      "[6100]\ttraining's l1: 0.0359403\tvalid_1's l1: 0.108489\n",
      "[6200]\ttraining's l1: 0.0354126\tvalid_1's l1: 0.108328\n",
      "[6300]\ttraining's l1: 0.0349121\tvalid_1's l1: 0.108183\n",
      "[6400]\ttraining's l1: 0.0344181\tvalid_1's l1: 0.108051\n",
      "[6500]\ttraining's l1: 0.0338962\tvalid_1's l1: 0.107904\n",
      "[6600]\ttraining's l1: 0.0334056\tvalid_1's l1: 0.107757\n",
      "[6700]\ttraining's l1: 0.0329179\tvalid_1's l1: 0.107634\n",
      "[6800]\ttraining's l1: 0.0324571\tvalid_1's l1: 0.107526\n",
      "[6900]\ttraining's l1: 0.031995\tvalid_1's l1: 0.107423\n",
      "[7000]\ttraining's l1: 0.0315482\tvalid_1's l1: 0.107316\n",
      "[7100]\ttraining's l1: 0.031102\tvalid_1's l1: 0.107184\n",
      "[7200]\ttraining's l1: 0.0306569\tvalid_1's l1: 0.107064\n",
      "[7300]\ttraining's l1: 0.0302342\tvalid_1's l1: 0.106964\n",
      "[7400]\ttraining's l1: 0.0298074\tvalid_1's l1: 0.106845\n",
      "[7500]\ttraining's l1: 0.0293923\tvalid_1's l1: 0.106749\n",
      "[7600]\ttraining's l1: 0.0289944\tvalid_1's l1: 0.106644\n",
      "[7700]\ttraining's l1: 0.0285951\tvalid_1's l1: 0.106556\n",
      "[7800]\ttraining's l1: 0.028215\tvalid_1's l1: 0.106434\n",
      "[7900]\ttraining's l1: 0.0278536\tvalid_1's l1: 0.106363\n",
      "[8000]\ttraining's l1: 0.0274995\tvalid_1's l1: 0.106266\n",
      "[8100]\ttraining's l1: 0.02712\tvalid_1's l1: 0.106177\n",
      "[8200]\ttraining's l1: 0.0267685\tvalid_1's l1: 0.106095\n",
      "[8300]\ttraining's l1: 0.0264268\tvalid_1's l1: 0.106023\n",
      "[8400]\ttraining's l1: 0.0260887\tvalid_1's l1: 0.105937\n",
      "[8500]\ttraining's l1: 0.0257616\tvalid_1's l1: 0.10586\n",
      "[8600]\ttraining's l1: 0.0254224\tvalid_1's l1: 0.105782\n",
      "[8700]\ttraining's l1: 0.0251018\tvalid_1's l1: 0.105707\n",
      "[8800]\ttraining's l1: 0.0247829\tvalid_1's l1: 0.105636\n",
      "[8900]\ttraining's l1: 0.0244779\tvalid_1's l1: 0.10556\n",
      "[9000]\ttraining's l1: 0.0241675\tvalid_1's l1: 0.105491\n",
      "[9100]\ttraining's l1: 0.0238694\tvalid_1's l1: 0.105419\n",
      "[9200]\ttraining's l1: 0.0235703\tvalid_1's l1: 0.105353\n",
      "[9300]\ttraining's l1: 0.023294\tvalid_1's l1: 0.105284\n",
      "[9400]\ttraining's l1: 0.0230083\tvalid_1's l1: 0.105218\n",
      "[9500]\ttraining's l1: 0.02274\tvalid_1's l1: 0.105146\n",
      "[9600]\ttraining's l1: 0.0224648\tvalid_1's l1: 0.105094\n",
      "[9700]\ttraining's l1: 0.0222005\tvalid_1's l1: 0.105043\n",
      "[9800]\ttraining's l1: 0.02193\tvalid_1's l1: 0.104992\n",
      "[9900]\ttraining's l1: 0.0216709\tvalid_1's l1: 0.104924\n",
      "[10000]\ttraining's l1: 0.0214048\tvalid_1's l1: 0.104869\n",
      "[10100]\ttraining's l1: 0.0211378\tvalid_1's l1: 0.104802\n",
      "[10200]\ttraining's l1: 0.0208844\tvalid_1's l1: 0.104749\n",
      "[10300]\ttraining's l1: 0.0206426\tvalid_1's l1: 0.104697\n",
      "[10400]\ttraining's l1: 0.0203955\tvalid_1's l1: 0.104651\n",
      "[10500]\ttraining's l1: 0.0201502\tvalid_1's l1: 0.104599\n",
      "[10600]\ttraining's l1: 0.0199139\tvalid_1's l1: 0.104556\n",
      "[10700]\ttraining's l1: 0.0196796\tvalid_1's l1: 0.104518\n",
      "[10800]\ttraining's l1: 0.0194421\tvalid_1's l1: 0.104475\n",
      "[10900]\ttraining's l1: 0.0192282\tvalid_1's l1: 0.104436\n",
      "[11000]\ttraining's l1: 0.0190038\tvalid_1's l1: 0.104394\n",
      "[11100]\ttraining's l1: 0.0187852\tvalid_1's l1: 0.104355\n",
      "[11200]\ttraining's l1: 0.0185693\tvalid_1's l1: 0.104309\n",
      "[11300]\ttraining's l1: 0.018353\tvalid_1's l1: 0.104267\n",
      "[11400]\ttraining's l1: 0.0181517\tvalid_1's l1: 0.104231\n",
      "[11500]\ttraining's l1: 0.017944\tvalid_1's l1: 0.104196\n",
      "[11600]\ttraining's l1: 0.0177498\tvalid_1's l1: 0.104161\n",
      "[11700]\ttraining's l1: 0.0175576\tvalid_1's l1: 0.104122\n",
      "[11800]\ttraining's l1: 0.0173631\tvalid_1's l1: 0.10408\n",
      "[11900]\ttraining's l1: 0.0171737\tvalid_1's l1: 0.10405\n",
      "[12000]\ttraining's l1: 0.016979\tvalid_1's l1: 0.104014\n",
      "[12100]\ttraining's l1: 0.0167967\tvalid_1's l1: 0.103974\n",
      "[12200]\ttraining's l1: 0.0166098\tvalid_1's l1: 0.103941\n",
      "[12300]\ttraining's l1: 0.0164263\tvalid_1's l1: 0.103911\n",
      "[12400]\ttraining's l1: 0.0162412\tvalid_1's l1: 0.103873\n",
      "[12500]\ttraining's l1: 0.0160653\tvalid_1's l1: 0.103843\n",
      "[12600]\ttraining's l1: 0.0158995\tvalid_1's l1: 0.103815\n",
      "[12700]\ttraining's l1: 0.0157307\tvalid_1's l1: 0.103779\n",
      "[12800]\ttraining's l1: 0.0155634\tvalid_1's l1: 0.103755\n",
      "[12900]\ttraining's l1: 0.0153946\tvalid_1's l1: 0.103728\n",
      "[13000]\ttraining's l1: 0.0152238\tvalid_1's l1: 0.103699\n",
      "[13100]\ttraining's l1: 0.0150658\tvalid_1's l1: 0.103668\n",
      "[13200]\ttraining's l1: 0.0149099\tvalid_1's l1: 0.103643\n",
      "[13300]\ttraining's l1: 0.0147546\tvalid_1's l1: 0.103621\n",
      "[13400]\ttraining's l1: 0.0146017\tvalid_1's l1: 0.1036\n",
      "[13500]\ttraining's l1: 0.0144402\tvalid_1's l1: 0.103574\n",
      "[13600]\ttraining's l1: 0.0142856\tvalid_1's l1: 0.103553\n",
      "[13700]\ttraining's l1: 0.0141266\tvalid_1's l1: 0.103525\n",
      "[13800]\ttraining's l1: 0.0139723\tvalid_1's l1: 0.103496\n",
      "[13900]\ttraining's l1: 0.0138286\tvalid_1's l1: 0.103467\n",
      "[14000]\ttraining's l1: 0.0136854\tvalid_1's l1: 0.103439\n",
      "[14100]\ttraining's l1: 0.0135531\tvalid_1's l1: 0.10341\n",
      "[14200]\ttraining's l1: 0.0134164\tvalid_1's l1: 0.103387\n",
      "[14300]\ttraining's l1: 0.013277\tvalid_1's l1: 0.103366\n",
      "[14400]\ttraining's l1: 0.0131441\tvalid_1's l1: 0.103343\n",
      "[14500]\ttraining's l1: 0.0130061\tvalid_1's l1: 0.103327\n",
      "[14600]\ttraining's l1: 0.0128747\tvalid_1's l1: 0.103303\n",
      "[14700]\ttraining's l1: 0.0127433\tvalid_1's l1: 0.103287\n",
      "[14800]\ttraining's l1: 0.0126151\tvalid_1's l1: 0.10327\n",
      "[14900]\ttraining's l1: 0.0124944\tvalid_1's l1: 0.103251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's l1: 0.0123662\tvalid_1's l1: 0.103232\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\ttraining's l1: 0.0123662\tvalid_1's l1: 0.103232\n",
      "3JHN Fold 4, logMAE: -2.270774089701595\n"
     ]
    }
   ],
   "source": [
    "N_FOLDS = 5\n",
    "submission = submission_csv.copy()\n",
    "\n",
    "cv_scores = {}\n",
    "for coupling_type in Config.MODEL_PARAMS.keys():\n",
    "    cv_score = train_and_predict_for_one_coupling_type(\n",
    "        coupling_type, submission, n_folds=N_FOLDS)\n",
    "    cv_scores[coupling_type] = cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1JHN</td>\n",
       "      <td>-1.106502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1JHC</td>\n",
       "      <td>-0.390804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2JHH</td>\n",
       "      <td>-1.966835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2JHN</td>\n",
       "      <td>-2.076056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.387011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3JHH</td>\n",
       "      <td>-2.006237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3JHC</td>\n",
       "      <td>-1.260956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3JHN</td>\n",
       "      <td>-2.272088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  cv_score\n",
       "0  1JHN -1.106502\n",
       "1  1JHC -0.390804\n",
       "2  2JHH -1.966835\n",
       "3  2JHN -2.076056\n",
       "4  2JHC -1.387011\n",
       "5  3JHH -2.006237\n",
       "6  3JHC -1.260956\n",
       "7  3JHN -2.272088"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'type': list(cv_scores.keys()), 'cv_score': list(cv_scores.values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And cv mean score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.558311090702058"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(cv_scores.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check for all cells to be filled with predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[submission['scalar_coupling_constant'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>12.797239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>164.063110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>2.448566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>164.063110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>12.797239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658152</th>\n",
       "      <td>91.457123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658153</th>\n",
       "      <td>2.108005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658154</th>\n",
       "      <td>-7.776263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658155</th>\n",
       "      <td>-9.894598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658156</th>\n",
       "      <td>91.556610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         scalar_coupling_constant\n",
       "id                               \n",
       "4658147  12.797239               \n",
       "4658148  164.063110              \n",
       "4658149  2.448566                \n",
       "4658150  164.063110              \n",
       "4658151  12.797239               \n",
       "4658152  91.457123               \n",
       "4658153  2.108005                \n",
       "4658154 -7.776263                \n",
       "4658155 -9.894598                \n",
       "4658156  91.556610               "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(f'{Config.SUBMISSIONS_PATH}/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 629.975,
   "position": {
    "height": "651.575px",
    "left": "703px",
    "right": "20px",
    "top": "7.9875px",
    "width": "774.725px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
