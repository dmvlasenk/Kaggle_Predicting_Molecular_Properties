{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "#warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
    "#warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupling_type = '2JHN'\n",
    "train_csv = load_train()\n",
    "n_atoms = 7\n",
    "XY_Data = build_XY(train_csv, coupling_type, n_atoms, False)\n",
    "#XY_Data = pd.read_csv(f'{Config.INPUT_XY}/{coupling_type}.csv', index_col=0)\n",
    "XY_Data, _ = train_test_split(XY_Data, test_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_Data, y_Data = build_x_y_data(XY_Data)    \n",
    "XY_train, XY_Val =  train_test_split(XY_Data, test_size=0.3, random_state=228)\n",
    "X_train, y_train = build_x_y_data(XY_train)    \n",
    "X_val, y_val = build_x_y_data(XY_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_2</th>\n",
       "      <th>atom_3</th>\n",
       "      <th>atom_4</th>\n",
       "      <th>atom_5</th>\n",
       "      <th>atom_6</th>\n",
       "      <th>r_x_1</th>\n",
       "      <th>r_x_2</th>\n",
       "      <th>r_x_3</th>\n",
       "      <th>r_x_4</th>\n",
       "      <th>r_x_5</th>\n",
       "      <th>...</th>\n",
       "      <th>d_4_2</th>\n",
       "      <th>d_4_3</th>\n",
       "      <th>d_5_0</th>\n",
       "      <th>d_5_1</th>\n",
       "      <th>d_5_2</th>\n",
       "      <th>d_5_3</th>\n",
       "      <th>d_6_0</th>\n",
       "      <th>d_6_1</th>\n",
       "      <th>d_6_2</th>\n",
       "      <th>d_6_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.042530</td>\n",
       "      <td>3.134420</td>\n",
       "      <td>4.157781</td>\n",
       "      <td>4.955673</td>\n",
       "      <td>4.757038</td>\n",
       "      <td>-1.067688</td>\n",
       "      <td>0.212587</td>\n",
       "      <td>-0.282790</td>\n",
       "      <td>-0.479255</td>\n",
       "      <td>-0.770634</td>\n",
       "      <td>...</td>\n",
       "      <td>1.658874</td>\n",
       "      <td>2.392976</td>\n",
       "      <td>2.669473</td>\n",
       "      <td>1.935284</td>\n",
       "      <td>2.082391</td>\n",
       "      <td>2.431792</td>\n",
       "      <td>2.963041</td>\n",
       "      <td>2.069803</td>\n",
       "      <td>2.405976</td>\n",
       "      <td>2.647346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.249587</td>\n",
       "      <td>2.569876</td>\n",
       "      <td>2.553271</td>\n",
       "      <td>2.140455</td>\n",
       "      <td>2.403251</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.640091</td>\n",
       "      <td>0.873691</td>\n",
       "      <td>0.887282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520463</td>\n",
       "      <td>0.597216</td>\n",
       "      <td>0.440065</td>\n",
       "      <td>0.543723</td>\n",
       "      <td>0.445237</td>\n",
       "      <td>0.783877</td>\n",
       "      <td>0.502180</td>\n",
       "      <td>0.614595</td>\n",
       "      <td>0.403336</td>\n",
       "      <td>0.774146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.177559</td>\n",
       "      <td>0.084758</td>\n",
       "      <td>-2.000683</td>\n",
       "      <td>-2.490387</td>\n",
       "      <td>-3.436701</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078170</td>\n",
       "      <td>1.078723</td>\n",
       "      <td>1.762759</td>\n",
       "      <td>1.005555</td>\n",
       "      <td>1.083384</td>\n",
       "      <td>1.006182</td>\n",
       "      <td>1.785467</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>1.088632</td>\n",
       "      <td>1.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.088135</td>\n",
       "      <td>0.209613</td>\n",
       "      <td>-0.959353</td>\n",
       "      <td>-1.272946</td>\n",
       "      <td>-1.464971</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098597</td>\n",
       "      <td>2.027695</td>\n",
       "      <td>2.267415</td>\n",
       "      <td>1.452749</td>\n",
       "      <td>1.550527</td>\n",
       "      <td>2.045917</td>\n",
       "      <td>2.541570</td>\n",
       "      <td>1.469537</td>\n",
       "      <td>2.223422</td>\n",
       "      <td>2.156590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.067222</td>\n",
       "      <td>0.220387</td>\n",
       "      <td>0.041553</td>\n",
       "      <td>-0.108669</td>\n",
       "      <td>-1.071961</td>\n",
       "      <td>...</td>\n",
       "      <td>1.527560</td>\n",
       "      <td>2.234305</td>\n",
       "      <td>2.656921</td>\n",
       "      <td>2.118483</td>\n",
       "      <td>2.209579</td>\n",
       "      <td>2.408760</td>\n",
       "      <td>3.015891</td>\n",
       "      <td>2.149516</td>\n",
       "      <td>2.458239</td>\n",
       "      <td>2.622725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.044226</td>\n",
       "      <td>0.229657</td>\n",
       "      <td>0.295155</td>\n",
       "      <td>0.300863</td>\n",
       "      <td>0.097556</td>\n",
       "      <td>...</td>\n",
       "      <td>2.145198</td>\n",
       "      <td>2.925505</td>\n",
       "      <td>3.002592</td>\n",
       "      <td>2.366688</td>\n",
       "      <td>2.440380</td>\n",
       "      <td>3.147599</td>\n",
       "      <td>3.325073</td>\n",
       "      <td>2.461068</td>\n",
       "      <td>2.617726</td>\n",
       "      <td>3.254888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-0.916332</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.590589</td>\n",
       "      <td>0.633578</td>\n",
       "      <td>2.316046</td>\n",
       "      <td>...</td>\n",
       "      <td>3.834378</td>\n",
       "      <td>3.877147</td>\n",
       "      <td>4.417424</td>\n",
       "      <td>3.746297</td>\n",
       "      <td>3.727495</td>\n",
       "      <td>4.339425</td>\n",
       "      <td>4.634705</td>\n",
       "      <td>4.104904</td>\n",
       "      <td>4.160572</td>\n",
       "      <td>4.676648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            atom_2       atom_3       atom_4       atom_5       atom_6  \\\n",
       "count  8347.000000  8347.000000  8347.000000  8347.000000  8347.000000   \n",
       "mean      6.042530     3.134420     4.157781     4.955673     4.757038   \n",
       "std       0.249587     2.569876     2.553271     2.140455     2.403251   \n",
       "min       6.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%       6.000000     1.000000     1.000000     6.000000     1.000000   \n",
       "50%       6.000000     1.000000     6.000000     6.000000     6.000000   \n",
       "75%       6.000000     6.000000     6.000000     6.000000     6.000000   \n",
       "max       8.000000     8.000000     8.000000     8.000000     9.000000   \n",
       "\n",
       "             r_x_1        r_x_2        r_x_3        r_x_4        r_x_5  ...  \\\n",
       "count  8347.000000  8347.000000  8347.000000  8347.000000  8347.000000  ...   \n",
       "mean     -1.067688     0.212587    -0.282790    -0.479255    -0.770634  ...   \n",
       "std       0.034179     0.036439     0.640091     0.873691     0.887282  ...   \n",
       "min      -1.177559     0.084758    -2.000683    -2.490387    -3.436701  ...   \n",
       "25%      -1.088135     0.209613    -0.959353    -1.272946    -1.464971  ...   \n",
       "50%      -1.067222     0.220387     0.041553    -0.108669    -1.071961  ...   \n",
       "75%      -1.044226     0.229657     0.295155     0.300863     0.097556  ...   \n",
       "max      -0.916332     0.347900     0.590589     0.633578     2.316046  ...   \n",
       "\n",
       "             d_4_2        d_4_3        d_5_0        d_5_1        d_5_2  \\\n",
       "count  8347.000000  8347.000000  8347.000000  8347.000000  8347.000000   \n",
       "mean      1.658874     2.392976     2.669473     1.935284     2.082391   \n",
       "std       0.520463     0.597216     0.440065     0.543723     0.445237   \n",
       "min       1.078170     1.078723     1.762759     1.005555     1.083384   \n",
       "25%       1.098597     2.027695     2.267415     1.452749     1.550527   \n",
       "50%       1.527560     2.234305     2.656921     2.118483     2.209579   \n",
       "75%       2.145198     2.925505     3.002592     2.366688     2.440380   \n",
       "max       3.834378     3.877147     4.417424     3.746297     3.727495   \n",
       "\n",
       "             d_5_3        d_6_0        d_6_1        d_6_2        d_6_3  \n",
       "count  8347.000000  8347.000000  8347.000000  8347.000000  8347.000000  \n",
       "mean      2.431792     2.963041     2.069803     2.405976     2.647346  \n",
       "std       0.783877     0.502180     0.614595     0.403336     0.774146  \n",
       "min       1.006182     1.785467     1.005191     1.088632     1.004500  \n",
       "25%       2.045917     2.541570     1.469537     2.223422     2.156590  \n",
       "50%       2.408760     3.015891     2.149516     2.458239     2.622725  \n",
       "75%       3.147599     3.325073     2.461068     2.617726     3.254888  \n",
       "max       4.339425     4.634705     4.104904     4.160572     4.676648  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "scaler = StandardScaler()\n",
    "train_input = scaler.fit_transform(X_train)   \n",
    "cv_input =  scaler.transform(X_val)\n",
    "train_target = y_train\n",
    "cv_target = y_val\n",
    "\n",
    "#train_input.describe()\n",
    "#input_data=StandardScaler().fit_transform(df_train_.loc[:,input_features])\n",
    "#target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_2</th>\n",
       "      <th>atom_3</th>\n",
       "      <th>atom_4</th>\n",
       "      <th>atom_5</th>\n",
       "      <th>atom_6</th>\n",
       "      <th>r_x_1</th>\n",
       "      <th>r_x_2</th>\n",
       "      <th>r_x_3</th>\n",
       "      <th>r_x_4</th>\n",
       "      <th>r_x_5</th>\n",
       "      <th>...</th>\n",
       "      <th>d_4_2</th>\n",
       "      <th>d_4_3</th>\n",
       "      <th>d_5_0</th>\n",
       "      <th>d_5_1</th>\n",
       "      <th>d_5_2</th>\n",
       "      <th>d_5_3</th>\n",
       "      <th>d_6_0</th>\n",
       "      <th>d_6_1</th>\n",
       "      <th>d_6_2</th>\n",
       "      <th>d_6_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "      <td>8347.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.042530</td>\n",
       "      <td>3.134420</td>\n",
       "      <td>4.157781</td>\n",
       "      <td>4.955673</td>\n",
       "      <td>4.757038</td>\n",
       "      <td>-1.067688</td>\n",
       "      <td>0.212587</td>\n",
       "      <td>-0.282790</td>\n",
       "      <td>-0.479255</td>\n",
       "      <td>-0.770634</td>\n",
       "      <td>...</td>\n",
       "      <td>1.658874</td>\n",
       "      <td>2.392976</td>\n",
       "      <td>2.669473</td>\n",
       "      <td>1.935284</td>\n",
       "      <td>2.082391</td>\n",
       "      <td>2.431792</td>\n",
       "      <td>2.963041</td>\n",
       "      <td>2.069803</td>\n",
       "      <td>2.405976</td>\n",
       "      <td>2.647346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.249587</td>\n",
       "      <td>2.569876</td>\n",
       "      <td>2.553271</td>\n",
       "      <td>2.140455</td>\n",
       "      <td>2.403251</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.640091</td>\n",
       "      <td>0.873691</td>\n",
       "      <td>0.887282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520463</td>\n",
       "      <td>0.597216</td>\n",
       "      <td>0.440065</td>\n",
       "      <td>0.543723</td>\n",
       "      <td>0.445237</td>\n",
       "      <td>0.783877</td>\n",
       "      <td>0.502180</td>\n",
       "      <td>0.614595</td>\n",
       "      <td>0.403336</td>\n",
       "      <td>0.774146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.177559</td>\n",
       "      <td>0.084758</td>\n",
       "      <td>-2.000683</td>\n",
       "      <td>-2.490387</td>\n",
       "      <td>-3.436701</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078170</td>\n",
       "      <td>1.078723</td>\n",
       "      <td>1.762759</td>\n",
       "      <td>1.005555</td>\n",
       "      <td>1.083384</td>\n",
       "      <td>1.006182</td>\n",
       "      <td>1.785467</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>1.088632</td>\n",
       "      <td>1.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.088135</td>\n",
       "      <td>0.209613</td>\n",
       "      <td>-0.959353</td>\n",
       "      <td>-1.272946</td>\n",
       "      <td>-1.464971</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098597</td>\n",
       "      <td>2.027695</td>\n",
       "      <td>2.267415</td>\n",
       "      <td>1.452749</td>\n",
       "      <td>1.550527</td>\n",
       "      <td>2.045917</td>\n",
       "      <td>2.541570</td>\n",
       "      <td>1.469537</td>\n",
       "      <td>2.223422</td>\n",
       "      <td>2.156590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.067222</td>\n",
       "      <td>0.220387</td>\n",
       "      <td>0.041553</td>\n",
       "      <td>-0.108669</td>\n",
       "      <td>-1.071961</td>\n",
       "      <td>...</td>\n",
       "      <td>1.527560</td>\n",
       "      <td>2.234305</td>\n",
       "      <td>2.656921</td>\n",
       "      <td>2.118483</td>\n",
       "      <td>2.209579</td>\n",
       "      <td>2.408760</td>\n",
       "      <td>3.015891</td>\n",
       "      <td>2.149516</td>\n",
       "      <td>2.458239</td>\n",
       "      <td>2.622725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.044226</td>\n",
       "      <td>0.229657</td>\n",
       "      <td>0.295155</td>\n",
       "      <td>0.300863</td>\n",
       "      <td>0.097556</td>\n",
       "      <td>...</td>\n",
       "      <td>2.145198</td>\n",
       "      <td>2.925505</td>\n",
       "      <td>3.002592</td>\n",
       "      <td>2.366688</td>\n",
       "      <td>2.440380</td>\n",
       "      <td>3.147599</td>\n",
       "      <td>3.325073</td>\n",
       "      <td>2.461068</td>\n",
       "      <td>2.617726</td>\n",
       "      <td>3.254888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-0.916332</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.590589</td>\n",
       "      <td>0.633578</td>\n",
       "      <td>2.316046</td>\n",
       "      <td>...</td>\n",
       "      <td>3.834378</td>\n",
       "      <td>3.877147</td>\n",
       "      <td>4.417424</td>\n",
       "      <td>3.746297</td>\n",
       "      <td>3.727495</td>\n",
       "      <td>4.339425</td>\n",
       "      <td>4.634705</td>\n",
       "      <td>4.104904</td>\n",
       "      <td>4.160572</td>\n",
       "      <td>4.676648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            atom_2       atom_3       atom_4       atom_5       atom_6  \\\n",
       "count  8347.000000  8347.000000  8347.000000  8347.000000  8347.000000   \n",
       "mean      6.042530     3.134420     4.157781     4.955673     4.757038   \n",
       "std       0.249587     2.569876     2.553271     2.140455     2.403251   \n",
       "min       6.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%       6.000000     1.000000     1.000000     6.000000     1.000000   \n",
       "50%       6.000000     1.000000     6.000000     6.000000     6.000000   \n",
       "75%       6.000000     6.000000     6.000000     6.000000     6.000000   \n",
       "max       8.000000     8.000000     8.000000     8.000000     9.000000   \n",
       "\n",
       "             r_x_1        r_x_2        r_x_3        r_x_4        r_x_5  ...  \\\n",
       "count  8347.000000  8347.000000  8347.000000  8347.000000  8347.000000  ...   \n",
       "mean     -1.067688     0.212587    -0.282790    -0.479255    -0.770634  ...   \n",
       "std       0.034179     0.036439     0.640091     0.873691     0.887282  ...   \n",
       "min      -1.177559     0.084758    -2.000683    -2.490387    -3.436701  ...   \n",
       "25%      -1.088135     0.209613    -0.959353    -1.272946    -1.464971  ...   \n",
       "50%      -1.067222     0.220387     0.041553    -0.108669    -1.071961  ...   \n",
       "75%      -1.044226     0.229657     0.295155     0.300863     0.097556  ...   \n",
       "max      -0.916332     0.347900     0.590589     0.633578     2.316046  ...   \n",
       "\n",
       "             d_4_2        d_4_3        d_5_0        d_5_1        d_5_2  \\\n",
       "count  8347.000000  8347.000000  8347.000000  8347.000000  8347.000000   \n",
       "mean      1.658874     2.392976     2.669473     1.935284     2.082391   \n",
       "std       0.520463     0.597216     0.440065     0.543723     0.445237   \n",
       "min       1.078170     1.078723     1.762759     1.005555     1.083384   \n",
       "25%       1.098597     2.027695     2.267415     1.452749     1.550527   \n",
       "50%       1.527560     2.234305     2.656921     2.118483     2.209579   \n",
       "75%       2.145198     2.925505     3.002592     2.366688     2.440380   \n",
       "max       3.834378     3.877147     4.417424     3.746297     3.727495   \n",
       "\n",
       "             d_5_3        d_6_0        d_6_1        d_6_2        d_6_3  \n",
       "count  8347.000000  8347.000000  8347.000000  8347.000000  8347.000000  \n",
       "mean      2.431792     2.963041     2.069803     2.405976     2.647346  \n",
       "std       0.783877     0.502180     0.614595     0.403336     0.774146  \n",
       "min       1.006182     1.785467     1.005191     1.088632     1.004500  \n",
       "25%       2.045917     2.541570     1.469537     2.223422     2.156590  \n",
       "50%       2.408760     3.015891     2.149516     2.458239     2.622725  \n",
       "75%       3.147599     3.325073     2.461068     2.617726     3.254888  \n",
       "max       4.339425     4.634705     4.104904     4.160572     4.676648  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(2024, activation=\"relu\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.4)(x)\n",
    "    x = Dense(2024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation=\"linear\")(x)  \n",
    "    model = Model(inputs=inp, outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GPU preferences\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "sess = tf.Session(config=config) \n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8347 samples, validate on 3578 samples\n",
      "Epoch 1/500\n",
      "8347/8347 [==============================] - 2s 280us/step - loss: 7.2105 - val_loss: 4.5420\n",
      "Epoch 2/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 3.5293 - val_loss: 3.9757\n",
      "Epoch 3/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 2.9660 - val_loss: 2.5399\n",
      "Epoch 4/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 2.5006 - val_loss: 2.3127\n",
      "Epoch 5/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 2.2750 - val_loss: 2.2773\n",
      "Epoch 6/500\n",
      "8347/8347 [==============================] - 1s 121us/step - loss: 2.0176 - val_loss: 2.0429\n",
      "Epoch 7/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 1.7526 - val_loss: 1.6210\n",
      "Epoch 8/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 1.4688 - val_loss: 1.4411\n",
      "Epoch 9/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 1.1913 - val_loss: 1.3949\n",
      "Epoch 10/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.9743 - val_loss: 1.0452\n",
      "Epoch 11/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.7911 - val_loss: 0.9793\n",
      "Epoch 12/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.7709 - val_loss: 0.9860\n",
      "Epoch 13/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.7705 - val_loss: 0.9008\n",
      "Epoch 14/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.7350 - val_loss: 0.8653\n",
      "Epoch 15/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.6706 - val_loss: 1.0802\n",
      "Epoch 16/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.6501 - val_loss: 1.3004\n",
      "Epoch 17/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.7688 - val_loss: 0.8536\n",
      "Epoch 18/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.6730 - val_loss: 0.8712\n",
      "Epoch 19/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.6474 - val_loss: 0.9392\n",
      "Epoch 20/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.6283 - val_loss: 1.0125\n",
      "Epoch 21/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.6596 - val_loss: 1.1961\n",
      "Epoch 22/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.7368 - val_loss: 0.9141\n",
      "Epoch 23/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.5771 - val_loss: 0.8821\n",
      "Epoch 24/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.5902 - val_loss: 0.9444\n",
      "Epoch 25/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.5427 - val_loss: 0.9311\n",
      "Epoch 26/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.5618 - val_loss: 0.6926\n",
      "Epoch 27/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.5074 - val_loss: 0.7757\n",
      "Epoch 28/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.5280 - val_loss: 0.7719\n",
      "Epoch 29/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.5128 - val_loss: 0.7301\n",
      "Epoch 30/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.5416 - val_loss: 0.7380\n",
      "Epoch 31/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.5547 - val_loss: 0.9659\n",
      "Epoch 32/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.5334 - val_loss: 0.8736\n",
      "Epoch 33/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.5421 - val_loss: 0.7051\n",
      "Epoch 34/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4934 - val_loss: 0.8327\n",
      "Epoch 35/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4957 - val_loss: 0.8086\n",
      "Epoch 36/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.5181 - val_loss: 0.7314\n",
      "Epoch 37/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.5178 - val_loss: 0.7179\n",
      "Epoch 38/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.5042 - val_loss: 0.6928\n",
      "Epoch 39/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.4807 - val_loss: 0.7990\n",
      "Epoch 40/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.4732 - val_loss: 0.8465\n",
      "Epoch 41/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.5048 - val_loss: 0.7127\n",
      "Epoch 42/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.4570 - val_loss: 0.6499\n",
      "Epoch 43/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.5522 - val_loss: 0.6871\n",
      "Epoch 44/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.5687 - val_loss: 0.7023\n",
      "Epoch 45/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.5068 - val_loss: 0.7409\n",
      "Epoch 46/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.4945 - val_loss: 0.8105\n",
      "Epoch 47/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4959 - val_loss: 0.6596\n",
      "Epoch 48/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.4550 - val_loss: 0.6572\n",
      "Epoch 49/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.4270 - val_loss: 0.6842\n",
      "Epoch 50/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4574 - val_loss: 0.7216\n",
      "Epoch 51/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4379 - val_loss: 0.6381\n",
      "Epoch 52/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.4664 - val_loss: 0.5430\n",
      "Epoch 53/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4637 - val_loss: 0.7306\n",
      "Epoch 54/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4785 - val_loss: 0.7466\n",
      "Epoch 55/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4689 - val_loss: 0.8437\n",
      "Epoch 56/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4755 - val_loss: 0.6699\n",
      "Epoch 57/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.4273 - val_loss: 0.5924\n",
      "Epoch 58/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4466 - val_loss: 0.5895\n",
      "Epoch 59/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.4269 - val_loss: 0.6303\n",
      "Epoch 60/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.4446 - val_loss: 0.5975\n",
      "Epoch 61/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.4619 - val_loss: 0.5619\n",
      "Epoch 62/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.4280 - val_loss: 0.7816\n",
      "Epoch 63/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4700 - val_loss: 0.6429\n",
      "Epoch 64/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.4606 - val_loss: 0.5197\n",
      "Epoch 65/500\n",
      "8347/8347 [==============================] - 1s 139us/step - loss: 0.4524 - val_loss: 0.5343\n",
      "Epoch 66/500\n",
      "8347/8347 [==============================] - 1s 142us/step - loss: 0.4452 - val_loss: 0.7420\n",
      "Epoch 67/500\n",
      "8347/8347 [==============================] - 1s 134us/step - loss: 0.4677 - val_loss: 0.5570\n",
      "Epoch 68/500\n",
      "8347/8347 [==============================] - 1s 135us/step - loss: 0.4359 - val_loss: 0.5981\n",
      "Epoch 69/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.4207 - val_loss: 0.6499\n",
      "Epoch 70/500\n",
      "8347/8347 [==============================] - 1s 121us/step - loss: 0.4085 - val_loss: 0.6413\n",
      "Epoch 71/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.4050 - val_loss: 0.6680\n",
      "Epoch 72/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.4436 - val_loss: 0.6516\n",
      "Epoch 73/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.4280 - val_loss: 0.5989\n",
      "Epoch 74/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.4024 - val_loss: 0.5319\n",
      "Epoch 75/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3914 - val_loss: 0.4795\n",
      "Epoch 76/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.4021 - val_loss: 0.5223\n",
      "Epoch 77/500\n",
      "8347/8347 [==============================] - 1s 120us/step - loss: 0.3776 - val_loss: 0.4931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.4002 - val_loss: 0.6263\n",
      "Epoch 79/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.4169 - val_loss: 0.5859\n",
      "Epoch 80/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.4211 - val_loss: 0.5381\n",
      "Epoch 81/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4200 - val_loss: 0.6744\n",
      "Epoch 82/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4188 - val_loss: 0.5950\n",
      "Epoch 83/500\n",
      "8347/8347 [==============================] - 1s 122us/step - loss: 0.4309 - val_loss: 0.5999\n",
      "Epoch 84/500\n",
      "8347/8347 [==============================] - 1s 122us/step - loss: 0.4110 - val_loss: 0.5383\n",
      "Epoch 85/500\n",
      "8347/8347 [==============================] - 1s 122us/step - loss: 0.3979 - val_loss: 0.6313\n",
      "Epoch 86/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.4132 - val_loss: 0.5417\n",
      "Epoch 87/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.3803 - val_loss: 0.6106\n",
      "Epoch 88/500\n",
      "8347/8347 [==============================] - 1s 120us/step - loss: 0.4119 - val_loss: 0.4729\n",
      "Epoch 89/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.3816 - val_loss: 0.5248\n",
      "Epoch 90/500\n",
      "8347/8347 [==============================] - 1s 122us/step - loss: 0.4286 - val_loss: 0.5536\n",
      "Epoch 91/500\n",
      "8347/8347 [==============================] - 1s 122us/step - loss: 0.3829 - val_loss: 0.5632\n",
      "Epoch 92/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.3857 - val_loss: 0.5411\n",
      "Epoch 93/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3788 - val_loss: 0.5102\n",
      "Epoch 94/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.3731 - val_loss: 0.6600\n",
      "Epoch 95/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.4140 - val_loss: 0.5637\n",
      "Epoch 96/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.4215 - val_loss: 0.5296\n",
      "Epoch 97/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.4204 - val_loss: 0.6304\n",
      "Epoch 98/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4133 - val_loss: 0.4935\n",
      "Epoch 99/500\n",
      "8347/8347 [==============================] - 1s 119us/step - loss: 0.4109 - val_loss: 0.5189\n",
      "Epoch 100/500\n",
      "8347/8347 [==============================] - 1s 122us/step - loss: 0.3728 - val_loss: 0.4661\n",
      "Epoch 101/500\n",
      "8347/8347 [==============================] - 1s 121us/step - loss: 0.3761 - val_loss: 0.5362\n",
      "Epoch 102/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3998 - val_loss: 0.5460\n",
      "Epoch 103/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.3628 - val_loss: 0.5312\n",
      "Epoch 104/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.4134 - val_loss: 0.4515\n",
      "Epoch 105/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.4002 - val_loss: 0.5735\n",
      "Epoch 106/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.4384 - val_loss: 0.5539\n",
      "Epoch 107/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3785 - val_loss: 0.6044\n",
      "Epoch 108/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.4158 - val_loss: 0.5603\n",
      "Epoch 109/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3896 - val_loss: 0.6605\n",
      "Epoch 110/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4031 - val_loss: 0.6429\n",
      "Epoch 111/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4916 - val_loss: 0.5615\n",
      "Epoch 112/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.4273 - val_loss: 0.6129\n",
      "Epoch 113/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.4280 - val_loss: 0.5472\n",
      "Epoch 114/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4025 - val_loss: 0.4752\n",
      "Epoch 115/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.4002 - val_loss: 0.4884\n",
      "Epoch 116/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4086 - val_loss: 0.4950\n",
      "Epoch 117/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3893 - val_loss: 0.5853\n",
      "Epoch 118/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3962 - val_loss: 0.5075\n",
      "Epoch 119/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3732 - val_loss: 0.4567\n",
      "Epoch 120/500\n",
      "8347/8347 [==============================] - 1s 132us/step - loss: 0.3767 - val_loss: 0.5785\n",
      "Epoch 121/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3904 - val_loss: 0.4771\n",
      "Epoch 122/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3951 - val_loss: 0.4995\n",
      "Epoch 123/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.3753 - val_loss: 0.4394\n",
      "Epoch 124/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3524 - val_loss: 0.5460\n",
      "Epoch 125/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.4248 - val_loss: 0.4908\n",
      "Epoch 126/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3907 - val_loss: 0.5476\n",
      "Epoch 127/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.4064 - val_loss: 0.4686\n",
      "Epoch 128/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3706 - val_loss: 0.5071\n",
      "Epoch 129/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.3590 - val_loss: 0.4493\n",
      "Epoch 130/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.3660 - val_loss: 0.4790\n",
      "Epoch 131/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3879 - val_loss: 0.5429\n",
      "Epoch 132/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.3491 - val_loss: 0.5261\n",
      "Epoch 133/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3758 - val_loss: 0.5471\n",
      "Epoch 134/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.3927 - val_loss: 0.5766\n",
      "Epoch 135/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3693 - val_loss: 0.5112\n",
      "Epoch 136/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3782 - val_loss: 0.4980\n",
      "Epoch 137/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4238 - val_loss: 0.5296\n",
      "Epoch 138/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3785 - val_loss: 0.4610\n",
      "Epoch 139/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3538 - val_loss: 0.4143\n",
      "Epoch 140/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3452 - val_loss: 0.4922\n",
      "Epoch 141/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3769 - val_loss: 0.4241\n",
      "Epoch 142/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3729 - val_loss: 0.4608\n",
      "Epoch 143/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3441 - val_loss: 0.4753\n",
      "Epoch 144/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3828 - val_loss: 0.4900\n",
      "Epoch 145/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3691 - val_loss: 0.5168\n",
      "Epoch 146/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3829 - val_loss: 0.4160\n",
      "Epoch 147/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3539 - val_loss: 0.6194\n",
      "Epoch 148/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.4099 - val_loss: 0.5070\n",
      "Epoch 149/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3894 - val_loss: 0.5071\n",
      "Epoch 150/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4087 - val_loss: 0.5284\n",
      "Epoch 151/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.4242 - val_loss: 0.4455\n",
      "Epoch 152/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3514 - val_loss: 0.5440\n",
      "Epoch 153/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3919 - val_loss: 0.4506\n",
      "Epoch 154/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3342 - val_loss: 0.4173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3354 - val_loss: 0.4371\n",
      "Epoch 156/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3192 - val_loss: 0.4109\n",
      "Epoch 157/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.3389 - val_loss: 0.3882\n",
      "Epoch 158/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3193 - val_loss: 0.5254\n",
      "Epoch 159/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3971 - val_loss: 0.4458\n",
      "Epoch 160/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.4007 - val_loss: 0.4856\n",
      "Epoch 161/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.3860 - val_loss: 0.5455\n",
      "Epoch 162/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3822 - val_loss: 0.4895\n",
      "Epoch 163/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3501 - val_loss: 0.4796\n",
      "Epoch 164/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.3389 - val_loss: 0.4088\n",
      "Epoch 165/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3476 - val_loss: 0.4450\n",
      "Epoch 166/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3199 - val_loss: 0.4705\n",
      "Epoch 167/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3518 - val_loss: 0.4535\n",
      "Epoch 168/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.3293 - val_loss: 0.4465\n",
      "Epoch 169/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3341 - val_loss: 0.4616\n",
      "Epoch 170/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.3495 - val_loss: 0.3910\n",
      "Epoch 171/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.3338 - val_loss: 0.4919\n",
      "Epoch 172/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3404 - val_loss: 0.4564\n",
      "Epoch 173/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3651 - val_loss: 0.4933\n",
      "Epoch 174/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3591 - val_loss: 0.5338\n",
      "Epoch 175/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3608 - val_loss: 0.4630\n",
      "Epoch 176/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.3390 - val_loss: 0.4422\n",
      "Epoch 177/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.3093 - val_loss: 0.4610\n",
      "\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 178/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.3118 - val_loss: 0.3680\n",
      "Epoch 179/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2649 - val_loss: 0.3653\n",
      "Epoch 180/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2546 - val_loss: 0.3604\n",
      "Epoch 181/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.2517 - val_loss: 0.3541\n",
      "Epoch 182/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2385 - val_loss: 0.3410\n",
      "Epoch 183/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2447 - val_loss: 0.3312\n",
      "Epoch 184/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2270 - val_loss: 0.3335\n",
      "Epoch 185/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2342 - val_loss: 0.3363\n",
      "Epoch 186/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.2655 - val_loss: 0.3484\n",
      "Epoch 187/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2292 - val_loss: 0.3253\n",
      "Epoch 188/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2331 - val_loss: 0.3298\n",
      "Epoch 189/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2261 - val_loss: 0.3171\n",
      "Epoch 190/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2254 - val_loss: 0.3329\n",
      "Epoch 191/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2338 - val_loss: 0.3180\n",
      "Epoch 192/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2524 - val_loss: 0.3247\n",
      "Epoch 193/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2506 - val_loss: 0.3200\n",
      "Epoch 194/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2362 - val_loss: 0.3178\n",
      "Epoch 195/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2374 - val_loss: 0.3099\n",
      "Epoch 196/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2172 - val_loss: 0.3168\n",
      "Epoch 197/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2245 - val_loss: 0.3087\n",
      "Epoch 198/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2417 - val_loss: 0.3214\n",
      "Epoch 199/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2369 - val_loss: 0.3110\n",
      "Epoch 200/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2280 - val_loss: 0.3130\n",
      "Epoch 201/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2205 - val_loss: 0.3182\n",
      "Epoch 202/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2259 - val_loss: 0.3138\n",
      "Epoch 203/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2245 - val_loss: 0.3194\n",
      "Epoch 204/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2320 - val_loss: 0.3143\n",
      "Epoch 205/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2317 - val_loss: 0.3137\n",
      "Epoch 206/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.2216 - val_loss: 0.3124\n",
      "Epoch 207/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2243 - val_loss: 0.3146\n",
      "Epoch 208/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.2444 - val_loss: 0.3147\n",
      "Epoch 209/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2250 - val_loss: 0.3110\n",
      "Epoch 210/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2212 - val_loss: 0.3049\n",
      "Epoch 211/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2327 - val_loss: 0.3126\n",
      "Epoch 212/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.2203 - val_loss: 0.3235\n",
      "Epoch 213/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2177 - val_loss: 0.3116\n",
      "Epoch 214/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.2232 - val_loss: 0.3172\n",
      "Epoch 215/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.2156 - val_loss: 0.3067\n",
      "Epoch 216/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.2243 - val_loss: 0.3139\n",
      "Epoch 217/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2268 - val_loss: 0.3061\n",
      "Epoch 218/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.2161 - val_loss: 0.3204\n",
      "Epoch 219/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2361 - val_loss: 0.3206\n",
      "Epoch 220/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2243 - val_loss: 0.3209\n",
      "Epoch 221/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2225 - val_loss: 0.3067\n",
      "Epoch 222/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2122 - val_loss: 0.3176\n",
      "Epoch 223/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2068 - val_loss: 0.3082\n",
      "Epoch 224/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2149 - val_loss: 0.3151\n",
      "Epoch 225/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2144 - val_loss: 0.3151\n",
      "Epoch 226/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2217 - val_loss: 0.3199\n",
      "Epoch 227/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2131 - val_loss: 0.3119\n",
      "Epoch 228/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2101 - val_loss: 0.3062\n",
      "Epoch 229/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2339 - val_loss: 0.3140\n",
      "Epoch 230/500\n",
      "8347/8347 [==============================] - 1s 133us/step - loss: 0.2308 - val_loss: 0.3148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00230: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 231/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2244 - val_loss: 0.3088\n",
      "Epoch 232/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2237 - val_loss: 0.3058\n",
      "Epoch 233/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2169 - val_loss: 0.3009\n",
      "Epoch 234/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2010 - val_loss: 0.2970\n",
      "Epoch 235/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.2090 - val_loss: 0.2982\n",
      "Epoch 236/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2127 - val_loss: 0.2977\n",
      "Epoch 237/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1986 - val_loss: 0.2985\n",
      "Epoch 238/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2098 - val_loss: 0.2994\n",
      "Epoch 239/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2067 - val_loss: 0.2959\n",
      "Epoch 240/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2020 - val_loss: 0.2978\n",
      "Epoch 241/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1960 - val_loss: 0.2995\n",
      "Epoch 242/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2107 - val_loss: 0.2985\n",
      "Epoch 243/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1964 - val_loss: 0.2978\n",
      "Epoch 244/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2200 - val_loss: 0.3012\n",
      "Epoch 245/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2296 - val_loss: 0.3018\n",
      "Epoch 246/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2057 - val_loss: 0.3017\n",
      "Epoch 247/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2057 - val_loss: 0.2972\n",
      "Epoch 248/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2001 - val_loss: 0.2946\n",
      "Epoch 249/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1973 - val_loss: 0.2956\n",
      "Epoch 250/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2173 - val_loss: 0.2977\n",
      "Epoch 251/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1993 - val_loss: 0.2931\n",
      "Epoch 252/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2047 - val_loss: 0.2952\n",
      "Epoch 253/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1990 - val_loss: 0.2959\n",
      "Epoch 254/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2163 - val_loss: 0.2970\n",
      "Epoch 255/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.1935 - val_loss: 0.2952\n",
      "Epoch 256/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2016 - val_loss: 0.2984\n",
      "Epoch 257/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2119 - val_loss: 0.2956\n",
      "Epoch 258/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2119 - val_loss: 0.2927\n",
      "Epoch 259/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2331 - val_loss: 0.2924\n",
      "Epoch 260/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2039 - val_loss: 0.3000\n",
      "Epoch 261/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1964 - val_loss: 0.2960\n",
      "Epoch 262/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2040 - val_loss: 0.2934\n",
      "Epoch 263/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1991 - val_loss: 0.2979\n",
      "Epoch 264/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2021 - val_loss: 0.2967\n",
      "Epoch 265/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2017 - val_loss: 0.2951\n",
      "Epoch 266/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2182 - val_loss: 0.2922\n",
      "Epoch 267/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2013 - val_loss: 0.2928\n",
      "Epoch 268/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1976 - val_loss: 0.2981\n",
      "Epoch 269/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2027 - val_loss: 0.2965\n",
      "Epoch 270/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2105 - val_loss: 0.2935\n",
      "Epoch 271/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1978 - val_loss: 0.2932\n",
      "Epoch 272/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2028 - val_loss: 0.3000\n",
      "Epoch 273/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2063 - val_loss: 0.2933\n",
      "Epoch 274/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.2051 - val_loss: 0.2940\n",
      "Epoch 275/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2146 - val_loss: 0.2918\n",
      "Epoch 276/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.2020 - val_loss: 0.2935\n",
      "Epoch 277/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1957 - val_loss: 0.2949\n",
      "Epoch 278/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2098 - val_loss: 0.2932\n",
      "Epoch 279/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1993 - val_loss: 0.2912\n",
      "Epoch 280/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2181 - val_loss: 0.2951\n",
      "Epoch 281/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.1970 - val_loss: 0.2948\n",
      "Epoch 282/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1918 - val_loss: 0.2914\n",
      "Epoch 283/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.2044 - val_loss: 0.2935\n",
      "Epoch 284/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1985 - val_loss: 0.2936\n",
      "Epoch 285/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2001 - val_loss: 0.2962\n",
      "Epoch 286/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.1919 - val_loss: 0.2921\n",
      "Epoch 287/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.1985 - val_loss: 0.2910\n",
      "Epoch 288/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1938 - val_loss: 0.2948\n",
      "Epoch 289/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.1995 - val_loss: 0.2964\n",
      "Epoch 290/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.2313 - val_loss: 0.2989\n",
      "Epoch 291/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.2020 - val_loss: 0.2920\n",
      "Epoch 292/500\n",
      "8347/8347 [==============================] - 1s 133us/step - loss: 0.2011 - val_loss: 0.2950\n",
      "Epoch 293/500\n",
      "8347/8347 [==============================] - 1s 133us/step - loss: 0.2034 - val_loss: 0.2928\n",
      "Epoch 294/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2256 - val_loss: 0.2951\n",
      "Epoch 295/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.1995 - val_loss: 0.2941\n",
      "Epoch 296/500\n",
      "8347/8347 [==============================] - 1s 135us/step - loss: 0.2004 - val_loss: 0.2947\n",
      "Epoch 297/500\n",
      "8347/8347 [==============================] - 1s 133us/step - loss: 0.1974 - val_loss: 0.2921\n",
      "Epoch 298/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.1929 - val_loss: 0.2911\n",
      "Epoch 299/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1950 - val_loss: 0.2914\n",
      "Epoch 300/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2120 - val_loss: 0.2948\n",
      "Epoch 301/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.2032 - val_loss: 0.2910\n",
      "Epoch 302/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2016 - val_loss: 0.2922\n",
      "Epoch 303/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1933 - val_loss: 0.2916\n",
      "Epoch 304/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2024 - val_loss: 0.2944\n",
      "Epoch 305/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.1986 - val_loss: 0.2947\n",
      "Epoch 306/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1987 - val_loss: 0.2924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2161 - val_loss: 0.2916\n",
      "\n",
      "Epoch 00307: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 308/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2012 - val_loss: 0.2913\n",
      "Epoch 309/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.1937 - val_loss: 0.2905\n",
      "Epoch 310/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2031 - val_loss: 0.2908\n",
      "Epoch 311/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1937 - val_loss: 0.2905\n",
      "Epoch 312/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1905 - val_loss: 0.2904\n",
      "Epoch 313/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2011 - val_loss: 0.2905\n",
      "Epoch 314/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2091 - val_loss: 0.2907\n",
      "Epoch 315/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1953 - val_loss: 0.2911\n",
      "Epoch 316/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2258 - val_loss: 0.2911\n",
      "Epoch 317/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2325 - val_loss: 0.2909\n",
      "Epoch 318/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1877 - val_loss: 0.2906\n",
      "Epoch 319/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2075 - val_loss: 0.2903\n",
      "Epoch 320/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.2042 - val_loss: 0.2904\n",
      "Epoch 321/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2057 - val_loss: 0.2902\n",
      "Epoch 322/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1972 - val_loss: 0.2909\n",
      "Epoch 323/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2046 - val_loss: 0.2912\n",
      "Epoch 324/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1983 - val_loss: 0.2913\n",
      "Epoch 325/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1969 - val_loss: 0.2902\n",
      "Epoch 326/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2015 - val_loss: 0.2897\n",
      "Epoch 327/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1860 - val_loss: 0.2896\n",
      "Epoch 328/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2019 - val_loss: 0.2901\n",
      "Epoch 329/500\n",
      "8347/8347 [==============================] - 1s 133us/step - loss: 0.2017 - val_loss: 0.2896\n",
      "Epoch 330/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1896 - val_loss: 0.2895\n",
      "Epoch 331/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.1912 - val_loss: 0.2899\n",
      "Epoch 332/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1988 - val_loss: 0.2897\n",
      "Epoch 333/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2139 - val_loss: 0.2898\n",
      "Epoch 334/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1924 - val_loss: 0.2906\n",
      "Epoch 335/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1892 - val_loss: 0.2909\n",
      "Epoch 336/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1978 - val_loss: 0.2910\n",
      "Epoch 337/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2027 - val_loss: 0.2901\n",
      "Epoch 338/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2061 - val_loss: 0.2898\n",
      "Epoch 339/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.2029 - val_loss: 0.2894\n",
      "Epoch 340/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1970 - val_loss: 0.2895\n",
      "Epoch 341/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.1940 - val_loss: 0.2895\n",
      "Epoch 342/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1985 - val_loss: 0.2898\n",
      "Epoch 343/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1949 - val_loss: 0.2905\n",
      "Epoch 344/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1943 - val_loss: 0.2907\n",
      "Epoch 345/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1931 - val_loss: 0.2904\n",
      "Epoch 346/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1868 - val_loss: 0.2900\n",
      "Epoch 347/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1943 - val_loss: 0.2906\n",
      "Epoch 348/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.1895 - val_loss: 0.2905\n",
      "Epoch 349/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1870 - val_loss: 0.2899\n",
      "Epoch 350/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1909 - val_loss: 0.2898\n",
      "Epoch 351/500\n",
      "8347/8347 [==============================] - 1s 123us/step - loss: 0.1890 - val_loss: 0.2899\n",
      "Epoch 352/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2072 - val_loss: 0.2893\n",
      "Epoch 353/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2081 - val_loss: 0.2897\n",
      "Epoch 354/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1906 - val_loss: 0.2898\n",
      "Epoch 355/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.1914 - val_loss: 0.2895\n",
      "Epoch 356/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1837 - val_loss: 0.2891\n",
      "Epoch 357/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.2047 - val_loss: 0.2893\n",
      "Epoch 358/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1874 - val_loss: 0.2892\n",
      "Epoch 359/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1888 - val_loss: 0.2889\n",
      "Epoch 360/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.2078 - val_loss: 0.2890\n",
      "Epoch 361/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1921 - val_loss: 0.2888\n",
      "Epoch 362/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1895 - val_loss: 0.2885\n",
      "Epoch 363/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.1897 - val_loss: 0.2888\n",
      "Epoch 364/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1888 - val_loss: 0.2891\n",
      "Epoch 365/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1926 - val_loss: 0.2893\n",
      "Epoch 366/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2133 - val_loss: 0.2902\n",
      "Epoch 367/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1933 - val_loss: 0.2905\n",
      "Epoch 368/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1941 - val_loss: 0.2905\n",
      "Epoch 369/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.1945 - val_loss: 0.2903\n",
      "Epoch 370/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1899 - val_loss: 0.2898\n",
      "Epoch 371/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.2168 - val_loss: 0.2904\n",
      "Epoch 372/500\n",
      "8347/8347 [==============================] - 1s 131us/step - loss: 0.2044 - val_loss: 0.2910\n",
      "Epoch 373/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1947 - val_loss: 0.2904\n",
      "Epoch 374/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1902 - val_loss: 0.2896\n",
      "Epoch 375/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1916 - val_loss: 0.2897\n",
      "Epoch 376/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1862 - val_loss: 0.2905\n",
      "Epoch 377/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1850 - val_loss: 0.2904\n",
      "Epoch 378/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1881 - val_loss: 0.2895\n",
      "Epoch 379/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.2061 - val_loss: 0.2894\n",
      "Epoch 380/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1953 - val_loss: 0.2892\n",
      "Epoch 381/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1874 - val_loss: 0.2892\n",
      "Epoch 382/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1902 - val_loss: 0.2891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00382: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 383/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.2193 - val_loss: 0.2891\n",
      "Epoch 384/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1926 - val_loss: 0.2889\n",
      "Epoch 385/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.1926 - val_loss: 0.2888\n",
      "Epoch 386/500\n",
      "8347/8347 [==============================] - 1s 130us/step - loss: 0.1928 - val_loss: 0.2889\n",
      "Epoch 387/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1944 - val_loss: 0.2888\n",
      "Epoch 388/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1869 - val_loss: 0.2889\n",
      "Epoch 389/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1965 - val_loss: 0.2890\n",
      "Epoch 390/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.1935 - val_loss: 0.2890\n",
      "Epoch 391/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1996 - val_loss: 0.2892\n",
      "Epoch 392/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1914 - val_loss: 0.2893\n",
      "Epoch 393/500\n",
      "8347/8347 [==============================] - 1s 124us/step - loss: 0.2162 - val_loss: 0.2894\n",
      "Epoch 394/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1934 - val_loss: 0.2893\n",
      "Epoch 395/500\n",
      "8347/8347 [==============================] - 1s 129us/step - loss: 0.2061 - val_loss: 0.2893\n",
      "Epoch 396/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1860 - val_loss: 0.2892\n",
      "Epoch 397/500\n",
      "8347/8347 [==============================] - 1s 125us/step - loss: 0.1853 - val_loss: 0.2890\n",
      "Epoch 398/500\n",
      "8347/8347 [==============================] - 1s 128us/step - loss: 0.1883 - val_loss: 0.2889\n",
      "Epoch 399/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1978 - val_loss: 0.2887\n",
      "Epoch 400/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1985 - val_loss: 0.2887\n",
      "Epoch 401/500\n",
      "8347/8347 [==============================] - 1s 126us/step - loss: 0.1856 - val_loss: 0.2887\n",
      "Epoch 402/500\n",
      "8347/8347 [==============================] - 1s 127us/step - loss: 0.1943 - val_loss: 0.2891\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00402: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 00402: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Build the Neural Net\n",
    "mol_type = coupling_type\n",
    "model_name_wrt = ('../input_added/NN/molecule_model_%s.hdf5' % coupling_type)\n",
    "cv_score=[]\n",
    "cv_score_total=0\n",
    "epoch_n = 500 #1000\n",
    "verbose = 1\n",
    "batch_size = 2048\n",
    "    \n",
    "nn_model=create_nn_model(train_input.shape[1])\n",
    "retrain = True\n",
    "if not retrain:\n",
    "    nn_model = load_model(model_name_wrt)\n",
    "#rmropt = RMSprop(lr=0.0001)\n",
    "nn_model.compile(loss='mae', optimizer=Adam())#, metrics=[auc])  \n",
    " # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=40,verbose=1, mode='auto', restore_best_weights=True)\n",
    "# Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=20, min_lr=1e-7, mode='auto', verbose=1)\n",
    "# Save the best value of the model for future use\n",
    "sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_loss', save_best_only=True, period=10)\n",
    "history = nn_model.fit(train_input,[train_target], \n",
    "        validation_data=(cv_input,[cv_target]), \n",
    "        callbacks=[es, rlr, sv_mod], epochs=epoch_n, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as tic\n",
    "\n",
    "def plot_history(history, label):\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    #ax1.title('Loss for %s' % label)\n",
    "    #ax1.ylabel('Loss')\n",
    "    #ax1.xlabel('Epoch')\n",
    "   # ax1.set_yscale('log')\n",
    "    ax1.set_yticks([0.1, 0.2, 0.3, 0.4, 0.6])\n",
    "    plt.ylim([0.05, 0.2])\n",
    "    ax1.get_yaxis().set_major_formatter(tic.ScalarFormatter())\n",
    "    ax1.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predict=nn_model.predict(cv_input)\n",
    "plot_history(history, mol_type)\n",
    "accuracy=np.mean(np.abs(cv_target-cv_predict[:,0]))\n",
    "print(np.log(accuracy))\n",
    "cv_score.append(np.log(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 667.594,
   "position": {
    "height": "689.594px",
    "left": "999.188px",
    "right": "20px",
    "top": "-5px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
